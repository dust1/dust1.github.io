[{"title":"LevelDB学习（上）：前置知识","url":"/2021/06/16/LevelDB学习（上）：前置知识/","content":"## 前言：\n在完成[Epidemic](https://www.github.com/dust1/epidemic)第一阶段后，我发现在数据存储部分的设计没有学习到更多知识，因此通过对LevelDB的学习了解一下在实际项目中对于数据的存储是如何设计与实现的。\n<!-- more -->\n由于LevelDB是我接触的新领域，因此在学习LevelDB之前需要补充一下前置知识，这样方便后续对LevelDB结构设计与源码阅读的理解。\n\n## Varint\nVarint 是一种紧凑的表示数字的方法。它用一个或多个字节来表示一个数字，值越小的数字使用越少的字节数。这能减少用来表示数字的字节数。\n在编程中表示整数的类型通常是int32,int64。但是我们在使用的时候并不会完全利用所有的位数，比如1，2，300等数字，这样的话会浪费大量的位数，因此对于很小的int32位数字，可以用1个byte来表示。但这样的缺点就是大数字需要5个byte表示。不过在系统中小数字还是占大多数的，因此采用Varint从整理来看能节省更多的存储开销。\n### Varint格式\nVarint中的每个byte的最高位bit有特殊的含义：\n* 1：后续的byte也是该数字的一部分\n* 0：统计结束\n\n因此1个byte中用于存储数字的位数为7位，在Varint中1个byte最高能表示的数字大小为：127，超过这个值就需要用2个byte表示。\n注意的一点是Varint采用小端字节序，对于二进制来说左边才是低位。\n\n### Varint编解码\n取一个数字300，用二进制表示为：0000 0001 0010 1100。将其用Varint编码后得：1010 1100 0000 0010。由于每个byte的首位都有特殊含义，在第一个byte中首字节是1，表示后续的byte也是该数字的一部分，因此得到的结果为：_010 1100 _000 0010。由于是小端字节序，所以在重新排序后得到结果为：_000 0010 _010 1100，去掉Varint的特殊位后得到：0001 0010 1100。\n<img src=\"/img/202106/006.png\" width=\"50%\" height=\"50%\">\n\n### 对于有符号的数字\n从上可知Varint并不能表示负数，因此Google采用zigzag编码来替代。zigzag采用正数和负数交错的方式来同时表示无符号数来表示有符号数字，如图所示：\n<img src=\"/img/202106/007.png\" width=\"50%\" height=\"50%\">\n使用zigzag编码，绝对值小的数字，无论正负都可以采用较少的byte来表示，充分利用了Varint这种技术。\n\n### Varint实现\n```java\nprivate static int encoder(int num) {\n    int result = 0;\n    while (num > 0) {\n        byte n = (byte) (num & 0b1111111);\n        num >>= 7;\n        result <<= 8;\n        if (num > 0) {\n            result += n ^ 0b10000000;\n        } else {\n            result += n;\n        }\n    }\n    return result;\n}\n\nprivate static int decoder(int num) {\n    int result = 0;\n    while (num > 0) {\n        result <<= 7;\n        byte n = (byte) (num & 0b11111111);\n        // int tag = (n & 0b10000000) >> 7;\n        //System.out.println(\"tag:\" + tag);\n        byte m = (byte) (n & 0b1111111);\n        result ^= m;\n        num >>= 8;\n    }\n    return result;\n}\n```\n\n## CRC\nCRC校验用于检验文件与请求的完整性。并且在校验值不匹配的情况下，可以针对数据损坏采取纠正措施。\n之所以称为 CRC，是因为校验（数据验证）值是一种冗余（它在不添加信息的情况下扩展消息）并且该算法基于循环码。CRC 很受欢迎，因为它们易于在二进制硬件中实现，易于数学分析，并且特别擅长检测由传输通道中的噪声引起的常见错误。由于校验值具有固定长度，生成它的函数偶尔会用作散列函数。\nCRC基于循环纠错码的理论，通过添加固定长度的校验值对消息进行编码，用于通信网络中的错误检测。CRC 码的规范需要定义一个所谓的生成多项式，该多项式成为多项式长除法中的除数，将消息作为被除数，其中商被丢弃，余数成为结果。\n通常是n-bit CRC 应用于任意长度的数据块将检测任何不超过n位的单个错误突发，并且它将检测到的所有较长错误突发的分数是(1 − 2−n )。当校验值为n位长时，CRC 称为n位 CRC 。\n\n### 缺点\n1. CRC专门用于信道通信上的非人为影响的问题，如果是人为故意的，攻击者可以在修改数据后重新编码CRC，在这种没有身份验证的情况下是无法区分的。\n2. CRC编码是可逆的，这使得它不适合应用在数字签名中。\n3. CRC是一个线性函数，具有有限等效保密协议的设计缺陷，容易被反向恢复密钥导致关键信息泄漏。\n\n### 计算\n要计算n位的CRC，首先需要将表示输入的位排成一行，并将表示CRC除数（即多项式）的n+1位模式放置在行的左下方。\n#### 例子\n设：\n* CRC的位数为3\n* 多项式为：x<sup>3</sup>+x+1\n* 消息的位数为14\n\n首先将多项式用二进制写成系数，得：1 * x<sup>3</<sup>> + 0 * x<sup>2</sup> + 1 * x + 1，在这种情况下，系数为1、0、1和1。计算的结果是3位长，因此这是一个3-CRC。但是我们需要4位来说明多项式。\n等待编码的消息为：11010011101100\n首先，用CRC对应位数个数的0填充。这样做是为了使字节码具有系统形式。这是计算3位CRC的第一步计算：\n```bash\n11010011101100 000    <---- 输入右填充3位\n1011    <---- 除数（4位） = x3+x+1\n-----------------------------------------\n01100011101100 000    <---- 结果\n该算法作用于每一步中除数上方的位，迭代的结果是多项式与上方位的按位异或（相同为0，不同为1）。这一步完成后将结果重新进行计算，但是在开始的时候将除数右移直到与新的输入左边的1对应：\n11010011101100 000 <--- 输入右填充 3 位\n1011 <--- 除数\n01100011101100 000 <--- 结果（注意前四位是与下面除数的异或，其余位不变）\n 1011 <--- 除数...\n00111011101100 000\n  1011\n00010111101100 000\n   1011\n00000001101100 000 <--- 请注意，除数移动以与被除数中的下一个 1 对齐（因为该步骤的商为零）\n       1011（换句话说，它不一定每次迭代移动一位）\n00000000110100 000\n        1011\n00000000011000 000\n         1011\n00000000001110 000\n          1011\n00000000000101 000\n           101 1\n-----------------\n00000000000000 100 <--- 余数（3 位）。\n```\n除法算法在此处停止，因为正体的消息中已经全部为0.\n由于除数每次都移动到结果最左端1的位置，最终的结果是待编码的位数都为0，最终结果中唯一可以为非0的区域是输入右边填充的n位，而这n位就是CRC的结果。\n\n#### 代码\n```java\n/**\n* @param value 待编码的树\n* @param fnc 除数\n*/\npublic static int check(int value, int fnc) {\n    //后期待编码的数字的长度\n    int valueLen = readIntLen(value);\n    //CRC的位数\n    int fncLen = readIntLen(fnc) - 1;\n    // 往右填充后的值\n    int n = value << fncLen;\n    for (int i = 0; i < valueLen; i++) {\n        int nLen = readIntLen(n);\n        if (nLen <= fncLen) {\n            break;\n        }\n        //除数要移动的位数\n        int moveLen = nLen - fncLen - 1;\n        int nFnc = (fnc << moveLen);\n        n = n ^ nFnc;\n    }\n    return n;\n}\n\n/**\n* 获取n的长度，\n* @param n 计算到从最高位的1为止\n*/\nprivate static int readIntLen(int n) {\n    int result = 0;\n    while (n > 0) {\n        n >>= 1;\n        result += 1;\n    }\n    return result;\n}\n```\n\n## Snappy压缩\nSnappy是一个压缩/解压库。它的目标不是最大压缩，也不是与其他任何压缩库兼容。它的目标是为了实现最快压缩与合理压缩。\nSnappy具有以下属性:\n* 快速：压缩速度为 250 MB/秒及以上，无需汇编代码。\n* 稳定：在过去几年，Snappy 在 Google 的生产环境中压缩和解压了数 PB 的数据。Snappy 比特流格式是稳定的，不会在版本之间发生变化。\n* 健壮：Snappy 解压器的设计不会在遇到损坏或恶意输入时崩溃。\n\n可以说，Snappy最大的优点就在于快速压缩。\n\n### LevelDB中的使用\n在LevelDB中，默认就开启了Snappy压缩，用于对写入的数据进行快速压缩，这可以在保证写入速率的情况下节省空间。\n对于LevelDB来说，这些优化方案不能够牺牲读写速度，因此采用Snappy，在保证速度的同时还能够获取一定的磁盘空间。\n\n## 互斥锁\n互斥锁是悲观锁的一种，当线程在获取竞争资源时无论是读还是写，都需要先获取到该资源的锁。它能保证在同一时刻竞争资源只会被一个线程修改。\n\n## 跳表(Skip list)\n跳表是使用概率平衡而不是严格强制平衡的数据结构。因此，跳表的插入和删除算法比平衡树的等效算法简单得多，速度也快得多。\n二叉树在插入随机顺序的元素的时候工作的很好，但是一旦按顺序插入元素，他就会退化为链表导致性能下降。由此产生了平衡树算法，在执行操作的时候重新排列树以保持平衡。\n跳表是平衡树的概率替代方案。它通过随机数生成器来平衡树。在理论上跳表也是能够退化为链表（多个随机数生成为顺序的情况），但是这种情况发生的比例很低，且没有任何输入序列能够连续产生最坏情况。（随机主元素的快速排序）\n跳表具有类似于随机插入构建的搜索树的平衡性，但实际使用中不需要随机插入。\n### 跳表与平衡树的优势\n1. 跳表的实现比平衡树更简单；\n2. 通过概率去平衡数据结构比明确地保持平衡更容易；\n3. 跳表随着数据的插入操作速度是恒定增加的；\n4. 跳表比平衡树节省空间。\n\n### 实现推理\n设：\n* 给定一个长度为n的链表\n* 链表中的元素按顺序存储\n\n1. 在搜索链表时，可能要检查每个节点：\n<img src=\"/img/202106/008.png\" width=\"50%\" height=\"50%\">\n\n2. 如果链表按顺序排列，给每两个节点一个指向链表中后两个节点的指针，由此在原有的基础上构建出原长度的1/2长的链表，此时我们只需要检查n/2+1个节点：\n<img src=\"/img/202106/009.png\" width=\"50%\" height=\"50%\">\n\n3. 按照上述步骤继续增加间隔，此时是新增每四个节点建立一个指针，检查链表只需要n/4+2：\n<img src=\"/img/202106/010.png\" width=\"50%\" height=\"50%\">\n\n4. 如果每个节点前面有一个指针指向2i节点，则必须检查的节点数量可以减少到log<sub>2</sub>n：\n<img src=\"/img/202106/011.png\" width=\"50%\" height=\"50%\">\n\n> 但是这种数据结构只方便用于快速搜索，无法有效地插入删除。\n\n具有k个前向指针的节点称为k级节点。如果每个第2i个节点前面都有一个指针2i节点，那么节点的级别有简单的模式分布：50%是级别1，25%是级别2，12.5%是级别3，以此类推。如果节点的级别是随机选择的，但是比例相同如下：\n<img src=\"/img/202106/012.png\" width=\"50%\" height=\"50%\">\n此时一个节点的第i个前向指针不是指向前面的2i个节点，而是志向下一个i级或者更高级别的节点。此时插入删除只需要局部修改；节点的级别是在插入节点时随机选择的，永远不需要改变。某些级别的安排会导致执行时间很短，但这种安排会很少见。因为这些数据结构是带有跳过中间节点的额外指针的链表，所以我们将其称为跳表。\n\n> 在分配节点等级阶段就引入随机，但是在宏观上却保持一个准确值，在这里体现的就是跳表的比例正确，但是跳表节点出现的地点随机，这样在一个小表中可以方便地进行插入删除操作，对整体的读取速度影响较小，因为在宏观上多级节点数量比例是正确的。\n\n### 算法详情\n每个元素由一个节点表示，节点的级别在插入节点时随机选择，而不考虑数据结构中的元素数量。第i级节点有i个前向指针，索引从1到i。在节点中不需要存储节点的级别。级别上限为某个适当的常量MaxLevel。列表的级别是列表中当前的最大级别（如果列表为空，则为1）。\n列表的标题在第一级到MaxLevel有前向指针。级别高于列表当前最大级别的标头的前向指针指向NIL\n> NIL类似于一个null节点，用来标识检索结束\n\n#### 初始化\n一个元素NIL被分配并被赋予一个大于任何合法键的键。所有跳过列表的所有级别都以NIL终止。\n初始化一个新列表，使列表的级别等于1，并且列表头的所有前向指针都指向NIL。\n> 在初始化阶段创建一个NIL与初始节点\n\n#### 搜索算法\n<img src=\"/img/202106/013.png\" width=\"50%\" height=\"50%\">\n\n1. 首先获取跳表中的header节点设置为x\n2. 遍历所有key小于要搜索元素key的节点\n3. 如果遍历完成后无法得到有效节点，则前往下一层\n4. 如果在第一层也无法找到对应节点则返回失败\n\n#### 插入与删除算法\n<img src=\"/img/202106/014.png\" width=\"50%\" height=\"50%\">\n若要插入，则可以直接采用搜索+拼接的方式。\n\n伪代码如下：\n<img src=\"/img/202106/015.png\" width=\"50%\" height=\"50%\">\n维护向量更新，以便当搜索完成（准备执行拼接）时，update[i]包含指向第i级或更高级别的最右侧节点的指针，该节点位于插入位置的左侧。\n\n#### 随机选择Level\nLevel是在不参考元素数量的情况下随机生成：\n<img src=\"/img/202106/016.png\" width=\"50%\" height=\"50%\">\n这里设置了一个初始值newLevel，并通过随机数生成函数判断生成的随机数是否小于p，如果小于p则newLevel+1，返回newLevel与MaxLevel的最小值。\n这里通过随机数概率的形式获取一个随机的Level。\n\n## LSM-Tree\n见这篇文章：[初探LSM-Tree](/2021/06/03/初探LSM-Tree/#more)\n\n## 问题\n在了解了前置知识后我对LevelDB抱有很多疑问，因此后面就带着这些问题去学习LevelDB：\n* SSTable大小如何划分\n* rolling merge过程是如何实现的，是跟论文原文一样将C0部分/全部数据跟C1全部数据组合还是如何\n* 磁盘组件索引在内存中是如何展现的，即这部分数据在内存中是以什么形式展示的\n* rolling merge过程中对并发访问是如何解决的\n* checkpoint是如何创建以及使用的\n* 查询/添加/删除操作的具体实现步骤\n* sstable的具体结构实现\n* memtable到sstable的数据结构转换过程","tags":["存储"]},{"title":"LSM-Tree开销与组件大小计算","url":"/2021/06/05/LSM-Tree开销与组件大小计算/","content":"如何准确地估算LSM-Tree中各个组件地阈值大小是一个数学问题。但是在实际计算过程中为了简化计算，所求出的最终结果会是一个近似值而不是准确的数值。\n> 对于计算机来说差的那点数值在计算时间的表现上并不明显。\n\n<!-- more -->\n> 题外话：我发现论文中的数学推导很多数据都不知道是从哪里来的。还是要问问相关人士。\n\n## 前言\n首先需要确定影响组件大小的因素以及对他们的假设：\n* π：磁盘以multi-page blocks为单位读写一个page时的开销\n* p：磁盘随机读写一个page的开销\n* M：从C0中merge到C1中的一个page-sized的叶节点中的记录的平均数目\n* d：1MB磁盘存储开销\n* m：1MB内存存储开销\n* H：每秒H个磁盘页面访问的的IO传输需求\n* S：数据存储总量\n\n## 开销的计算\n> 对于磁盘来说，如果磁盘容量是瓶颈，则在磁盘填满时应用程序只使用了磁盘磁臂所提供的IO能力的一部分；反过来，如果我们发现在添加数据时，磁盘磁臂已经被充分使用，但是磁 盘还有剩余空间，这就意味着IO能力是瓶颈。\n\n这个结论很重要，因为对于我们来说，在任何时候这两种开销总会有一种成为访问瓶颈。因此应用程序针对磁盘的访问开销D就是：\n> D = max(H·p, S·d) -> 访问开销=max（数据存储总开销，数据访问的IO开销）\n\n在没有缓存的情况下D就是应用程序支持数据S访问的总开销T。如果在S不变的情况下，应用程序针对磁盘的总开销随着IO的访问频率H成线性增长。当IO的开销`H·p`超过磁盘开销`S·d`后可以引入内存缓存来取代磁盘IO。这个时候就表示数据的访问频率高到IO使用率达到100%但是依旧有大量的请求没有得到IO。那么在这种情况下磁盘的开销就又只取决于所需的磁盘存储空间，此时访问数据的开销B就可以简单地表示为内存的开销与磁盘存储的开销：\n> B = S·m + S·d -> 相当于将磁盘中的所有数据都载入内存\n\n此时用于支持应用程序的数据访问的总开销T就是由D和B的最小值决定的：\n> T = min(max(H·p, S·d), S·m + S·d)\n\n由此可以画出数据访问总开销与访问H/S的关系，其中`COST-TOT=T`\n<img src=\"/img/202106/004.png\" width=\"50%\" height=\"50%\">\n\n## 组件大小的计算\n设：\n* S<sub>i</sub>：表示组件Ci的大小\n* S：表示LSM-Tree所有组件大小之和，S=S<sub>0</sub>+S<sub>1</sub>+...S<sub>k</sub>\n* S<sub>p</sub>：页大小（以bytes为单位）\n* R：针对C<sub>0</sub>的插入速率，单位bytes/秒。为了方便计算，这个速率是相对平稳的数值，即一个常量\n* H：每秒H个磁盘页面访问的的IO传输需求\n* r<sub>i</sub>：相临两个组件大小只比。r<sub>i</sub> = S<sub>i</sub>/S<sub>i-1</sub> (i=1,2...k)\n* 所有的组件C<sub>i</sub>都有一个相对固定的大小（因为删除会和会将插入抵消掉，因此实际的C<sub>i</sub>大小是会变化的）\n* 不同组件的blocks将会以混合的方式使用不同的磁盘，以达到磁盘磁臂利用率的平衡\n\n这样最小化H就等价于最小化总的磁盘磁臂开销\n> 前提这是是IO开销大于磁盘存储开销的情况\n\n由此我们可以得到一个问题：`对于给定的R，找到可以使总的IO请求率H最小化的ri值`。\n对于S，我们可以得到：\n> S = S<sub>0</sub> + r<sub>1</sub>·S<sub>0</sub> + r<sub>1</sub>·r<sub>2</sub>·S<sub>0</sub>+...\n\n在总S固定不变的前提下，由于r<sub>i</sub>的变化会使的问题复杂化，但是当假设S<sub>k</sub>也是固定不变的时候，由于S与S<sub>k</sub>是固定的，则当所有的r<sub>i</sub>值等于某个常数r时，就可以取得最小值。此时可以用S与S<sub>0</sub>来表示r:\n> S = S<sub>0</sub> + r.S<sub>0</sub> + r<sup>2</sup>.S<sub>0</sub> + . . . + r<sup>k</sup>.S<sub>0</sub>\n\n我们假设数据在到达C<sub>k</sub>之前都不会被删除，因此对于C<sub>0</sub>的数据插入速率与C<sub>i-1</sub>到C<sub>i</sub>的数据移出速率应该是一样的（0 < i ≤ k）。\n当C<sub>i-1</sub>为磁盘组件的情况，从C<sub>i-1</sub>到C<sub>i</sub>的rolling merge过程将会包含从C<sub>i-1</sub>中以每秒R/S<sub>p</sub>个page的multi-page block读取。该合并过程还会包含从C<sub>i</sub>中以每秒r<sub>i</sub>·(R/S<sub>p</sub>)个page的multi-page block读取（这是因为在merge过程中在C<sub>i</sub>中扫描的page数量是C<sub>i-1</sub>的r<sub>i</sub>倍）。最后，数据还会以每秒(r+1)·(R/S<sub>p</sub>)个page的multi-page磁盘写入来将属于C<sub>i</sub>的新merge出来的数据写入。（由于merge引起导致C<sub>i</sub>组件的增大，因此数据成为了(r+1)·(R/S<sub>p</sub>)个page）。将所有的组件的C<sub>i</sub>对应值相加，就可以得到总的multi-page IO大小H（以每秒的page数为单位）：\n> H=(R/S<sub>p</sub>)·((2·r<sub>1</sub>+2) + (2·r<sub>2</sub>+2) + … +(2·r<sub>k-1</sub>+2) + (2·r<sub>k</sub>+1))\n\n每一项的(2·r<sub>i</sub>+k)代表了在组件C<sub>i</sub>上的所有IO:\n1. R/S<sub>p</sub>：从C<sub>i-1</sub>到C<sub>i</sub>的merge读入属于C<sub>i-1</sub>中的数据\n2. r<sub>i</sub>·(R/S<sub>p</sub>)：从C<sub>i-1</sub>到C<sub>i</sub>的merge读入属于C<sub>i</sub>中的数据\n3. (r+1)·(R/S<sub>p</sub>)：将merge的结果写入\n\n对于C<sub>0</sub>没有读取前一个组件的开销，对于C<sub>k</sub>也没有最后。因此该公式可以简化为：\n> H=(2R/S<sub>p</sub>)(Σ(r<sub>i</sub>)+k-1/2)，(1≤i≤k)\n\n化简后得：\n> H=(2R/S<sub>p</sub>)·(k·(1+r)-1/2)\n\n由此我们得到了两个重要的公式：\n> 公式1：S = S<sub>0</sub> + r.S<sub>0</sub> + r<sup>2</sup>.S<sub>0</sub> + . . . + r<sup>k</sup>.S<sub>0</sub>\n> 公式2：H = (2R/S<sub>p</sub>)·(k·(1+r)-1/2)\n\n根据公式1可得：r会伴随着S<sub>0</sub>的减小而增大；根据公式2可得：H与r成正比。\n得：H会随着S<sub>0</sub>的减小而增大\n\n## 当LSM-Tree为两组件时\n应用程序针对数据访问的总开销分为内存开销与磁盘开销，内存开销就是存储C<sub>0</sub>的存储开销，表达式为：\n> m·S<sub>0</sub>\n\n存储开销是由访问IO开销与磁盘存储开销的最大值决定的，在这里的IO开销基于multi-page blcok访问频率H（pages/秒），则应用程序针对数据访问的总开销T：\n> T = m·S<sub>0</sub> + max(d·S<sub>1</sub>, π·H)\n\n由于考虑的是两组件的情况，所以k=1，r=S<sub>1</sub>/S<sub>0</sub>，令：\n> s = m·S<sub>0</sub>/d·S<sub>1</sub> = 内存开销与S1数据的存储开销之比为s\n> t = 2·((R/S<sub>p</sub>)/S<sub>1</sub>)·(π/d)·(m/d)\n> C = T/(d·S<sub>1</sub>) = 总开销与S<sub>1</sub>数据的存储开销之比\n\n将公式2替换后得到：\n> C = s + max[1, t(d/m)(r + 1/2)\n\n又r = S<sub>1</sub>/S<sub>0</sub>\n得：\n> C = s + max[1, t(d/m)(S<sub>1</sub>/S<sub>0</sub> + 1/2)\n\n假设S<sub>1</sub>/S<sub>0</sub>得比值会很大，则可以将1/2消除求近似值：\n> C ≈ s + max[1, t(d/m)(S<sub>1</sub>/S<sub>0</sub>)\n\n化简得：\n> C ≈ s + max[1, t/s]\n\n由于磁盘访问得开销是数据访问的IO开销与存储开销的最大值，因此这也意味着：\n> t/s ≈ π·H / d·S<sub>1</sub>\n\n这样，相对于开销C，就是变量t和s的函数。变量t和s的定义如下：\n* t：t实际上是应用程序所需的multi-page block IO频度的某种形式化表示。对于t来说，π/d体现的是磁盘IO与空间开销之比，m/d体现的是内存开销与磁盘开销之比，这两个值是两个常量。而((R/S<sub>p</sub>)/S<sub>1</sub>)则体现的是数据的访问密度（单位：页/bytes），如果R越大，则t越大。\n* s：s代表了为实现LSM-tree我们所需要提供的内存大小。\n\n为确定S<sub>0</sub>的大小，最简单地规则就是，沿着s=t这个分界线，此时C = s+1。同时磁盘存储和IO能力都已被完全利用。（当s=t时，由于t/s ≈ π·H / d·S<sub>1</sub>，说明此时π·H = d·S<sub>1</sub>，表示磁盘存储空间所具有的IO能力被完全使用）。\n当t≤1时，在s=t时可以取得最小值；对于t>1，C的最小值是沿着曲线s=t<sup>1/2</sup>的，此时C=2·t<sup>1/2</sup>，带入等式可得：\n> 公式3：T<sub>min</sub> = 2[(m·S<sub>1</sub>) · (2·π·R/S<sub>p</sub>)]<sup>1/2</sup>\n\n由上可得，在t≥1的情况下，LSM-tree的总开销是足以将所有LSM数据保存下来的所需内存的开销(m·S<sub>1</sub>)和用于支持将插入数据写入到磁盘所需的multi-page block IO的磁盘开销(2·π·R/S<sub>p</sub>)的几何平均数的2倍。\n总开销中的一半将会被用于S<sub>0</sub>的内存开销，剩下的一半用于对于S<sub>1</sub>的IO访问开销。\n这里没有体现磁盘存储开销，因为当t≥1的情况下，IO访问的开销已经大于或等于磁盘存储开销，使的磁盘访问开销的主导是IO而不是容量。\n\n在t≤1（数据较冷）的情况下，最小成本出现在s=t，可知C=t+1≤2。这意味在这种情况下总开销总是不会超过用于将S1存储在磁盘上的开销的两倍。在这种情况下，我们根据磁盘的存储要求来调整磁盘大小，然后使用其所有IO容量来最小化内存使用。\n\n### 两组件时的例子\n首先提供的数据如下：\n* 插入速率R为16,000bytes/秒\n* 1条索引记录为16字节\n* 一个page的大小为4KB\n* 考虑一个20天的周期，每天8小时\n\n忽略其他开销，在20天之后产生的数据将是576,000,000条记录，总大小为9.2GB。对于一个两组件的LSM-Tree来说，我们需要一个大小为9.2GB的S<sub>1</sub>用于存储数据，开销就是d·S<sub>1</sub>=9600。得H=9600/π=3700 pages/秒。结合公式2，解出r=460，根据r=S<sub>1</sub>/S<sub>0</sub>，当S<sub>1</sub>为9.2GB的时候S<sub>0</sub>=20MB。此时t=0.22。\n> 这里的计算都是估算值，并不是一个准确的值，毕竟对于计算机来说可以接受一定的误差范围内的时间开销\n\n通过这个例子我们可以知道，在两组件LSM-tree的情况下计算C<sub>0</sub>，需要的必须参数有：\n1. 数据插入速率R\n2. 每条索引的大小\n3. 在C<sub>1</sub>的叶子节点中，每个page的大小\n4. 磁盘以multi-page blocks为单位读写一个page时的开销\n5. 1MB磁盘存储开销\n\n## 当LSM-Tree为三组件时\n当R增长10倍，意味着t也增大10倍，从0.22变为2.2，现在t就是大于1的了。利用公式2来计算出两组件LSM-tree的最小开销为$27,000，其中一半用于13.5GB的磁盘，一半用于135MB的内存。此时，仍有4.3GB的磁盘空间未被使用，加上用于缓存的2MB内存，总开销是$27,200。\n\n插入频率R=160,000bytes/秒，意味着每秒需要有40个页面(每个页面4KB)从C0 merge到C1。因为C1大小大概是C0的68倍，因此将C0中的一个页面的merge需要读写C1中的68个页面，每秒就是5440(C0每秒40个，40·68·2=5440)个页面。这也是13.5个磁盘在使用multi-page block IO的情况下所能提供的IO能力。\n\n对于使用一个三组件LSM-tree来处理这种R=160,000bytes/秒的情况，可以按照两组件的那种方式先计算出最大的那个磁盘组件的一个开销以及IO频率的一个开销平衡点。由于r=S<sub>1</sub>/S<sub>0</sub>，i=1，2。对于三组件来说k=2，带入公式2得：\n> 3700+3700/r=(2·160,000/4000)·(2·(1+r)-1/2)\n\n我们可以计算出r=23并且S0=17MB。\n\n内存组件C0大小为17MB，稍小的那个磁盘组件大小C1是它的23倍，大概是400MB，C2又比C1大23倍，是9.2GB。那么每个页面在从C0 merge到C1时，将会引入23个页面读和23个页面写操作，这样每秒就需要读写1840(40*23*2)个页面。类似的，每秒也需要有40个页面从C1 merge到C2，每个页面也会引入23个页面读和23个页面写操作，这样每秒也会需要读写1840(40*23*2)个页面。这样总的IO频率就是3680，刚好是9.2G的磁盘在使用multi-page block IO的情况下所能提供的IO能力。\n","tags":["分布式存储"]},{"title":"初探LSM-Tree","url":"/2021/06/03/初探LSM-Tree/","content":"## 前言\nLSM-Tree，全称The Log-Structured Merge-Tree（日志结构合并树）。\n<!-- more -->\nLog-Structured源自Ousterhout和Rosenblum在1991年发表的经典论文<<[The Design and Implementation of a Log-Structured File System](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.8933) >>,这篇论文提出了一种新的磁盘存储管理方式，在这种结构下，针对磁盘内容的所有更新将会被顺序地写入一个类日志的结构中，从而加速文件写入和回收速度。该日 志包含了一些索引信息以保证文件可以快速地读出。日志会被划分为多个段来进行管理。这种方式非常适合于存在大量小文件写入的场景。\nThe Log-Structured Merge-Tree (LSM-Tree)就是设计用于来为那些长期具有很高记录更新(插入或删除)频率的文件来提供低成本的索引机制。LSM-Tree通过使用某种算法，该算法会对索引变更进行延迟及批量处理，并通过一种类似于归并排序的方式联合使用一个基于内存的组件和一个或多个磁盘组件。\n## LSM-Tree相关概念\n### 索引组件\nLSM-Tree由两个或多个类树的数据结构组件构成。一个两组件的LSM-Tree如下所示：\n<img src=\"/img/202106/001.png\" width=\"50%\" height=\"50%\">\n其中，C0-Tree驻留在内存中，而C1-Tree保存在磁盘上。\n\n#### C0/内存组件\nC0并不一定需要B树结构，因为C0不会持久化到磁盘中(或者说在使用C0树的过程中永远不会从磁盘中读取)。因此一棵AVL(平衡二叉搜索树)就可以满足需求，但依然需要保持插入节点的顺序。\n当C0增长到它的阈值大小时，它会从最左边的一系列记录开始删除节点。\n\n#### C1/磁盘组件\nC1具有一个类似B树的目录结构，这种结构可以在牺牲一定CPU效率的前提下减少树的高度，从LSM-Tree中体现出来的就是减少磁盘IO的开销，这也是LSM-Tree最大的优点。\n\n> Q:为什么采用B树或者类似B树的目录结构呢？\n> A:为了将叶子节点按顺序排序，使的叶子节点的数据是连续的，减少磁盘臂的移动开销。\n\n每个磁盘组件都是由以B树类型的结构组织的page-sized的节点组成，同时在根节点下的各层的节点都是按照key的大小顺序排列的，同时这些节点又会被放置到mutil-page block中。\n它的意思是B树中的多个节点组成一个mutil-page block结构，在B树中可以获取到这个mutil-page block结构，然后就可以直接获取到这个mutil-page block下所有的节点，这样的好处就是在执行rolling merge的时候不需要重建检索一遍B树，而是通过遍历mutil-page block来批量获取节点。\n\n### multi-page block\n一个抽象的概念，在LSM-Tree中，每条数据的索引在基于磁盘的组件中表现的就是B树的节点。多条索引记录组成mutil-page block。结构图如下：\n<img src=\"/img/202106/005.png\" width=\"50%\" height=\"50%\">\n在大多数情况，每个mutil-page block都装满了节点，但是也有些情况一个multi-page block只有少数节点组成，这个节点通常是上次rolling merge所合并的最后一个节点，因为受到删除操作的影响，最终进入filling block的数据量大小是不可控的。\n\n#### emptying block\n一组合并前所有节点组成的队列，每当进行一次合并后将其中的节点抛出。\n#### filling block\n完成数据合并后的最终结果，其中的数据是连续的，当它被填满后会被刷入磁盘中，写入时会在磁盘中新开辟存储空间，并不会覆盖旧的数据。这是为了出现异常时快速恢复\n### rolling merge\nLSM-Tree中的C0存在内存中，而内存大小的成本是很大的，因此当C0的大小达到一定阈值后要被持久化到磁盘中，在LSM-Tree中，C0在持久化到磁盘前需要和C1的数据进行一次合并，这个过程就被成为rolling merge。\n在这个过程中会将C0的部分或者全部数据删除并合并到C1中。\n我们可以把整个两组件LSM-tree的rolling merge过程想象成一个具有一定步长的游标循环往复地穿越在C0和C1的键值对上，不断地从C0中取出数据放入到磁盘上才C1中。该rolling merge游标在C1树的叶节点和更上层的目录级都会有一个逻辑上的位置。\n## 插入、删除、查询流程\n### 插入\n当数据插入的LSM-Tree的时候，它会执行两个操作：将其追加写入一个特殊的日志文件中用于异常/重启恢复；将其加入C0中。\n当C0大小达到阈值的时候进行rolling merge，将C0的一部分数据移入C1中。\n### 删除\n当某一个记录要被删除时，如果目标索引不在C0中，则在C0中创建这个索引并加上删除标记，后续如果有其他用户需要对索引进行查询或者更新操作就可以直接返回，而不用在磁盘中查询。\n实际的删除操作并不会立刻生效，而是会在C0记录中添加一条删除记录，这条记录会随着rolling merge往大的组件移动，一旦遇到对应的要删除的数据对象则执行数据清除工作。\n> 如果要对原数据进行修改，在LSM-Tree中可以看成两条连续的记录，分别是删除、添加。\n> 之所以要这样是因为有些更新操作会修改数据的ID，而不是其他数据，这样操作的后果就是目录中针对该数据的索引需要变更，而为了单条数据去扫描整个目录是十分愚蠢的，因此将这种更新操作转化为连续的两步删除与添加操作。\n#### 断言式删除\n这是LSM-Tree中的批量删除的方式，它会在记录中添加比如：`删除创建时间超过20天的记录`这样的断言，当这条断言随着rolling merge移动到大的组件中时，那些符合条件的被载入内存的数据将会被清除。\n### 查询\n在执行等值匹配查询时，磁盘组件Ci的一个节点可能是存在于单独的一个内存页中，或者是通过它所在的muti-page block而缓存在内存中。\n\n当在LSM-Tree中进行精确匹配查找或者随机查找时，步骤如下：\n1. 首先会到C0中查找所需的那个值或者域\n2. 如果C0中没有结果，则从C1中查找\n\n与B树相比，这样的查找方式在一定条件下会有额外的IO开销与CPU开销，因为LSM-Tree的查找需要经过内存与磁盘。但是这两步简单的操作可以有很多优化方式：\n1. 如果生成逻辑可以保证索引值时唯一的，比如时间戳。则一个匹配查找已经在一个早期的Ci组件中找到的时候就可以宣告完成。因为越早的组件它的数据就越新，虽然旧的组件也会有这条数据，但那是已经过期的了。\n2. 如果查询条件里使用了最近时间戳，那么可以让那些查找到的值不要向最大的组件中移动，因为这些组件通常时间比较前。一个查询操作可能需要随着rolling merge一直传播到最大的组件，但如果在查询条件中加上时间限制，由于LSM-Tree的记录都是顺序的，因此在监测到有记录的时间戳已经超过查询条件后就可以直接结束查询。\n3. 当merge扫描到(Ci, Ci+1)对时，可以将最近常访问的值继续保留在Ci中，因为merge完成后原始数据会在内存中保留一段时间，这短时间内可以为常用数据提供快速查找\n> 第三点在短期事务UNDO日志中就有所使用，对于短期事务，在中断发生时通常都是针对相对近期的数据的访问，这样大部分的索引就都会仍然处于内存中。\n4. 通过记录每个事物的启动时间，就可以保证所有最近的t0秒内发生的事物的所有日志都可以在C0中找到，而不需要磁盘访问。类似于第2条的优化方式，也是按照时间将查询的范围缩小。\n\n### long-latency find\n对于那些需要等待很长时间的查询操作来说，通过向C0中插入一个`find note entry`，当它被移入更大的组件中时如果遇到符合条件的数据则会记录下来，随着rolling merge的进行，查询也会生效，直到到达最大组件，本次long-latency find所对应的那些匹配的ID记录就生成完毕了。\n\n## rolling merge\n通常，一个具有C<sub>0</sub>、C<sub>1</sub>...C<sub>k</sub>个组件大小不断增长的LSM-Tree，其中C<sub>0</sub>组件是放在内存中，其他组件都是在磁盘中驻留。当C<sub>i-1</sub>超过阈值时，会由(C<sub>i-1</sub>,C<sub>i</sub>)之间的异步的rolling merge过程负责将记录从小的组件移到更大的组件中。\n\n当C0的大小达到一定阈值的时候会进行异步rolling merge，此时将C0中的部分或全部数据与C1的节点进行合并，然后将接入重新写入C1。当C1大小达到一定阈值后也会启动rolling merge操作将数据转移到C2中。以此类推。\n每次进行rolling merge的时候会处理多个操作：将B树中针对单条操作的IO开销通过批处理进行分摊。\nrolling merge的执行顺序如下：\n1. 读取一个包含了C1-Tree中叶子节点的multi-page block，将C1的一系列记录加载进缓存中\n> 这里的multi-page block就是前面的emptying block，因为这些数据会和C0的叶子节点进行合并。而合并的结果将会被保存在filling block中。\n2. 每次merge直接从缓存中以磁盘页的大小读取C1的叶子节点，将来自叶子节点的记录与从C0-Tree中拿到的叶子节点级的记录进行merge\n3. 在C1-Tree中创建保存merge结果的叶子节点，这些新的叶子节点会被放在C1的最右边\n4. 将新的节点刷入磁盘，这些信息会被放入新的磁盘位置，然后将C1的指针指向这个位置。\n\nCi树的变更如下：\n<img src=\"/img/202106/002.png\" width=\"50%\" height=\"50%\">\n\n从图中可以看到，C1与C0合并之后原有的旧节点会被删除。但是在实际合并过程中这些节点的删除时间在新的节点被刷入磁盘后，在合并的时候并不会被删除，这是为了保证在合并时这些数据依然能对外提供服务。以及在合并时发生异常后的恢复工作。直到后续新增的数据足够多使的涵盖这些block的checkpoint过期后才能回收。否则提前回收的化如果要将LSM-Tree恢复到这些block还生效的时间点，恢复就会失败。\n> 对于节点的删除，通常来说就是覆盖写。并不会执行物理删除，因为覆盖写的效率比随机写高很多 。\n\n由于C1与C0的叶子节点都是顺序排序，因此在C1读取到中间的时候可能就知道C0中已经没有节点可以进行merge了，但是为了保持C1中叶子节点顺序，每次rolling merge都需要将C1整个读取一遍。因此一次rolling merge结束的标志是读取到C1的最大索引段。\n> 这里的整个读取并不是说将C1一次性读取，而是将C1分批次读入内存中。\n\n虽然在写入的时候是批处理，但是在进行合并的时候是以单个索引节点为单位。在此时会发生的情况如下：\n1. 首先会创建一个新的基于C1的子节点组件\n2. 从emptying block获取C1的一个索引节点\n3. 从C0中读取小于该索引或者跟这个索引是连续的数据将其合并并放入filling block中\n4. 如果C0中有关于删除C1这条记录的删除操作，则将其直接抛弃。这也是LSM-Tree中的删除操作\n5. 当filling block满了之后(达到C1节点的大小阈值)将其刷入磁盘中\n\n每一次rolling merge，更大的组件中的数据量总是会发生变化，当它的组件大小超过阈值后也会触发rolling merge将其与下一个更大的组件合并。\n\n> 从我学习java的角度出发，这种方式有点类似于JVM的垃圾回收机制，C0就是新生代、C1以及之后的更大的节点就是S0与S1、最大的组件就是老年代。\n> 当然这有点牵强，因为数据在LSM-Tree的移动依据是创建时间，而对象在堆中的移动依据更大程度在于是否有被引用。\n\n## 组件大小估算\n估算组件大小主要影响因素有两个：磁盘IO开销与数据插入频率。具体推导过程[点击此链接](/2021/06/05/LSM-Tree开销与组件大小计算)\n\n## 如何处理并发\n在任何情况下，所有未被锁住的Ci组件的节点都是可以始终被访问的，磁盘访问也可以执行以定位内存中的节点，即使该节点是正在进行rolling merge的muti-page block的一部分。\n在考虑各种因素的情况下，LSM-Tree的并发要处理三个问题：\n1. 查询操作不能同时去访问另一个进程的rolling merge正在修改的磁盘组件的节点内容\n2. 针对C0组件的查询和插入操作也不能与正在进行的rolling merge的同时对树的相同部分进行访问\n3. 从C<sub>i-1</sub>到C<sub>i</sub>的rolling 的由表有时需要越过从Ci到Ci+1的rollinng merge的由表，因为数据从Ci-1移出速度 >= 从Ci移出的速率，这意味着Ci-1所关联的游标的循环周期要更快。因此无论如何，所采用的并发访问机制必须允许这种交错发生，而不能降至要求在交会点，一个进程必须阻塞在另一个进程之后。\n\n第一个和第二个问题可以概括为：正在merge过程中发生修改或者合并的结果，在合并完成前不能被查询与插入操作“看到”。第三个问题在于数据从C<sub>i-1</sub>移入C<sub>i</sub>与C<sub>i</sub>将数据移入C<sub>i+1</sub>时，两个游标在扫描的过程中会发生交叉，如果都是查询还好，但是C<sub>i-1</sub>移入的游标还负责将合并结果插入C<sub>i</sub>中，这就要求在合并完成前新插入的数据不能被C<sub>i</sub>到C<sub>i+1</sub>的rolling merge游标“看到”，更严格来说，就算C<sub>i-1</sub>的rolling merge完成，但是C<sub>i</sub>的rolling merge还没完成的情况下，它依然不能看到合并的新节点。\n\n节点是LSM-tree中用于避免基于磁盘的组件的并发访问冲突的加锁单位。正在因rolling merge而被更新的节点会被加上写锁，同时正在因查询而被读取的节点将会被加上读锁。有专门的目录机制用于解决死锁的问题。\n\n由于C<sub>0</sub>存在内存当中，因此它的数据结构并没有特殊的要求。在2-3树的情况下，我们可以用写锁锁住一个包含了merge到C1中的某个节点的受影响的边界内的所有记录的(2-3)树目录节点下的子树；同时用读锁锁住查找操作所涉及的查找路径上的所有节点，这种不同类型的访问就是互斥的。\n> 2-3树就是最简单的B树\n\n无论是在内存中还是在磁盘上，在LSM-Tree中的所有组件的节点都是顺序排列的，那么在C<sub>0</sub>属于2-3树的情况下，在选取每一批参与rolling merge的multi-page block的时候，可以在2-3树中选取一个节点，以该节点为root节点的子树都是本次要被rolling merge的节点。那么，可以在这个节点上加上写锁；同时，在查找的时候，所有涉及的节点都要被加上读锁。不同类型的锁是互斥的。\n当节点中的所有节点都已经参与rolling merge，且所有新节点都已经写入新的组件中之后才释放写锁；当被查找的叶子节点被扫描后就释放读锁，这样就能让一个更快的游标越过更慢的游标。\n\n假设正在执行两个磁盘组件之间的rolling merge，将记录从C<sub>i-1</sub>(又称为该rolling merge过程的内组件)移到C<sub>i</sub>(又称为该rolling merge过程的外组件)。\n游标在C<sub>i-1</sub>的叶级节点中具有明确定义的内部组件位置，指向它即将向外迁移到C<sub>i</sub>的下一个条目的位置，同时在C<sub>i-1</sub>的更高层的目录上沿着到达该叶子节点的访问路径都有一个对应位置。同时在C<sub>i</sub>组件的叶子节点与到达该叶子节点的访问路径中，该游标也有一个位置。在一次rolling merge过程中，游标会读取C<sub>i-1</sub>与C<sub>i</sub>中的待合并的节点，为了提供并发能力，通常这些节点都是批量地载入内存中并放置在emptying block中。当merge发生后会从emptying block中取出C<sub>i-1</sub>与C<sub>i</sub>的节点并执行合并，直到合并完成并刷入磁盘这段时间内都会对这些数据加上写锁。\n\n### 锁的释放\n* 读锁：一旦叶节点上的条目被扫描完了，就会释放\n* 写锁：滚动游标使用的写锁会在合并到更大的组件后背释放\n\n## checkpoing与恢复\n### checkpoint\n当新的记录被插入的LSM-Tree的C0组件后，它会在C<sub>0</sub>达到阈值后通过rolling merge刷入磁盘，在这期间系统发生异常并重启的话对于一些缓存在内存中的数据来说就是不安全的。\n因此LSM-Tree也面临一个经典的恢复问题：重构那些已经存在于内存中的而在系统crash丢失的内容。\n针对这些新记录的事务型插入日志已经被常态性地写出到一个顺序性的日志文件中，因此只需要简单地将这些插入日志(日志里包含了所有field的值插入记录所对应的RID)作为索引值恢复的基础即可。这种用于索引恢复的新方式必须内建到系统恢复算法中，同时它也会影响到对于这种事务型插入历史日志的存储空间回收时间，在恢复发生时回收会被延迟。\n对于LSM-Tree来说，需要准确定义checkpoint的格式以及如何确定从顺序型日志中的哪里开始往下读取并回放日志。\n在LSM-Tree中，在T<sub>0</sub>时刻要创建checkpoint，它会等待所有正在进行中的merge步骤，保证所有节点的写锁都被释放；然后将新纪录的插入延迟到checkpoint完成，此时没有C<sub>0</sub>中的读锁也不会被占用。然后通过以下步骤创建checkpoint：\n1. 将C<sub>0</sub>组件的内容写入到一个已知的磁盘位置；之后，针对C<sub>0</sub>组件的记录插入就又可以开始了，但是merge步骤将会继续被延迟\n2. 将所有磁盘组件在内存中的脏页flush到磁盘，这些其实就是磁盘组件位于内存的缓存数据。\n3. 创建一个具有如下信息的特殊的checkpoint日志：\n    1. 日志序列号（Log Sequence Number，LSN<sub>0</sub>），即T<sub>0</sub>时刻最后插入的索引行\n    2. 所有组件的根节点的磁盘地址\n    3. 上面这些组件中的所有merge游标的位置\n    4. 当前关于新的multi-page block的动态分配信息\n\n当将C<sub>0</sub>写入磁盘后，不需要等其他步骤完成就可以恢复针对C<sub>0</sub>的插入，因此在创建checkpoint的时候暂停时间并不会很长；同时也会影响到新插入到C<sub>0</sub>的记录被持久化到磁盘的时间会有所延迟。\n\n### 恢复\n当系统崩溃恢复后会执行以下操作：\n1. 在日志中定位一个检查点；\n2. 将之前写入硬盘的C<sub>0</sub>和其它组件在内存中缓存的multi-page block加载到内存中；\n3. 将日志中在LSN<sub>0</sub>之后的部分读入内存，执行其中索引条目的插入操作；\n4. 读取检查点日志中硬盘部件（C<sub>1</sub>~C<sub>k</sub>）的根的位置和合并游标，启动rolling merge，覆盖检查点之后的多页块\n5. 当检查点之后的所有新索引条目都已插入至LSM-tree且被索引后，恢复即完成。\n\n一次checkpoint恢复后可以做到：复原C<sub>0</sub>的数据、复原磁盘组件在内存中的缓存、复原所有磁盘组件正在进行的merge步骤。最后一点包含了：所有组件所有merge游标的位置、所有组件merge过程中的emptying block与filling block信息。\n\n在恢复过程中的一个细节是如何处理目录变化，在创建checkpoint的时候，当filling-page block满了并刷入内存后立刻更新磁盘目录树中的上级节点指针，使得所有刷入磁盘的目录数据都是merge完成后的新数据。当checkpoint创建后持久化到磁盘中的filling-page block很有可能未满，不过这没有关系，在恢复的时候会将这部分信息全部读入内存中，所要做的就是在filling-page block满了并写入磁盘后重新修改目录树的指针，将其指向这个磁盘位置即可。这个修改指针的操作实际上并没有多余的IO开销，他是随着merge的进行顺便修改。\n\n## 相关阅读\n1. 防止死锁发生的目录锁机制：[B树并发](https://link.springer.com/article/10.1007/BF00263762)\n2. 用于事务隔离的key range锁：[数据库系统的并发控制和恢复](https://sigmod.org/publications/dblp/db/books/dbtext/bernstein87.html)\n3. 关于phantom update的问题：Jim Gray and Andreas Reuter, \"Transaction Processing, Concepts and Techniques\",Morgan Kaufmann 1992.\n4. 关于垃圾收集的问题：[日志结构文件系统的设计与实现](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.117.5365&rep=rep1&type=pdf)","tags":["分布式存储"]},{"title":"结构优化-续","url":"/2021/05/29/结构优化-续/","content":"接上篇，上篇其实写的有些着急了，因此在按照方案修改的时候发现还有很多地方存在问题，除了一些显眼的代码复用修改外遇到的另外问题就是方法过长。下面通过一个`读取日志`函数的改进过程记录下自己对方法优化的方式。\n<!-- more -->\n## 正文\n废话不多说，先上原代码：\n```java\n    /**\n     * 读取日志\n     * 其中：\n     *      bucket.add：将路由节点加入路由表中\n     */\n    private void readLog1(String logPath) {\n        File file = new File(logPath);\n        if (!file.exists()) {\n            return;\n        }\n\n        var findEd = new HashSet<String>();\n        var list = new ArrayList<LayoutLog>();\n        try (var raf = new RandomAccessFile(file, \"r\")) {\n            if (raf.length() == 0) {\n                return;\n            }\n            long start = raf.getFilePointer();\n            long nextEnd = start + raf.length() - 1;\n            String result;\n            raf.seek(nextEnd);\n            int c = -1;\n            while (nextEnd >= start) {\n                c = raf.read();\n                if (c == '\\r' || c == '\\n') {\n                    result = raf.readLine();\n                    LayoutLog layoutLog = LogParser.parser.parseLayout(result);\n                    if (Objects.nonNull(layoutLog)) {\n                        if (bucket.contains(layoutLog.getNodeId())\n                                || findEd.contains(layoutLog.getNodeId())) {\n                            break;\n                        }\n\n                        findEd.add(layoutLog.getNodeId());\n                        list.add(layoutLog);\n                    }\n                    nextEnd -= 1;\n                }\n                nextEnd -= 1;\n                if (nextEnd >= 0) {\n                    raf.seek(nextEnd);\n                    if (nextEnd == 0) {\n                        LayoutLog layoutLog = LogParser.parser.parseLayout(raf.readLine());\n                        if (Objects.nonNull(layoutLog)) {\n                            if (bucket.contains(layoutLog.getNodeId())\n                                    || findEd.contains(layoutLog.getNodeId())) {\n                                break;\n                            }\n\n                            findEd.add(layoutLog.getNodeId());\n                            list.add(layoutLog);\n                        }\n                    }\n                }\n            }\n        } catch (IOException e) {\n            Logger.systemLog.error(LogFormat.SYSTEM_ERROR_FORMAT, \"读取路由节点操作日志文件失败\", e.getMessage());\n        }\n\n        list.forEach(node -> bucket.add(node.toNode()));\n    }\n```\n这个函数的作用在于从下往上按行读取日志记录并解析，然后将解析后的路由节点信息加入路由表中。\n\n存在的问题如下；\n* 这个函数位于`RouterLayout`类中，这个类不应该有直接对文件操作的动作。\n* 函数过长且可读性差，这个函数我在一周前写的，但现在需要一行一行重新阅读才能了解具体是如何操作的\n\n我对此的主要分析过程如下：\n\n首先，确定该函数具体需要执行哪些操作，将其绘制成流程图如下：\n<img src=\"/img/202105/013.png\" width=\"50%\" height=\"50%\">\n\n这样我知道了这个函数主要进行的操作，然后根据OOP理论将各个操作将其划分到对应的对象中：\n* 主流程肯定位于`Routerlayout`对象中\n* 读取文件的过程应该要有一个专门的`日志读取器`\n* 日志的解析也是一样，要有一个专门的`日志解析器`将其从字符串转换为对象\n\n同时观察其中的if判断逻辑，我第一时间想到的是通过`迭代器`进行日志文件的逐行获取，这个想法没问题！因此获得的`日志读取器`对象如下:\n```java\n    /**\n    * 日志阅读器\n    * 从日志的最后一行逐行往上读取日志\n    */\n    public class LogReader implements Iterator<String>, Closeable {\n\n        private final boolean exist;\n\n        private final RandomAccessFile raf;\n\n        private final long start;\n\n        private long nextEnd;\n\n        private String returnLine;\n\n\n        public static LogReader create(String logPath) throws IOException {\n            var file = new File(logPath);\n            return new LogReader(logPath, file.exists());\n        }\n\n        private LogReader(String logPath, boolean exist) throws IOException {\n            this.exist = exist;\n            this.raf = new RandomAccessFile(logPath, \"r\");\n            this.start = raf.getFilePointer();\n            this.nextEnd = start + raf.length() - 1;\n            this.returnLine = null;\n            if (nextEnd >= 0) {\n                raf.seek(nextEnd);\n            }\n        }\n\n        @Override\n        public boolean hasNext() {\n            if (!exist || nextEnd < start) {\n                return false;\n            }\n            try {\n                if (raf.length() == 0) {\n                    return false;\n                }\n                int c = -1;\n                while (nextEnd >= start) {\n                    c = raf.read();\n                    if (c == '\\r' || c == '\\n') {\n                        returnLine = raf.readLine();\n                        //越过换行符\n                        nextEnd -= 1;\n                        if (Objects.nonNull(returnLine))\n                            break;\n                    }\n                    //往上读取\n                    nextEnd -= 1;\n                    if (nextEnd >= 0) {\n                        raf.seek(nextEnd);\n                        if (nextEnd == 0) {\n                            returnLine = raf.readLine();\n                        }\n                    }\n                }\n            } catch (IOException e) {\n                Logger.systemLog.error(LogFormat.SYSTEM_ERROR_FORMAT, \"读取日志文件失败\", e.getMessage());\n            }\n            return Objects.nonNull(returnLine);\n        }\n\n        @Override\n        public String next() {\n            var result = returnLine;\n            returnLine = null;\n            return result;\n        }\n\n        @Override\n        public void close() throws IOException {\n            raf.close();\n        }\n    }\n```\n> 实现Closeable接口的原因是需要关闭文件的读取对象流。\n\n在使用的时候可以直接通过使用一般的迭代器一样使用日志读取器：\n```java\n    var reader = FileReader.create(logPath);\n    while (reader.hasNext()) {\n        var logLine = reader.next();\n    }\n```\n\n由于原来就有日志的读取器，因此只需要针对解析流程做一个简化即可，最终读取日志的函数修改如下：\n```java\n    /*\n     读取本地日志，将日志中的节点重新加入到路由表中\n     LOG_FILE = logPath\n     */\n    private void readLog() {\n        var logFile = new File(LOG_FILE);\n        if (!logFile.exists()) {\n            return;\n        }\n        try (var reader = LogReader.create(LOG_FILE)) {\n            while (reader.hasNext()) {\n                String logLine = reader.next();\n                var layout = LogParser.parser.parseLayout(logLine);\n                if (Objects.isNull(layout))\n                    continue;\n                if (bucket.contains(layout.getNodeId()))\n                    break;\n                bucket.add(layout.toNode());\n            }\n        } catch (IOException e) {\n            Logger.systemLog.error(LogFormat.SYSTEM_ERROR_FORMAT, \"读取日志出错\", e.getMessage());\n        }\n    }\n```\n瞬间清新了很多！\n\n## 总结\n在对函数进行瘦身的时候，首先需要划分出函数要执行的各个操作，并将各个操作按照类型进行`分区`，这里的`分区`到最后就是将其封装为各个对象。","tags":["杂谈"]},{"title":"记录一次结构改进经历","url":"/2021/05/24/记录一次结构改进经历/","content":"记录的是我的个人项目[Epidemic](https://github.com/dust1/epidemic)的改进经历，一个阶段写完之后发现问题还是很多，在有针对地画出UML图后就连我这种新手也能对一些问题一目了然。果然设计系统的前期准备工作还是不充分以及还是代码写的少。\n<!-- more -->\n## 原先情况\n在一个阶段写完后主要遇到有以下几个情况：\n1. 不同类之间存在相同元素与相同函数\n2. 对类的引用设计有问题，导致某个类需要使用另一个类的时候需要重新修改整个类的构造函数，并且在传递的过程中需要经过一两个中间类\n3. 对定时任务设计不理想，系统中有两种任务调度：定时任务与异步任务\n\n这是原先的UML图例：\n<img src=\"/img/202105/010.jpg\" width=\"100%\" height=\"100%\">\n\n\n### Layout相关\n可以发现，RouterLayout与StorageLayout之间还存在公共参数。同时定时任务只关联RouterLayout，导致StorageLayout需要执行异步或者定时任务的时候十分困难，在实际的实现中代码十分丑陋。\n\n同时对于最主要的两个基类RouterLayout与StorageLayout，其中有很多方法设计的不合理。比如RouterLayout的findFriend，该方法的含义为`判断路由节点是否为第一次启动，如果是则从配置文件中的关联节点获取集群中的与它接近的路由节点填充路由表`，这种定义的函数应该要在RouterLayout启动的时候就要执行。\n\n还有StorageLayout中的store函数，其中的参数是GRPCs中定义的Request，这就使得StorageLayout与RPC模块过于耦合，不利于模块的拆分，不符合软件设计的高内聚低耦合原则。跟这个情况类似的还有RouterLayout的ping函数。\n\nFileStorageLayout中的两个`RandomAccessFile`类型的参数，原本的意思是设置这两个参数减少文件写入过程中对目标文件的定位过程，但实际上已经有了一个writeName参数可以用于快速定位，所花费的只有对象创建的开销，而这个开销对比IO来说可以忽略不计；\n\ngetVersion与public参数version之间的冲突，两者的功能重复，需要删除一个；\n\nRouterLayout中的persistence()函数用于持久化，但在后续的设计中以及参考其他文件系统的实现中发现，持久化的操作不需要放在主对象中，而是最好放在守护进程中，减少对对象的占用。\n\nstrSizeToLong该方法与StorageLayout类的定义格格不入，不符合DDD原则。\n\nping函数的使用情况对于StorageLayout来说表示根据给入的新节点检查是否有距离该节点更近的文件，如果存在则将其放入Re-Publishing队列中等待定时推送；对于RouterLayout来说表示检查该节点是否已存在与当前路由表中，如果不存在则将其加入。两者虽然都是对一个ping过来的节点执行操作，但是实现的逻辑完全不同。因此需要将其拆分为两个不同的函数用于区分。或者将其重新命名成具有可识别性的新函数。\n\nRouterLayout中的SNAPSHOT_FILENAME参数表示路由表持久化的名称，这个参数不应该放在RouterLayout这种抽象基类中，因为不同的路由实现会有不同的持久化方式，为了兼容多种不同实现，需要将其下移。\n\n## 改进方法\n\n### Layout相关\n各种修改方法如下：\n1. 在StorageLayout与RouterLayout的基础上继续抽象一个抽象类BaseLayout\n2. 将StorageLayout与RouterLayout各自的storagePath与routerPath抽象为path参数并放置在BaseLayout中\n3. 将config参数上移到BaseLayout中\n4. 将version以及versionFileName上移到BaseLayout中，同时也意味着在BaseLayout中实现版本的校验\n5. 将StorageLayout与RouterLayout持久化相关的消息头上移到BaseLayout中\n6. 将isCompatibleVersion函数上移到BaseLayout中\n7. 将RouterLayout与StorageLayout中的ping函数删除并用新的haveNewNode替代，该方法存在BaseLayout中\n8. 删除FileStorageLayout中的两个RandomAccessFile参数\n9. 删除getVersion参数\n10. 删除RouterLayout的persistence函数\n11. 删除strSizeToLong函数，将其功能交给config实现\n12. 将RouterLayout的SNAPSHOT_FILENAME参数下移给其实现\n13. 删除StorageLayout与StorageLayout的load函数，其功能由BaseLayout的before函数替代。\n14. 删除StorageLayout的chunkSize参数，该参数的获取由config提供\n15. 将StorageLayout的find函数修改为findFile\n\n最终结构如下：\n<img src=\"/img/202105/011.png\" width=\"100%\" height=\"100%\">\n\n### Timer相关\n首先是对这一块功能进行分析，在系统中涉及到的主要有三个功能：\n1. 持久化路由节点\n2. 定时与路由表中的节点进行通信，确认其中有那些节点已经不可达\n3. 定时执行Re-Publishing操作，将文件推送给距离更近的节点\n\n其中第一点由于有到达一定数量就触发以及操作日志，因此并不需要将路由表完全持久化到磁盘，因为没有持久化到磁盘的也会被写入操作日志，在重启的时候对操作日志进行恢复即可。因此这一步操作可以放置在线程组中进行。\n\n第2与第3点可以作为两个守护线程分别执行。\n\n第2点需要获取RouterLayout的路由表信息。第3点需要获取StorageLayout的待Re-Publishing文件列表。虽然都需要对应Layout的数据，但是两者确实完全不同的内在。对于第2条来说，执行检查的时候需要获取到当前RouterLayout维护的整张路由表的备份然后进行操作，每一次操作总是要获取到一次完整数据；而对于第3点来说，只需要满足Re-Publishing条件的文件信息即可，并不需要每次都读取全部文件信息。因此这两者的实现是完全不一样的。\n* 第2点：需要获取到RouterLayout的对象引用，并在合适的时候获取路由表的复制。\n* 第3点：需要建立与StorageLayout的单项通信队列，当要执行Re-Publishing的时候从队列中获取满足条件的文件信息即可\n\n对于RouterLayout来说实际维护路由表的类是KademliaBucket，因此定时任务调用的也应该是这个对象。\n\n### 最终情况如下\n<img src=\"/img/202105/012.jpg\" width=\"100%\" height=\"100%\">\n\n恩，比原来清晰很多，接下里就是按照这个修改代码了。","tags":["杂谈"]},{"title":"epidemic-一个P2P存储系统","url":"/2021/05/23/epidemic-一个P2P存储系统/","content":"# 构建网络拓扑\nEpidemic采用Kademlia协议来构建网路拓扑，通过在配置文件中设置的随机字符串（可选）生成一个160位的二进制数作为网络节点的Id。\n两个节点之间的距离通过他们各自的id进行异或（XOR）计算，定义两个节点之间的距离为两个id之间前缀相同的位数。\n> Dis(M, N) = XOR(M, N)\n<!-- more -->\n由此可以知道节点之间距离越近其公共前缀越长。\n> 这里的距离表示的是Id之间的距离，并不是节点之间实际的网络距离。\n\n基于此，我们可以构建出一棵前缀树用于记录并管理所有的节点信息：\n<img src=\"/img/202105/009.png\" width=\"50%\" height=\"50%\">\n\n与这个相关的文章链接在[这里](https://zhuanlan.zhihu.com/p/38425656)\n\n# 节点路由\n由Kademlia协议可以得知，任意两个节点之间的距离的范围为[0, 160]。当距离为160的时候两个节点相同，因此对于一个存储节点来说，可以维护一个长度为160的bucket数组，数组的下标就是其他节点与自身节点的距离，bucket本身则是一个固定长度的集合用于保存网络节点，其中的网络节点满足他们与本地节点的距离大于等于所在下标。\n\n从id的生成策略可知一共可以生成2^160个不同的id，如果要让一个存储节点维护所有的id显然是非常不现实的。因此节点有一个配置参数（BucketKey）用于表示每个Bucket维护多少个节点，每当超过这个数值之后会将新的节点加入一个有界队列的缓存中。\n\n缓存是一个实现了LRU的有界队列，它的大小也是BucketKey，当超过这个数值后会将队列末尾的节点丢弃。\n\n## Bucket分裂：\n在具体的实现中一开始只有1个Bucket，它的下标为0表示其中维护的所有节点与本地节点的距离大于等于0。当集合中节点数量超过大小最大后进行Bucket分裂，分裂的实现为创建一个新的Bucket并将原本Bucket中的所有与本地节点距离大于0的节点放入新的集合中。在分裂之后的2个Bucket中，下标为0的Bucket集合中所有节点与本地节点的距离都为0，下标为1的Bucket集合中的所有节点与本地节点的距离都大于等于1.\n\n## 路由算法：\n该算法的核心在于如何只根据目标id在集群中找到它的地址或者与该id最接近的目标节点地址。\n在一个对等网络中，某节点要查询其他节点的信息时，它能依赖的信息有两个：\n\t1. 目标节点id\n\t2. 当前节点所维护的路由表\n由于每个节点不能维护一张涵盖所有节点的路由表，因此对于寻找目标节点的核心思想是：逐步迭代、递进查找。\n\n# 节点变更\n## 节点上线：\n当一个新节点上线后需要给它一个一存在的节点地址作为接入对等网络的“联系人”。当新节点启动后通过“联系人”调用查询节点函数，查询的目标节点是它自身的节点，“联系人”返回距离新节点最近的节点列表后重复查询操作，直到新节点的Bucket数量超过2或者某次返回的节点已经全部位于Bucket中为止。\n\n收到查询操作的“联系人”节点也能得知有新节点上线，并将这个新节点加入自身的路由表管理。\n\n## 节点离线：\n节点离线并不会触发特殊操作。对于其他节点来说，有一个定时任务每经过一定时间就执行ping操作用于检查Bucket中无法通信的节点，如果某个节点无法通信它会被从bucket集合中被删除，并从节点缓存队列中查询是否有与当前节点距离相同的缓存节点，如果存在则从缓存中取出并加入bucket中。\n\n# 对象存储\n在对等网络中实现对象存储需要面对两个问题：\n\t1. 如何建立对象与网络节点之间的映射\n    2. 如何保证节点动态变化时保证对象的可访问\n## 对象与节点的映射：\n节点与对象的映射有一种办法就是建立一张节点与对象之间的映射表，但是这样这个映射表将会变得庞大并成为系统瓶颈，违背了对等网络的原则。\n\n第二种方法就是通过算法计算文件的特征码，并且这个特征码的长度为160位与节点ID对应，将文件存入与它的特征码最接近的节点中。\n> 这个特征码还能作为文件的完整性校验码。\n\n## 对象Re-Publishing：\n在节点的动态变化下，会产生一下两个问题：\n1. 对象<key, value>被存储在与key距离最接近的K个节点，如果K个节点全部离线，那么对象便不可达；\n2. 对象<key, value>被存储在与key距离最接近的K个节点，如果集群新加入一个节点N，对象距离N更近，需要对象迁移到N。\n针对上诉情况的一个解决方案是：当一个新节点加入并被其他节点感知到后，如果节点检测到自身所保存的某些文件距离新节点更近，则将该文件推送到新节点。这个操作叫做Re-Publishing。\n> 感知过程通常是新节点加入后通过“联系人”从集群中获取节点丰富自己路由表的时候。\n\n由于集群网络中节点每时每刻都在改变，如果每次新节点加入都进行一次数据拷贝，则集群中大量的流量都会被用来进行文件复制而不是提供客户端使用。\n\n我们可以通过定时执行这个操作来减少对网络中流量的占用。在Kademlia作者的论文中推荐的时间为1小时，每经过1小时对网络中的每个对象执行一次Re-Publishing。每一次Re-Publishing包括两个步骤：\n1. 查询当前最近的K个节点信息\n2. 节点向上一步获得的节点发送数据存储消息，从而完成数据更新\n\n\n这样做会导致短时间内网络流量激增，通过对上诉行为的分析可以知道有很多操作其实是不需要的。\n1. 如果1小时内网络拓扑并没有变化则不需要进行Re-Publishing\n2. 对于K个节点，其中如果已存在要Re-Publishing的对象，则不需要进行Re-Publishing\n3. 新节点加入的时候与他相临的其他节点都可以收到通知，并不需要重新进行一轮查询\n\n# 相关链接：\n[epidemic](https://github.com/dust1/epidemic)","tags":["存储"]},{"title":"为什么磁盘采用4KB的页","url":"/2021/05/09/为什么磁盘采用4KB的页/","content":"## 前置概念:\n\n页表是一种特殊的数据结构，放在系统空间的页表区，存放逻辑页与物理页帧的对应关系。 每一个进程都拥有一个自己的页表，PCB表中有指针指向页表。\n<!-- more -->\n<center>\n<img src=\"/img/202105/001.png\" width=\"50%\" height=\"50%\">\n</center>\n\n逻辑地址转换成物理地址的过程是：用页号p去检索页表，从页表中得到该页的物理块号，把它装入物理地址寄存器中。同时，将页内地址d直接送入物理地址寄存器的块内地址字段中。这样，物理地址寄存器中的内容就是由二者拼接成的实际访问内存的地址，从而完成了从逻辑地址到物理地址的转换。\n\n用固定大小的页(Page)来描述逻辑地址空间，用相同大小的页框(Frame)来描述物理内存空间，由操作系统实现从逻辑页到物理页框的页面映射，同时负责对所有页的管理和进程运行的控制。\n\n## 正文\n\n在使用4KB作为磁盘的分页前，大部分磁盘驱动器厂商采用的是512B的分页。美国国家半导体32016机器上的页大小为512字节，对于16MB内存大小的计算机来说可以处理32768个页面。即使每个页面只需要16字节的开销，整体就需要512KB的内存空间，即内存的1/32。\n\n对于操作系统来说，现在Linux每逻辑分页的开销为64字节，当然，在32位系统中linux对每页的开销为32字节。\n\n4KB磁盘驱动已经在90年代就已经出现了。但是在2009年，西部数据才正式主推4KB的磁盘分页驱动器。\n\n在90年代，人们发现随着磁盘驱动器容量的扩大512字节领域的意义越来越小，在以MB为单位统计磁盘大小的时候512字节是在技术因素与保持最小浪费空间中所达成的良好平衡。\n\n在进行硬盘驱动器设计时需要平衡三个因素：面密度、读取驱动器盘片时的信噪比（SNR）以及错误检查和纠正（ECC）。当面密度增加，扇区变小，其SNR降低。为了保持可靠性，需要对ECC进行改进，而改进的方式就是使用更多的位数。\n\n<center>\n<img src=\"/img/202105/002.png\" width=\"50%\" height=\"50%\">\n各种面密度的SNR\n</center>\n\n在这种时候，磁盘驱动商们需要往面密度中添加尽可能多的ECC，否则无法保证数据的可靠性。因此，提高ECC校验块的大小就成了最好的解决办法。对于大块，ECC在更大的数据块上使用效率更高，虽然对于单个大块比在512上需要的空间更多，但是平均下来的空间利用率比512要小得多。\n<center>\n<img src=\"/img/202105/003.png\" width=\"50%\" height=\"50%\">\n</center>\n\n对于4K扇区来说需要100字节的ECC数据，而对于8位操作系统上的512扇区来说需要320字节的数据（40 x 8）。而且随着数据量的增多，每次发生错误的错误数据规模也在增加，对于4KB的错误数据来说，需要擦除8个512规模的扇区，而4KB扇区只需要擦除一个。\n\n### 为什么选4KB而不是2KB或者8KB呢？\n有一个因素是在x86处理器上正常内存页面也为4KB（当然也有4MB）。x86页面大小又导致文件系统簇（文件系统中最小的存储单元）变为4KB，因为4KB簇恰好适合内存页面。在数据总量增长的同时，对于小文件以及小的簇的需求也在逐渐减少，即小于4KB的文件少，浪费空间。这也导致NTFS，EXT3和HFS+在现代磁盘驱动器上均默认4KB簇，因此4KB物理扇区与4KB逻辑簇完美映射，而4KB文件系统与4KB内存页面完美映射。因此，目前4KB是磁盘驱动器的完美分页大小。\n<center>\n<img src=\"/img/202105/004.png\" width=\"50%\" height=\"50%\">\n</center>\n\n采用4KB扇区的另一个好处除了适应现代操作系统以外还能够支持磁盘驱动商设计出更大的磁盘。Western Digital据此可以轻松设计超过2TB的驱动器。\n\n从理论上说Western Digital估计4K扇区的使用将使它们在格式化效率上立即提高7％-11％。ECC突发纠错能力有望提高50％，总体错误率能力提高2个数量级。\n<center>\n<img src=\"/img/202105/005.png\" width=\"50%\" height=\"50%\">\n</center>\n\n但是4KB并不能立即全面推广，因为并不是所有的计算机都能支持4KB规模，因此需要有一个过渡期使用512仿真技术，该技术可以使高级格式驱动器（4KB）以具有512B扇区的形式向驱动器控制器和操作系统公开，而实际上它们将具有4K扇区。\n<center>\n<img src=\"/img/202105/006.png\" width=\"50%\" height=\"50%\">\n</center>\n\n但是这样做会有一个缺点：512B的数据与4KB的数据会有错位。其中的原因在于除数据本身外还带有数据对应的ECC数据，这会导致一个逻辑簇中最终将跨越4KB，这将会导致性能问题，其中最受影响的是随机写入，因为随机写入过程中需要对磁盘进行寻址计算同时还要对数据按分区计算ECC。\n\n<center>\n<img src=\"/img/202105/007.png\" width=\"50%\" height=\"50%\">\n</center>\n\n因此，对于一个读-修改-写（RMW）采用更新往往比采用直接写效率要高。（这与写入SSD的脏块类似）\n\n<center>\n<img src=\"/img/202105/008.png\" width=\"50%\" height=\"50%\">\n</center>\n\n## 扩展阅读\n[分区存储](https://zonedstorage.io/introduction/zoned-storage/)","tags":["杂谈"]},{"title":"一次使用grpc的记录","url":"/2021/04/26/一次使用grpc的记录/","content":"最近在设计一个P2P分布式存储系统，网络路由方面采用的是Kalelima协议，在这个协议的实现中主要有四个RPCs：\n* Ping：节点发起ping请求，主要用来检测与其他节点网络连接是否通顺\n* Store：存储请求，参数为<key, value>键值对，key是value的SHA1值，value是要保存的数据。\n* FindNode：寻找节点，参数是一个节点SHA1值，返回的是一个三元组<ip, 端口, 节点id>集合\n* FindValue：寻找值，参数是一个文件的SHA1值，如果不存在该文件，则返回距离这个文件最近的一个三元组<ip, 端口, 节点id>集合，如果存在文件，则直接返回文件的数据内容\n<!-- more -->\n我对.proto的设计如下：\n```proto\nsyntax = \"proto3\";\n\noption java_multiple_files = true;\noption java_package = \"com.dust.grpc.kalelima\";\noption java_outer_classname = \"KalelimaServiceProto\";\n\n// 定义的一个服务\nservice KalelimaService {\n  // Ping函数\n  rpc Ping(PingPackage) returns (PingPackage) {}\n\n  // 存储\n  rpc Store(StoreRequest) returns (StoreResponse) {}\n\n  // 寻找节点\n  rpc FindNode(FindRequest) returns (stream FindNodeResponse) {}\n\n  // 寻找值\n  rpc findValue(FindRequest) returns (stream FindValueResponse) {}\n}\n\n//ping函数发起的请求，携带发起者的id以及发起时间，收到ping之后的节点将id修改成自身id，时间不变原样返回\nmessage PingPackage {\n    string nodeId = 1;\n    int32 timestamp = 2;\n}\n\n//存储请求，携带有一个sha1编码的key和数据对象。其中key还是该对象的checksum\nmessage StoreRequest {\n    string key = 1;\n    bytes data = 2;\n}\n\n//存储响应，code = 1表示成功；code = 0表示失败。当code = 0的时候errmsg才会有值\nmessage StoreResponse {\n    int32 code = 1;\n    string errmsg = 2;\n}\n\n//寻找请求，key既可以表示节点id，又可以表示文件id。具体根据请求的接口来判断\nmessage FindRequest {\n    string key = 1;\n}\n\n//寻找节点返回，返回的是一个三元组<IP地址、端口、节点id>列表\nmessage FindNodeResponse {\n    string host = 1;\n    int32 port = 2;\n    string nodeId = 3;\n}\n\n//寻找文件呢返回，如果文件不存在则返回离这个文件最近的三元组<IP地址、端口、节点id>列表，否则直接返回值\nmessage FindValueResponse {\n    string host = 1;\n    int32 port = 2;\n    string nodeId = 3;\n\n    bytes data = 4;\n}\n```\n\n然后在生成代码的时候问题来了，这里官方会给出两种生成方式：1、使用protoc编译器生成；2、使用maven插件生成\n\n一开始我觉得改xml麻烦就直接通过protoc生成，生成的结果让我有点疑惑：\n<img src=\"/img/202104/025.png\" width=\"50%\" height=\"50%\">\n我的Service呢？我这么大的一个Service呢？其中的`KalelimaServiceProto`并不是一个服务文件的接口，而是类似于描述文件的东西。\n\n百度之后发现是缺少了一行命令`option java_generic_services = true;`加上后终于有Service了。\n<img src=\"/img/202104/026.png\" width=\"50%\" height=\"50%\">\n\n看着也没问题，是一个抽象类，其中定义了.proto文件中定义的抽象方法。好的那就去实现它吧！\n<img src=\"/img/202104/027.png\" width=\"50%\" height=\"50%\">\n\n实现之后问题来了，我该如何将这个服务注册在服务端中呢？查询了一些文档发现许多教程都是通过maven生成的代码，那我用maven生成代码中创建服务端的部分将这个Service注册进去可以吗？\n\n不可以！虽然grpc有addService方法，但是他们两个传入的Service和protoc生成的Service完全不是一个接口：\n```java\npublic interface Service {\n    ServiceDescriptor getDescriptorForType();\n\n    void callMethod(MethodDescriptor var1, RpcController var2, Message var3, RpcCallback<Message> var4);\n\n    Message getRequestPrototype(MethodDescriptor var1);\n\n    Message getResponsePrototype(MethodDescriptor var1);\n}\n```\n```java\npublic interface BindableService {\n    ServerServiceDefinition bindService();\n}\n```\n甚至连jar包都不是同一个！\n\n查询了protobuf的官方文档后发现，proto和grpc应该是这样的一种关系：\n<img src=\"/img/202104/028.png\" width=\"50%\" height=\"50%\">\nprotobuf只是grpc的一小部分，负责传输协议，而不是RPCs！\n\n因此如果只要grpc的话需要自己手动实现自己的RPC。\n<img src=\"/img/202104/029.png\" width=\"50%\" height=\"50%\">\n为了这个我浪费了快3小时。\n\n最后还是老老实实用grpc，至于自己实现RPCs，就要等到后面一个版本了，将Vertx与protobuf结合感觉不要太香。","tags":["grpc"]},{"title":"分布式存储笔记3-5：分布式数据库系统（Google Spanner）","url":"/2021/04/19/分布式存储笔记3-5：分布式数据库系统（Google-Spanner）/","content":"<hr>\n这是一篇在阅读《大规模分布式存储系统：原理解析与架构实战》时的阅读笔记，由于长时间碎片阅读的关系导致在做这种读书笔记的时候接近复制粘贴。虽然其中会有一小部分自己的想法但都十分零碎，希望后续能改进。\n<hr>\n\nGoogle spanner是谷歌的全球级分布式数据库。Spanner的扩展性达到了全球级，可以扩展到数百个数据中心，数百万台机器，上万亿行记录。除了夸张的可扩展性之外，他还能通过同步复制和多版本控制来满足外部一致性，支持跨数据中心事务。\n<!-- more -->\nGoogle Spanner的成功表示分布式技术能够给用户呈现关系数据库的数据模型.\n\n## 数据模型：\nSpanner的数据模型与Megastore系统比较类似。\n<img src=\"/img/202104/022.png\" width=\"50%\" height=\"50%\">\n如图所示，对于一个典型的相册应用，需要存储其用户和相册，可以用上面的两个SQL语句来创建表。\n\nSpanner的表是层次化的，最底层的表是目录表（Directory table），其他表创建时，可以用INTERLEAVE IN PARENT来表示层次关系。\n> 图中所示，Users是Directory的上层，Albums是Users的上层\n\nSpanner中的目录相当于Megastore中的实体组，一个用户的信息（user_id, email）以及这个用户下的所有相片构成一个目录。\n\n实际存储时，Spanner会将同一个目录的数据存放到一起，只要目录不太大，同一个目录的每个副本都会分配到同一台机器。因此，针对同一个目录的读写事务大部分情况下都不会涉及跨机操作。\n\n## 架构：\nSpanner与Bigtable、Megastore不同，它是构建在新的分布式文件系统Colossus之上。相比GFS，Colossus主要改进点在于实时性，并且支持海量小文件。\n\n> GFS、Bigtable、Megastore之间的不同：\n> * GFS：构建了分布式存储的底层数据、文件存储。\n> * Bigtable：在基础的数据存储基础上抽象了“表格”的概念，对数据进行一个初步的分类汇总\n> * Megastore：在Bigtable的基础上，即在“表格”的基础上更近一层提供了“对象组”概念，将表格抽象为现实事务中能够存在的对象。\n\n由于Spanner是全球性的，因此它有两个其他分布式存储系统没有的概念：\n* Universe：一个Spanner部署实例称为一个Ubiverse。目前全球有3个，一个开发、一个测试、一个线上。Universe支持多数据中心部署，且多个业务可以共享同一个Universe。\n* Zones：每个Zone属于一个数据中心，而一个数据中心可能有多个Zone。一般来说，Zone内部的网络通信代价较低，而Zone与Zone之间通信代价很高。\n> 前面说过，谷歌的服务器网络与传统的三层式网络不同，它是扁平化拓扑结构，即三级CLOS网络，同一个集群内最多支持20480台服务器。也就是说一个Zone就是一个三级CLOS网络集群。\n* Universe Master：监控这个Universe里Zone级别的状态信息\n* Placement Deiver：提供跨Zone数据迁移功能\n* Location Proxy：提供获取数据的位置信息服务。客户端需要通过它才能够知道数据由哪台Spanserver服务\n* Spanserver：提供存储服务，功能相当于Bigtable的Tablet Server。\n\n每个Spanserver会服务多个子表，而每个子表又包含多个目录。客户端往Spanner发送读写请求时，首先查找目录所在的Spanserver，接着从Spanserver读写数据。\n<img src=\"/img/202104/023.png\" width=\"50%\" height=\"50%\">\n\nQ：如何存储目录与Spanserver之间的映射关系？\n\nA：假设每个用户对应一个目录，全球共有50亿用户，这在单台机器上是无法保存的，这里谷歌没有说明如何做到，但猜测是将映射关系这样的元数据当成元数据表格，和普通用户表格采取相同存储的方式。\n> 把映射关系存入表格中，这样就从“如何存储目录与Spannserver之间的映射关系”这个问题转变为“如何在分布式表格系统中存入大量数据”。而针对问题2，Bigtable中有成熟清晰的做法。将用户id划分为多个范围分别存储，并维护用户id范围与表格之间的映射。\n\n### 复制与一致性：\n<img src=\"/img/202104/024.png\" width=\"50%\" height=\"50%\">\n如图所示，每个数据中心运行着一套Colossus，每个机器有100～1000个子表，每个子表会在多个数据中心部署多个副本。为了同步操作系统中的操作日志，每个子表上会运行一个Paxos状态机。\n\nPaxos协议会选出一个副本作为主副本，这个主副本的寿命默认是10秒。\n\n正常情况下，主副本快要到期的时候会主动将其再次选为主副本，如果出现异常，在10秒过后其他副本会开启一轮新的选举。\n\n通过Paxos协议，实现了跨数据中心的多个副本之间的一致性。\n\n每个主副本所在的spanserver还会实现一个锁表用于并发控制，读写事务操作某个子表上的目录时需要通过锁表避免多个事务之间互相干扰。\n\n除了锁表，每个主副本上还有一个事务管理器。如果事务在一个Paxos组里面，可以绕过事务管理器。然是一旦事务跨多个Paxos组，就需要事务管理器来协调。\n\n锁表实现单个Paxos组内的单机事务，事务管理器实现跨多个Paxos组的分布式事务。为了实现分布式事务，采用的是两阶段提交协议，此时有一个Paxos组的主副本为协调者，其他Paxos组的主副本为参与者。\n\n### TrueTime：\n为了实现并发控制，数据库需要给每一个事务分配一个全局唯一的事务id。\n\n但是在分布式系统中很难做到全局唯一，常用的办法就是采用单台服务器用于专门生成全局唯一id的作用。例如Google Percolator（Google Caffeine的底层存储系统呢）中的做法的，专门部署一套Oracle数据库用于生成全局唯一id。虽然Oracle逻辑上是个单点，但是他功能单一，因而能够高效完成任务。\n\nSpanner选择了另一种做法：全球时钟同步机制TrueTime。\n\nTrueTime是一个提供本地时间的接口，但与Linux上的gettimeofday接口不一样的是：它出了返回一个时间戳t，还会给出一个误差e。例如：返回的时间戳是20:23:30:100，而误差是5ms，那么真实的时间在20:23:30:95～106之间。真实的系统e平均下来只有4毫秒。\n\nTrueTimeAPI实现的基础是GPS和原子钟。之所以要用两种技术来处理，是因为原子钟很稳定，当GPS失灵的时候，原子钟仍然能够保证在相当长的时间内，不会出现偏差。\n\n> 原子钟就是通过几个校准时间的域名获取准确的时间。这跟前面那个假设一台单点的全局唯一id服务器好像。只不过把这个工作交给了专业机构。\n\n每个数据中心都需要部署一些主时钟服务器（Master），其他机器上部署一个从时钟进程（Slave）来从主时钟服务器同步时钟信息。有的主时钟用GPS，有的主时钟服务器用原子钟。\n\nSlave在同步的时候根据多数派读的规则进行，通过对比多台主时钟服务器返回的结果来确定本地的时钟。\n> 采用时钟的好处：主时钟服务器不会有数据积存，对硬盘的依赖程度低，性能高。因为处理每个请求都在内存中完成，不需要有IO的读写操作。\n\n### 并发控制：\nSpanner使用TrueTime来控制并发，实现外部一致性，支持一下几种事务：\n* 读写事务\n* 只读事务\n* 快照读，客户端提供时间戳\n* 快照读，客户端提供时间范围\n\n#### 不考虑TrueTime\n即不考虑TrueTime的误差，假设TrueTimeAPI返回的时间都是精准的。如果事务读写的数据只属于同一个Paxos组，那么，\n\n每个读写事务的执行步骤如下：\n    a. 获取系统当前时间戳\n    b. 执行读写操作，并将第1步取得的时间戳作为事务的提交版本\n\n每个只读事务的执行步骤如下：\n    a. 获取系统当前时间戳，作为读事务的版本\n    b. 执行读取操作，返回客户端所有提交版本小于读事务版本的事务操作结果\n\n> 读写事务需要对数据进行修改，因此需要在事务内部对数据进行读写。如果是读取操作，则返回的数据也是当前事务读取出来的结果\n\n> 只读事务不涉及写操作，因此只需要返回最后一个成功的事务的数据状态即可，因为它自身不会对数据进行修改，所以不需要让只读事务本身再去重新对数据进行扫描读取。\n\n快照读和只读事务的区别在于：快照读将指定读取事务的版本，而不是取系统当前时间戳。\n\n如果事务读别的数据涉及多个Paxos组，那么事务就扩大为分布式事务，需要使用两阶段提交协议。执行步骤如下：\n1. Prepare：客户端将数据发往多个Paxos组的主副本，同时，协调者主副本发起prepare协议，请求其他的参与者主副本锁住需要操作的数据。\n2. Commit：协调者主副本发起Commit协议，要求每个参与者主副本执行提交操作并解除prepare阶段锁定的数据。协调者主副本可以将它的当前时间戳作为该事务的提交版本，并发送给每个参与者主副本。\n\n只读事务读取每个Paxos组中提交版本小于读事务版本的事务操作结果。\n\n只读事务需要保证不会读取到不完整的数据，也就是说，不会读取到尚未提交的事务。\n\n例如：只读事务读取到了两个Paxos组的事务A和B，其中A已经提交，但是B尚未提交还处于prepare阶段。只读事务会等待B协调者发起commit请求后再读取。\n\n#### 考虑TrueTime\n问题在于，只要事务T1的提交操作早于事务T2的开始操作，即使考虑TrueTimeAPI的误差因素（-e到+e之间，e的平均值为4ms），Spanner也能保证事务T1的提交版本小于事务T2的提交版本。\n\nSpanner使用了一种被称为延迟提交（Commit Wait）的手段，即如果事务T1的提交版本为时间戳t<sub>commit</sub>，那么，事务T1会在t<sub>commit</sub>+e之后才能提交。如果事务T2开始的绝对时间为t<sub>abs</sub>。那么，事务T2的提交版本至少为t<sub>abs</sub>+e。这样，就保证了事务T1和T2之间严格的顺序，这也意味着每个写事务的延时至少为2e。\n>  这里是考虑不同机器之间TrueTime的误差，由于TrueTimeAPI在调用的时候会产生误差e，因此为了确保不同服务器上的事务执行顺序的准确性，需要在执行过程中将误差给计算进去。这里主要的想法就是宁愿事务提交的晚也不要过早提交导致顺序错误。通过在事务完成后再延时2e的时间将这个误差给算进事务，使得其他服务器虽然事务执行需要在其后，但是读取的TrueTime在其前的情况下也能保证最终事务执行的顺序。\n\n>  例子：有两个事务T1、T2.执行顺序是T1-T2.两者同时从TrueTimeAPI中获取时间戳：t1、t2.由于会有误差，假设t2 < t1，如果按照这个时间戳来计算，T2的提交顺序将会在T1之前，T2的提交需要往后延时2e时间，保证在误差范围内T1优先执行。\n\nSpanner实现功能完备的全球数据库付出了一定代价。\n\n### 数据迁移：\n目录是Spanner中对数据分区、复制和迁移的基本单位，用户可以指定一个目录有多少副本，分别存放在哪些机房中，例如将用户的目录存放在这个用户所在地区附近的几个机房中。\n\n一个Paxos组包含多个目录，目录可以在Paxos组之间移动。Spanner移动一个目录一般处于以下几种考虑：\n* 某个Paxos组的负载太大，需要切分；\n* 将数据移动到离用户更近的地方，减少访问延时\n* 把经常一起访问的目录放进同一个Paxos组\n\n移动一个50MB的目录大约需要几秒钟时间。实现时，首先将目录的实际数据移动到指定位置，然后再用一个院子操作更新元数据没从而完成整个移动过程。\n>  这个过程如果中途出现意外，那么需要保证原有元数据依旧能够提供访问，所以数据的移动首先是将数据复制一份，而不是“剪切”。\n\n## 讨论：\n\n谷歌的分布式存储系统从Bigtable-Megastore-Spanner。底层通过分布式技术实现可扩展性，上层通过关系数据库的模型和接口将系统的功能暴露给用户。\n\n体现了分布式技术与传统数据库技术融合的必然性。\n\n","tags":["存储"]},{"title":"分布式存储笔记3-5：分布式数据库系统（Microsoft SQL Azure）","url":"/2021/04/16/分布式存储笔记3-5：分布式数据库系统（Microsoft-SQL-Azure）/","content":"<hr>\n这是一篇在阅读《大规模分布式存储系统：原理解析与架构实战》时的阅读笔记，由于长时间碎片阅读的关系导致在做这种读书笔记的时候接近复制粘贴。虽然其中会有一小部分自己的想法但都十分零碎，希望后续能改进。\n<hr>\n\nMicrosoft SQL Azure是微软的云关系型数据库，后端存储又称为云SQL Server。它构建在SQL Server之上，通过分布式技术提升传统关系型数据库的可扩展性和容错能力。\n<!-- more -->\n## 数据模型\n### 逻辑模型\n云SQL Server将数据划分为多个分区，通过限制事务只能在一个分区执行来规避分布式事务。并通过主备复制（Primary-Copy）协议将数据复制到多个副本保证高可用。\n\n云SQL Server中一个逻辑数据库称为表格组（table group），它既可以是有主键的也可以是无主键的。\n\n如果一个表格组是有主键的，要求表格组中的所有表格都要有一个相同的列，称为划分主键（partitioning key）。划分主键可以当成表格主键使用，但不是强制的要求。如下所示：\n<img src=\"/img/202104/018.png\" width=\"50%\" height=\"50%\">\n上图两张表中【Id】是划分主键，且在Customers表中属于表的主键，而在Orders中不是，Orders表的主键为组合主键<Id, Oid>(<顾客Id，订单Id>)。\n\n> 表格组就类似于Bigtable的对象组，即一个表格组是为某张表为主服务的，其他的表都属于这张主表的从表，抽象成一个对象来说，主表表示对象本体，从表表示对象附带的属性。因此图中的表格组的划分主键为Customers的Id（顾客Id）。其他所有的表都必须要有一个同样的字段。Customers与其他表之间的关系为has one或者has many。\n\n如果表格组是有主键的，云SQL Server支持自动地水平拆分表格组并分散到整个集群。同一个行组总是被一台物理地SQL Server服务，从而避免分布式事务。\n> 一个行组就相当于一个对象与和它相关地所有属性。这样拆分避免了与某个对象相关的事务是分布式事务，但由于一个对象由单机提供服务，当对象的数据量过大时又会出现“数据倾斜”问题。\n\n这样的好处是避免了分布式事务的两个问题：阻塞及性能；缺点是限制了用户的使用模式。只读事务可以跨多个行组，但事务隔离级别最多支持读取已提交（read-committed）\n> RC：在一个事务中对同一个数据的读取会有两种或多种情况，这是因为事务执行过程中其他事务提交后对数据的修改被当前事务可见造成的。这就表示了分布式的只读事务会受到其他事务对数据提交修改的影响。\n\n> 但这样的划分方式依旧避免不了查询多个用户时跨表、跨集群的情况\n\n### 物理模型：\n在物理层面，每个有主键的表格组根据划分主键列有序地分成多个数据分区（partition）。这些分区之间互相不重叠，并且覆盖了所有划分主键值。确保每一行组属于一个唯一分区。\n\n分区是云SQL Server复制、迁移、负载均衡的基本单位。\n> 物理层面的数据分区，能够通过程序设定保存的物理分区吗？这样的话要涉及到驱动或层面了吧。\n\n每个分区包含多个副本（默认为3），每个副本存储在一台物理的SQL Server上。由于每个行组属于一个分区，这就意味着每个行组的数据量不能超过分区允许的最大值，即单台SQL Server的容量上限。\n> 虽然在逻辑层面允许行组的数据量能够达到程序的理论上限，但是在物理层面由于行组被限定在数据分区里，因此行组的实际大小不能超过分区允许的最大容量。\n\n一般来说，同一个交换机或者同一个机架内的机器出现故障的概率较大，因此他们属于同一个故障域，即出现故障的时候往往是同时宕机。\n\n云SQL Server保证每个分区的多个副本分布在不同的故障域。同GFS或者HDFS一样，根据网络拓扑的层级进行备份：同一机架、同一机房、同一网络。\n\n同一般的分布式存储系统一样，主备副本之中，只有主分区才会提供服务，副分区只接受主分区通过操作日志同步过来的操作。因为主备副本同步会有一个时间差，如果备副本提供读服务，就会读取到过期数据。\n> 这里的同步时间差指的是备副本通过操作日志回放操作的时间，在强一致性的分布式系统中，通常要求主备副本在操作日志同步完成之后才认定此次写入成功。\n<img src=\"/img/202104/019.png\" width=\"50%\" height=\"50%\">\n如图所示，有四个逻辑分区PA、PB、PC、PD，每个分区有一个主副本和两个备副本。每台SQL Server混合存放了主副本和备副本。如果某台机器发生了故障，可以将服务快速地迁移到其他备副本节点上。\n\n分区的划分是同台的，如果某个分区超过了允许的最大分区大小或者负载太高，这个分区将会被分裂为两个分区。同时它的备副本也将会被分裂为两个分区，同时为了负载均衡，所以主分区分裂后并不一定还是主分区，可能会让副分区分裂后的分区作为提供服务的新的主分区。\n\n## 架构\n云SQL Server分为四个主要部分：SQL Server实例、全局分区管理、协议网管、分布式基础部件。\n<img src=\"/img/202104/020.png\" width=\"50%\" height=\"50%\">\n\n* SQL Server实例：每个SQL Server实例是一个运行着SQL Server的物理进程。每个物理数据库包含多个子数据库，他们之间相互隔离。子数据库是一个分区，包含用户的数据以及schema信息。\n* 全局分区管理器：它维护分区映射表信息，包括每个分区的主键范围，每个副本所在的服务器，以及每个副本的状态，包括副本当前是主还是备，前一次是主还是备，正在变成主，正在备拷贝或者正在被追赶。当服务器发生故障时，分布式基础部件检测并确保服务器故障后通知全局分区管理器。全局分区管理器接着执行重新配置操作。另外，全局分区管理器监控集群中的SQL Server工作集，负责负载均衡、副本拷贝等管理操作。\n> SQL Server实例就是存储节点，而全局分区管理器则是中心节点。从全局分区管理器管理每个分区的主键范围这点可以看到，SQL Server的扩容是按照通用的range+hash模式来进行分库操作。即分区时按照主键范围进行分区，分区内又按照hash值映射到对应表中。而全局分区管理器只需要负责维护分区的主键范围，后面的hash映射可以交给SQL Server实例完成。\n* 协议网关：负责将用户的数据库连接请求转发到相应的主分区上。协议网管通过全局分区管理器获取分区所在SQL Server电视里，后续的读写事务操作都在网关与SQL Server实例之间进行。\n> 为了防止全局分区管理器称为整个系统的性能瓶颈，因此通过将数据缓存到网关本地的方式减少读写流程对全局分区管理器的使用频率。\n* 分布式基础组件：用于维护机器上下线状态，检测服务器故障并为集群你中的各种角色执行选举主节点操作。他在每台服务器上都运行了一个守护进程。\n> 简单的说就是各个节点之间的通信模块，负责串联整个集群中的所有节点，并传递相关信息。\n\n> 分布式存储系统的设计离不开客户端、存储节点、中心节点这三类，不同的系统会根据实际需要适当地添加新的组件或者将这三者的一部分功能抽离成一个新组件。在分布式系统中，中心节点需要及时地判断集群中各个节点的生存状态，因此各个节点通过心跳、互斥锁服务等方式将自身的存活状态告知中心节点；客户端在读写时需要根据读写数据内容获取对应的存储节点信息，同时要保证信息的及时性与一致性，因此这部分信息通常由中心节点负责维护，这就是集群元数据；由于中心节点需要和集群中所有节点保持通信、维护元数据修改，为了避免其称为整个系统的性能瓶颈，需要客户端缓存元数据信息以减轻读写过程对中心节点的使用频率。\n\n### 复制与一致性\n云SQL Server采用“Quorum Commit”的复制协议，用户数据存储三个副本，至少写成功两个副本才可以返回客户端成功。\n> 这样在读取的时候只需要R + W > N，即可保证读取的结果中一定有正确值。\n<img src=\"/img/202104/021.png\" width=\"50%\" height=\"50%\">\n事务T的主副本分区生成操作日志并发送到备副本。乳沟事务T回滚，主副本会发送一个ABORT消息给备副本，备副本将删除接收到的T事务包含的修改操作。如果T事务提交，主副本会发送COMMIT消息给备副本，并带上事务提交序号（Commit Sequence Number，CSN），每个备副本会把事务T的修改操作应用到本地数据库并发送ACK消息回复主副本。如果主副本接收到一半以上的成功ACK，他将在本地提交事务并成功返回客户端。\n\n> 从上面的操作来看，主副本执行事务的时候哪些修改操作发送到备副本之后，备副本并不会立即对这些操作进行恢复，而是等待主副本发送最终的命令确认这些操作是提交还是会滚。如果提交，则按照主副本提供的执行顺序对日志进行恢复。\n\n如果备副本发生故障，在他们恢复后将向主副本发送本地已经提交的最后一个事务的提交序号。如果两者相差不多，主副本将直接发送操作日志给备副本；如果两者相差太多，主副本将首先把数据库快照传给备副本，再把快照点之后的操作日志传给备副本。\n\n> 这个多于不多的判断在于备副本发送的最后一次提交的事务序号是否已经被保存为快照节点之后，如果已经被生成快照，则表示按照日志恢复记录的时间开销比直接传输数据要大。\n\n主备副本之间传送逻辑操作日志，而不是对磁盘物理页的redo&undo日志。\n\n数据库索引以及schema相关操作（如创建，删除表格）也通过操作日志发送。\n> 因为在一致性中，主备副本之间的数据是一致的，但是数据保存的物理页不一定一致，如果要求这也要同步，那么开销将比单纯地同步逻辑操作要大，同时也没有太大意义。\n\n同时，有些网卡会出现一些硬件问题，虽然TCP保证了数据传输过程中的准确性，但是网卡的问题依旧能够使得数据丢失或者被修改，因此对主备之间的所有消息都会做校验（checksum）。同样，某些磁盘会出现“位翻转“错误，因此，对写入到磁盘的数据也做校验。\n> 校验在主副本的时候将数据与数据MD5码同时发送，如果传输过程中数据发生改变，无论是数据本身还是校验码，他们都将无法匹配。磁盘同理。\n\n### 容错\n每个SQL Server最多服务650个逻辑分区，这些分区可能是主副本，也可能是备副本。全局分区管理器统一调度，每次选择一个分区执行重新配置（Reconfiguration）。如果出现故障的分区是备副本，全局分区管理器首先选择一台负载比较轻的服务器，接着从相应的主副本分区拷贝数据来增加副本；如果出现故障的是主副本，首先需要从其他副本中选择一个最新的备副本作为新的主副本，接着选择一台负载比较轻的机器增加备副本。\n\n由于云SQL Server采用“Quorum Commit”复制协议，不需要等待所有副本全部复制完成。\n\n全局分区管理器控制重新配置任务的优先级，否则，用户的服务会收到影响。比如某个数据分片的主副本出现故障，需要尽快从其他副本中选择副本切换为主副本你；某个数据分片只有一个副本，需要优先复制。\n\n针对某些服务器下线后重新上线也要配置相关策略：只有两个副本的状态持续较长一段时间（默认2小时）才开始复制第三个副本。\n\n全局分区管理器也采用“Quorum Commit”实现高可用性。它包含七个副本，同一时刻只有一个副本为主，分区相关的元数据操作至少需要在四个副本上成功（即超过一半的副本写入成功）。如果全局分区管理器主副本出现故障，分布式基础部件将负责从其他副本中选择出最新的副本作为新的主副本。\n\n### 负载均衡\n负载均衡的相关操作包含三种：副本迁移一季主备副本切换。\n\n新的服务器节点加入时，系统内的分区会逐步地迁移到新节点，为了避免过多的分区同时迁入新节点，全局分区管理器需要控制迁移的频率，否则系统整体性能可能会下降。\n> 如果同时迁入，会使得新机器的网络资源同时被多台节点占用，这样的后果就是所有节点的网络资源都少得可怜，使得数据迁移的时间被拉长；同时由于迁移过程中也会占用原先节点的网络资源，如果长时间被占用，将会导致这些迁移过程中的节点对外提供服务的效率变慢。\n\n如果主副本所在服务器负载过高，可以选择负载较低的备副本替换为主副本提供读写服务。这个过程称为主备切换，不涉及数据拷贝。\n> 如果将流量分流到备副本，那么将会导致两个同时提供服务的节点直接数据的不一致性。\n\n影响服务器节点负载的因素包括：读写次数，磁盘/内存/CPU/IO使用量。\n\n### 多租户\n云存储系统中多个用户的操作互相干扰，因此需要限制每个SQL Azure逻辑实例使用的系统资源：\n    1. 操作系统资源限制，比如CPU、内存、写入速度等，如果超过限制将在10秒内内拒绝相应的用户请求\n    2. SQL Azure逻辑数据库容量限制。每个逻辑数据库都预先设置了最大的容量，超过限制时拒绝更新请求，允许删除操作\n    3. SQL Server物理数据库大小限制。超过该限制时返回客户端系统错误，此时需要人工介入\n\n## 讨论\nMicrosoft SQL Azure将传统的关系型数据库SQL Server搬到云环境中，比较符合用户过去的使用习惯。但还是有一些区别：\n* 不支持的操作：Mucrosoft Azure作为一个针对企业级应用的平台，景观尝试支持尽量多的SQL特性，仍然有一些特性无法支持。比如USE操作：SQL Server可以通过USE切换数据库，不过在SQL Azure不支持，这时因为不同的逻辑数据库可能位于不同的物理机器。切换过程中需要切换网络连接等相关配置。\n* 观念转变：对于开发人员，需要用分布式系统的思维开发程序，比如一个连接除了成功、失败还有第三种不确定状态：云端没有返回操作结果，操作是否成功无从得知；对于DBA，数据库日常维护，比如升级、数据备份等工作都交给了微软，可能会有更多的精力关注业务系统架构。\n\n项目Azure Table Storage，SQL Azure在扩展性上有一些劣势，例如，单个SQL Azure实例大小限制。Azure Table Storage单个用户表格的数据可以分布到多个存储节点，数据总量几乎没有限制；而单个SQL Azure实例最大限制为50GB，如果用户的数据量大于最大值，需要用户在应用层对数据库进行水平或者垂直拆分，使用比较麻烦。\n> 这里可以看到分布式表格与分布式数据库的一些区别：分布式表格的灵活性比分布式数据库大，因为分布式数据库需要保证用户在使用的时候与平时使用的单机数据库没有多大区别，因此为了支持大部分的事务操作，对数据的分布做了限制，不允许将同一个用例的数据分布到多台服务器上，对一个用例的事务只会在单机层面完成，不会扩展为分布式事务。\n\n> 分布式表格系统中没有数据库引擎部分，所有数据都统一由存储节点维护，相关信息由中心节点维护，可以十分灵活地对数据分布进行重新排列。","tags":["存储"]},{"title":"分布式存储笔记3-5：分布式数据库系统（数据库中间层）","url":"/2021/04/08/分布式存储笔记3-5：分布式数据库系统（数据库中间层）/","content":"<hr>\n这是一篇在阅读《大规模分布式存储系统：原理解析与架构实战》时的阅读笔记，由于长时间碎片阅读的关系导致在做这种读书笔记的时候接近复制粘贴。虽然其中会有一小部分自己的想法但都十分零碎，希望后续能改进。\n<hr>\n\n对于关系型数据库，有很多思路可以实现关系数据库的可扩展性。例如：\n1. 在应用层划分数据，将不同的数据分片划分到不同的关系数据库上，如Mysql Sharding；\n2. 或者在关系数据库内部支持数据自动分片，如Microsoft SQL Azure；\n3. 或者干脆从存储引擎开始重写一个全新的分布式数据库，如Google Spanner以及Alibaba OceanBase。\n<!-- more -->\n# 数据库中间层：\n为了扩展关系数据库，最简单也是最常见的做法就是应用层按照规则将数据拆分为多个分片，分不到多个数据库节点，并引入一个中间层来对应用屏蔽后端的数据库拆分细节。类似于这样：\n<img src=\"/img/202104/015.png\" width=\"50%\" height=\"50%\">\n\n## 架构：\n以MysqlSharding架构为例，分成几个部分：中间层dbproxy集群、数据库组、元数据服务器、常驻进程。\n<img src=\"/img/202104/016.png\" width=\"50%\" height=\"50%\">\n\n### Mysql客户端库：\n应用程序通过Mysql原生的客户端与系统交互，支持JDBC，原有的单机访问数据库程序可以无缝迁移。\n\n### 中间层dbproxy：\n解析客户端SQL请求并转发到后端的数据库。在这一步它负责解析Mysql协议，执行SQL路由，SQL过滤，读写分离，结果归并，排序以及分组，等等。\n\n中间层由多个无状态的dbproxy进程组成，不存在单点问题。\n\n中间层通过LVS进行负载均衡，但由于部署负载均衡服务需要多经历一层网络开销，因此常见的做法是将LVS放置在Mysql客户端上，由客户端处理请求负载均衡以及中间层服务器故障等情况。\n\n### 数据库组dpgroup：\n每个dbgroup由N台数据库机器组成，其中一台为主机（Master），另外N-1台为备用（Slave）。\n\n主机提供服务，包括写事务与强一致读服务，并将操作以binlog的形式同步到备机器上，备机器可以支持有一定延迟的读事务。\n> 备机器上的数据从主机同步需要一个时间段，因此哪些刚写入的数据可能无法在备机器上读取到，但是可以提供已经经过一段时候后的数据的读取服务。\n\n### 元数据服务器：\n元数据服务器主要负责维护dbgroup拆分规则并用于dbgroup选主。\n\ndbproxy通过元数据服务器获取拆分规则从而确定SQL语句的执行计划。\n\n如果dbgroup的主机出现故障，需要通过元数据服务器选主。元数据服务器本身也需要多个副本实现HA，一种常见的方式是采用Zookeeper实现。\n> zookeeper本身能够存储信息，同时自身也是一个高效的分布式锁服务，通过互斥锁能够实现选主的功能\n\n### 常驻进程agents：\n部署在每台服务器上的常驻进程，用于实现监控，单点切换，安装，卸载程序等。\n\ndbgroup中的数据库需要进行主备切换，软件升级等，这些控制逻辑需要与数据库读写事务处理逻辑隔离开来。\n> 这些逻辑跟业务毫无关系，因此不能让这些逻辑阻塞事务的执行。\n\t\n如果数据库按照用户哈希分区，同一个用户的数据分布在同一个dbgroup上，这样容易出现“数据倾斜”问题。如果SQL请求只涉及同一个用户，那么中间层将请求转发给相应的dbgroup，等待返回结果并返回给客户端；\n\n如果SQL请求涉及多个用户，那么中间层需要转发给多个dbgroup，等待返回并将结构执行合并、分组、排序等操作后返回客户端。\n\n## 扩容：\nMysql Sharding集群一般按照用户id进行哈希分区，这里存在两个问题：\n1. 集群容量不够怎么办\n2. 单个用户的数据量太大怎么办（数据倾斜）\n\n### 问题1:\nMysql Sharding集群会采用双倍扩容的方案，即从2台服务器扩到4台，接着再扩容到8台。\n\n假设原来有2个dbgroup，第一个dbgroup的主机为A0，备机为A1，第二个dbgroup的主机为B0，备机为B1.按照用户id哈希取模，结果为奇数的用户分布在第一个dbgroup，结果为偶数的用户分布在第二个dbgroup。常见的一种扩容方式如下：\n1. 等待A0和B0的数据同步到其备服务器。\n2. 停止写服务，等待主备完全同步后解除A0与A1、B0与B1之间的主备关系\n3. 修改中间层的映射规则，将哈希值模4等于1的用户数据映射到A1，哈希值模4等于3的用户数据映射到B1\n4. 开启写服务，用户id哈希值模4等于0、1、2、3的数据分别写入到A0、A1、B0、B1.相当于有一半的数据分别从A0、B0迁移到A1、B1.\n5. 分别给A0、A1、B0、B1增加一台备机\n6. 最终，集群由2个dbgroup扩容到4个dbgroup。\n\n> 这种方案扩容后，A1、B1上的数据依旧有一部分属于A0、B0上，这部分并没有随着通过用户取模重新划分后删除，而是依旧保留，这种情况下这些数据属于垃圾数据，因为不可能再有响应的用户请求到这些节点上，也不会有主节点将数据同步过去，因此扩容后需要有一个负载均衡方案或者垃圾回收方案用于将这些数据删除。\n<img src=\"/img/202104/017.png\" width=\"50%\" height=\"50%\">\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;例如，A0的有主键范围为(0,  100]的用户数据,B0有用户主键范围是(100, 200]的数据，扩容后A0、A1、B0、B1的主键范围分别为(0, 50]、(50, 100]、(100, 150]、(150, 200]；但是他们拥有的数据的主键范围是(0, 100]、(0, 100]、(100, 200]、(100, 200]\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分库分表：常用的方案是range+hash的模式，首先通过判断数据主键的范围来确定数据属于哪个数据库组，然后主键和数据库组中的数据库数量取模最终确定数据存在哪里，扩容只需要新增一个范围内的数据库组即可。但是这样只适用于单表记录，如果是用户的话，一个用户的数据量会越来越大，最终达到单机的极限。\n\n### 问题2:\n可以在应用层定期统计大用户，并且将这些用户的数据按照数据量拆分到多个dbgroup。\n\n# 讨论：\n引入数据库中间层将后端分库分表对应用透明化再大型互联网公司内部很常见。这种做法简单但是会有一些问题：\n1. 数据库复制：Mysql主备之间只支持异步复制，而且主库压力较大时可能产生很大的延迟，因此主备切换可能会丢失最后一部分更新事务，这时需要人工介入\n2. 扩容问题：这个过程涉及到数据的重新划分，就像上面记录的，每次扩容都会在dbproxy上增加一层路由，容易出错。\n3. 动态数据迁移问题：如果某个数据库组压力过大，需要将其中部分数据迁移出去，迁移过程需要总控节点整体协调，以及数据库节点的配合，这个过程很难做到自动化。","tags":["存储"]},{"title":"分布式存储笔记3-4：分布式表格系统（Windows Azure Storage）","url":"/2021/04/06/分布式存储笔记3-4：分布式表格系统（Windows-Azure-Storage）/","content":"<hr>\n这是一篇在阅读《大规模分布式存储系统：原理解析与架构实战》时的阅读笔记，由于长时间碎片阅读的关系导致在做这种读书笔记的时候接近复制粘贴。虽然其中会有一小部分自己的想法但都十分零碎，希望后续能改进。\n<hr>\n\nWindows Azure Storage = WAS\n\nWAS包含三种数据存储服务：Windows Azure Blob、Windows Azure Table、Windows Azure Queue。三种数据存储共享一套底层架构。\n<!-- more -->\n## 整体架构\nWAS依赖底层的Windows Azure结构控制器（Fabric Controller）管理硬件资源。结构控制器的功能包括节点管理，网络配置，健康检查，服务启动，关闭，部署和升级。\n\nWAS还通过请求结构控制器获取网络拓扑信息，集群物理部署以及存储节点硬件配置信息。\n<img src=\"/img/202104/010.png\" width=\"50%\" height=\"50%\">\n\nWAS主要分为两个部分：定位服务、存储区\n### 定位服务（Location Service， LS）：\n功能有管理所有的存储区，管理用户到存储区之间的映射关系，收集存储区的负载信息，分配新用户到负载较轻的存储区。LS自身也有分布在两个不同地域以实现高可用。LS通过DNS服务来使得每个账户的请求定位到所属存储区。\n> 起到了中心节点的作用\n\n### 存储区（Storage Stamp）：\n每个存储区是一个集群，一般由10～20个机架组成，每个机架有18个存储节点，提供大约2PB存储容量。\n\n存储区分三层：文件流层（Stream Layer）、分区层（Partition Layer）以及前端层（Fornt-End Layer）。\n\n#### 文件流层：\n与GFS类似，提供分布式文件存储。其中文件称为流（stream），文件中的chunk称为范围（extent）。且不对外服务，需要经过服务分区层。\n#### 分区层：\n与Bigtable类似，将对象划分到不同的分区以被不同的分区服务器（Partition Server）服务，分区服务器将对象持久化到文件流层。\n> 在原始文件数据的基础上抽象出了对象的概念\n#### 前端层：\n前端层包括一系列无状态的web服务器，这些web服务器完成权限验证等功能并根据请求的分区名（Partition Name）将请求转发到不同的分区服务器。分区映射表（Partition Map）用来决定应该将请求转化到哪个分区服务器，前端服务器一半缓存了此表从而减少一次网络请求。\n\n从定位服务的功能来看，分区映射表应该是保存在定位服务器上。\n\n### 复制方式：\n#### 存储区内复制（Intra-Stamp Replication）：\n文件流层实现，同一个extent的多个副本之间的复制模式为强同步，每个成功的写操作必须保证所有的副本都同步成功，用来实现强一致性。\n> 在extent中主备副本的文件偏移应该也要一样。\n#### 跨存储区复制（Inter-Stamp Replication）：\n服务分区层实现，通过后台线程异步复制到不同的存储区，用来实现异地容灾。\n> 这种情况下应该只需要实现“至少一次成功写入”这样的弱一致性即可。\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;两者的区别实际上就是是否跨集群复制。在网络拓扑中可能就是跨机房或者跨机架。其中的主要区别就是网络因素：在网络条件允许的情况下追求强一致性；在网络通讯成本高的情况下追求数据的完整性而不是一致性。\n\n## 文件流层：\n文件流层提供内部接口供服务分区层使用。它提供类似文件系统的命名空间和API，但所有的写操作只能是追加，支持的接口包括：打开&关闭文件、改名、读取以及追加到文件。\n\n文件流层的文件称为流，每个流包含一系列的extent，每个extent由一系列的block组成。\n<img src=\"/img/202104/011.png\" width=\"50%\" height=\"50%\">\n\n\t加封：不允许对extent追加数据\n    未加封：允许对extent追加数据\n\nblock是数据读写的最小单位，每个block最大不超过4MB。文件流层对每个block计算校验和。读取操作总是给定某个block的边界，然后一次性连续读取一个或者多个完整的block数据；写入操作凑成一个或者多个block写入到系统。WAS中的block与GFS中的记录（record）概念是一致的。\n> 对于写入一整个完整文件来说，会将文件分成一个个完整的block，如果多余1KB，也会分隔成一个单独的block，而不是跟其他文件的block混合。\n\nextent是文件流层数据复制，负载均衡的基本单位，每个存储区默认对每个extent保留三个副本，每个extent的默认大小为1GB。如果存储小对象，则多个对象共用同一个extent；如果存储大对象（GB或者TB级别）则会将对象分成多个extent。与GFS中的chunk概念相同。\n\nstream用于文件流层对外接口，每个stream在层级命名空间中有一个名字。这stream与GFS中的file概念相同。\n\n### 架构：\n<img src=\"/img/202104/012.png\" width=\"50%\" height=\"50%\">\n\n文件流层由三部分组成：\n#### 流管理器（Stream Manager，SM）：\n流管理器维护了文件流层的元数据，包括文件流的命名空间，文件流到extent之间的映射关系，extent所在的存储节点信息。\n\n他还需要监控 extent存储节点，负责整个系统的全局控制，如extent复制，负载均衡，垃圾回收无用的extent，等等。流管理器会定期通过心跳的方式轮询extent存储节点。流管理器自身通过Paxos协议实现高可用性。\n> 对于每一个文件流层来说都是一个小型的文件存储系统。包括存储节点与中心节点，其中流管理器就是中心节点，但这个中心节点只能管理本集群的所有存储节点，如果涉及到跨集群复制等操作，需要通过分区层来调度。\n#### extent存储节点（Extent Node， EN）：\nextent存储节点实际存储每个extent的副本数据。每个extent单独存储成一个磁盘文件，这个文件包含extent中所有block的数据及checksum，以及针对每个block的索引信息。extent节点之间互相通信拷贝客户端追加的数据。extent还需要接受流管理器的命令，例如创建extent副本，垃圾回收指定extent等等。\n> extent以磁盘为单位，副本存放在多个extent之间，如果这样的话一个集群下线不就使得一部分数据无法提供服务了吗？不应该还有一部分数据是备份到集群之外的？从架构层面来看流管理器起到一个对存储节点的管理作用，而更上层的分区层应该就负责跨集群的数据操作。\n\n#### 客户端库（Partition Layer Client）：\n客户端库是文件流层提供给上层应用（分区层）的访问接口，不遵守POSIX规范。\n\n分区层访问文件流层时，首先访问流管理器节点，获取与之进行交互的extent存储节点你信息，然后直接访问这些存储节点。\n> 也就是说客户端库相当于提供了一个功能列表的作用，并不是完美分割分区层与文件流层。分区层从客户端库获取到文件流层extent命名空间等信息，然后直接与其通信。\n\n### 复制及一致性：\nWAS中的流文件只允许追加，不允许更改。追加操作是原子的，数据追加以数据块为单位，多个数据块可以由客户端凑成一个缓冲区一次性提交到文件流层的服务端，保证原子性。与GFS一样，客户端追加数据块可能失败需要重试，从而产生重复记录，分区层需要处理这种情况。\n\n分区层通过两种方式处理重复记录：\n* 对于元数据（metadata）和操作日志流（commmit log streams），所有的数据都有一个唯一的事务编号（trannsaction sequence），顺序读取时忽略编号相同的事务；\n* 对于每个表格中的行数据流（row data streams），只有最后一个追加成功的数据块才会被索引，因此先前追加失败的数据块不会被分区层读取到，将来也会被系统的垃圾回收机制删除。\n\n> 只有追加成功的block的文件信息才会作为元数据保存，而失败的相关信息会被上层忽略，但是在文件流层中能够被检测出来，说明文件流层需要维护整个extent的所有block的元数据，其中包括block在extent中的偏移地址与长度，通过这些数据才能够计算出一个extent中有多少多余的重复数据的空间。\n\n如图6-12，WAS追加流程如下：\n1. 如果分区层客户端没有缓存当前extent信息，例如追加到新的流文件或者上一个extent已经缝合，客户端请求SM创建一个新的extent。\n2. SM根据一定的策略，如存储节点负载，机架位置等，分配一定数量（默认为3）的extent副本到EN。其中一个副本作为主副本允许分区层写入，其他副本作为备副本，只允许接收主副本同步的数据。Extent写入过程中主副本维持不变，因此，WAS不需要类似GFS中的租约机制，大大简化了追加流程；\n> 那如果在追加过程中主extent宕机了该怎么办？从文中并没有看到相关处理方案。难道是直接提示追加失败吗？\n3. 分区层写请求发送到主副本。主副本执行以下操作：\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1) 决定追加的数据块在extent中的位置\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2) 定序：如果有多个客户端往同一个extent并发追加，主副本需要确定这些追加操作的顺序\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3) 将数据块写入主副本本身\n4. 主副本把待追加数据发送给某个备副本，被副本接着转发给其他备副本。\n5. 备副本写成后应答主副本\n6. 如果所有的副本都应答成功，主副本你应答客户端追加操作成功。\n\n> 问题同上，在追加的时候如果有一个extent出现故障，则客户端认定此次追加失败并告知SM，SM将已经写入的extent缝合（加封），然后创建新的extent用来提供追加操作。这样不会太浪费空间了吗？为什么不按照block来缝合？\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3需要按情况，如果是追加写失败，应该直接跳过失败的block，如果是新建extent并首次写入失败，则将这个extent直接缝合。\n\n每个extent副本都维护了已经成功提交的数据长度（commit length），如果初夏一场，各个副本当前长度可能不一致。SM缝合extent时首先请求所有的副本获取当前党读，如果副本之间不一致，SM将选择最小的长度值作为缝合后的长度。如果缝合操作的过程中某个副本所在的节点出现故障，缝合操作仍然可以成功执行，等到节点重启后，SM将强制该节点从extent的其他副本同步数据。\n> 这里的缝合不等于加封，这里的缝合相当于将多个不同extent之间进行数据的同步工作，同步的这部分就是写入失败的错误数据。当讲这部分切分到最小长度后又能够重新对外提供服务，如果没有切除干净也没事，最终生成block元数据的只会是写入成功的block，其他重复数据、错误数据将会在垃圾回收阶段被清除。\n\n文件流层保证如下两点：\n* 只要记录被追加并成功响应客户端呢，从任何一个副本都能读到相同的数据\n* 及时追加过程中出现故障，一旦extent被缝合，从任何一个被缝合的副本都能读到相同内容（按照最短长度切分）\n\n### 存储优化：\nextent需要面临两个问题：如何保证磁盘调度公平性以及避免磁盘随机写操作。\n1. 如果存储节点上某个磁盘当前已发出请求的期望完成时间超过100ms或者最近一段时间内某个请求的响应时间超过200ms，避免将新的IO请求调度到该磁盘。\n> 很多磁盘优先执行大块顺序写操作，这样会阻塞大量的随机读操作，导致后续读取超时。这种办法就是将随机读的时候检测IO是否繁忙，如果繁忙则到另一块磁盘上读取数据。这另一块磁盘就是extent的备份。\n2. 存储节点采用单独的日志盘（journal drive）顺序保存节点上所有extent的追加数据，追加操作分两步：\n    A. 将待追加的数据写入日志盘\n    B. 将数据写入extent文件，操作A将随机写变为针对日志盘的顺序写，一般来说，操作A先成功，操作B只是将数据保存到系统内存中。\n如果节点发生故障，需要通过日志盘中的数据恢复extent文件。\n> 将多个针对磁盘的随机写操作修改为针对单个日志文件的顺序写入，类似于Redis的单线程写入一样，减少磁盘的随机写，提高吞吐量并降低延时。\n3. 抹除码（erasure coding）机制用于减少extent副本占用的空间。每个extent副本都默认需要存储三份，为了降低存储成本，文件流层会对已经缝合的extent进行Reed-Solomon编码。\n\n文件流层在后台定期执行任务，将extennt划分为N个长度大致相同的数据块，并通过Reed-Solomon算法计算出M个纠错码段用于纠错。只要出现问题的数据段或者纠错码段总和小于或者等于M个，文件流层都能重建整个extent。\n\n推荐的配置是N=10，M=4，也就是只需要1.4倍的存储空间，就能容忍多达4个存储节点出现故障。[RS编码](https://zhuanlan.zhihu.com/p/103888948?utm_source=wechat_session)\n\n## 分区层：\n分区层位于文件流层之上，在文件数据的基础上抽象出了对象的概念，与Bigtable一样提供了事务支持。\n\n分区层内支持一种称为对象表（Object Table， OT）的数据架构，每个OT是一张最大可达若干PB的大表。对象表类似于Bigtable的子表，被动态地划分为连续的范围分区，并分散到WAS存储区的多个分区服务器上。范围分区之间互相不重叠。\n> 大致跟BIgtable一样，Bigtable维护子表，而分区层维护OT的范围分区。\n\nWAS存储区包含的对象表包括：账户表、Blob数据表、Entity数据表、Message数据表。\n\n分区层还有一张全局的Schema表格，保证所有的对象表哥的schema信息，即每个对象表包含的每个列的名字，数据类型及其他属性。（就是每张对象表的元数据信息）\n\n每个对象表的行主键包括用户账户名，分区名以及对象名三部分。\n\n系统内部还维护了一张分区映射表，用于记录每个范围当前分区所在的分区服务器。\n\n### 架构：\n<img src=\"/img/202104/013.png\" width=\"50%\" height=\"50%\">\n\n#### 客户端：\n提供分区层到WAS前端的接口，前端通过客户端以对不同对象的数据单元进行增删改查等操作。客户端通过分区映射表获取分区映射信息，但所有表格的数据内容都在客户端与分区服务器之间之间传送。\n> 减少分区映射表的网络压力，在分布式存储架构设计中，这种维护着全局数据范围分布的节点由于所有的读写操作都会经过这个节点，为了提高性能都不会传递数据本身。\n#### 分区服务器（Partition Server， PS）：\nPS实现分区的装载/卸除、分区内容的读和写（Bigtable对子表的权限，删除与添加子表权限）、分区的合并和分裂。一般来说，每个PS平均服务10个分区。\n#### 分区管理器（Partition Manager，PM）：\n管理所有的PS，包括分配分区给PS，指导PS实现分区的分裂及合并，监控PS，在PS之间进行负载均衡并实现PS的故障恢复等。每个WAS存储区有多个PM，他们之间通过Lock Server进行选主，持有租约的PM是主PM。\n> 只负责监控与发送命令，具体的分裂合并操作与相关对策交给分区服务器执行。\n#### 锁服务（Lock Service）：\nPaxos锁服务用于WAS存储区内选举主PM。另外，每个PS与锁服务之间都维持了租约。锁服务监控租约状态，PS的租约快到期时，会向锁服务重新续约。如果PS出现故障，PM需要首先等待PS上的租约过期后才可以将它原来的服务分区分配出去，PS租约如果过期也需要主动停止读写服务。\n> 为了防止出现多个PS同时读写同一个分区的情况，这部分的操作应该也会类似于Bigtable，PM在确定PS租约过期后向锁服务申请获取下线的PS的锁，如果获取成功则进行分区分配。如果获取失败表示PM出了问题或者PS重新上线\n\n### 分区数据结构：\n<img src=\"/img/202104/014.png\" width=\"50%\" height=\"50%\">\n与Bigtable基本一致。用户写操作首先追加到操作日志文件流（即写入操作日志中），接着修改内存表（Memory Table， 在内存中生效，此时数据的写入可以被其他用户看到），等到内存表到达一定大小后执行快照（checkpoinnt，对应bigtable的Minor Compaction）。\n\n分区服务器定期将多个小快照文件合并成更大的快照文件（对应Bigtable的Major Compaction）以减少读取所需要的文件数量。分区服务器会对每个快照文件维护热点行数据的缓存（Row Page Cache，对应Bigtable中的Block Cache和Key Value Cache），通过布隆过滤器过滤对快照文件呢不存在行的随机读请求。\n\n与Bigtable的不同：\n1. WAS中每个分区拥有一个专门的操作日志，而Bigtable中同一个TabletServer共用一个操作日志，通过<tableId>将数据进行划分\n2. WAS中每个分区维护各自的元数据（例如分区包含哪些快照文件，持久化成元数据文件流），分区管理器只管理每个分区与所在的分区服务之间的映射关系；也就是说分区之间的元数据不可见，相互独立，也不会互相交互。而Bigtable维护了两级元数据，分别子元数据表与根表，在一定的情况下TabletServer之间会交换元数据信息。\n3. WAS专门映入了Blob数据文件流用于支持Blob数据类型的文件。Bigtable只支持半结构化与结构化数据，Blob相关的服务直接由GFS提供。\n\n由于Blob数据量大，因此操作日志只记录Blob数据在数据流层中的索引信息，\n\n### 负载均衡：\nPM记录每个PS以及它服务的每个分区的负载。\n\n影响负载的因素包括：\n1. 每秒事务数\n2. 平均等待事务个数\n3. 节流率：控制器调节或延迟终端生成的速率，以优化网络吞吐量和CPU的使用\n4. CPU使用率\n5. 网络使用率\n6. 请求延时\n7. 每个分区的数据大小\n\nPS与PM通过心跳定时将这些信息发往PM，由PM决定PS是否要进行负载均衡。\n\n对分区进行负载的两个阶段：\n1. 卸载：PM首先发送一个卸载指令给PS，PS会执行一次快照操作。一旦完成后，PS停止待迁移分区的读写服务并告知PM卸载成功。如果卸载过程中PS出现异常，PM需要查询锁服务进行确认。\n2. 加载：PM发送加载指令给新的PS并且更新PM维护的分区映射表结构，将分区指向新的PS。新的PS加载分区并且开始提供服务。由于卸载时执行了一次快照操作，加载时需要回放的操作日志很少，保证了加载的快速。\n3. 在新的PS没有回复加载完成之前，PM中的对元数据的修改是对用户不可见的，常用的做法是写入REDO日志中但是没有在内存中生效。\n\n### 分裂与合并：\n有两种可能导致WAS对某个分区执行分裂操作：一种是分区太大，另一种是分区的负载过高。\n\nPM发起分裂操作，由PS确认分裂点。（因为PM没有持有PS的分区详细信息）。\n\n如果是基于分区大小的分裂操作，PS维护了每个分区的大小以及大致的中间位置，并将这个中间位置作为分裂点；如果是基于负载的分裂操作，PS自适应地计算分区中哪个主键范围的负载最高，并通过它来确定分裂点。\n\n把分区B分裂成为新分区C和D的步骤如下：\n1. PM告知PS将分区B分裂为C和D\n2. PS对B执行快照操作，接着停止分区B的读写服务\n3. PS发起一个“MultiModify”操作将分区B的元数据，操作日志及行数据流复制到C和D。这一步之需要拷贝每个文件的extent指针列表，不需要拷贝extent的内容。接着PS分别往C和D的元数据流写入新的分区范围\n4. PS开始对C和D这两个分区提供读写服务\n5. PS通知PM分裂成功，PM相应地更新分区映射表及元数据信息。接着PM会把C或者D中的其中一个分区迁移到另外一个不同的PS\n> MultiModify：在一次调用中实现多步修改操作。\n\n合并操作需要选择两个连续的负载较低的分区。\n\n把分区C和D合并成分区E的步骤如下：\n1. PM迁移C或者D，使得这两个分区被同一个PS服务。接着PM通知PS将分区C和D合并成E。\n2. PS分别对分区C和D执行快照操作，接着停止分区C和D的读写服务\n3. PS发起一个“MultiModify”操作合并分区C和D的操作日志及数据流，生成E的操作日志和行数据流。这里也只需要修改extent指针。\n4. PS对E构造新的元数据流，包含新的操作日志文件、行数据文件，C和D合并后的新的分区范围以及操作日志回放点和行数据文件索引信息\n5. PS加载新分区E并提供读写服务\n6. PS通知PM合并成功，接着PM相应地更新分区映射表及元数据。\n> 备份的作用是防止节点宕机导致数据丢失。\n\n## 讨论：\nWAS整体架构借鉴GFS+Bigtable并有所创新，与其不同点在于：\n1. Chunk大小选择。GFS中每个Chunk大小为64MB，WAS每个extent大小提高到1GB\n2. 元数据层次。Bigtable中元数据包含元数据表与根表两级，WAS中只有一级元数据\n3. GFS中多个Chunk副本之间是弱一致的，不保证每个Chunk的不同副本之间每个字节都相同，而WAS能保证这点\n\nBigtable每个Tablet Server的所有子表共享一个日志文件从而提高写入性能，WAS将每个范围分区的操作写入到不同的操作日志文件。\n\nPS:2021-04-06 AWS粗看跟之前的分布式表格架构十分不同，但将其拆分后其实并没有脱离分布式存储的设计，只不过比起GFS-Bigtable-Megastore-Spannenr这种过渡式的设计，AWS一步到位采用成熟的成体系的结构，GFS除了为Bigtable提供子表存储外还能对外提供Blob存储服务，而AWS无论是Blob存储还是子表存储都同一经过上层的定位服务，这种一体化的设计使得各层模块之间全面衔接，与谷歌的一层一层改进孰优孰劣不好区分。","tags":["存储"]},{"title":"分布式存储笔记3-4 分布式表格系统（Google Megastore）","url":"/2021/04/01/分布式存储笔记3-4-分布式表格系统（Google-Megastore）/","content":"<hr>\n这是一篇在阅读《大规模分布式存储系统：原理解析与架构实战》时的阅读笔记，由于长时间碎片阅读的关系导致在做这种读书笔记的时候接近复制粘贴。虽然其中会有一小部分自己的想法但都十分零碎，希望后续能改进。\n<hr>\n\nMegastore在Bigtable系统之上提供呢有好的数据库功能支持，增强易用性。Megastore是介于传统的关系型数据库和NoSQL数据库之间的存储技术。\n> 从Bigtable的单表应用向着传统数据库方向发展，使得一个分布式存储系统对外表现为传统的关系型数据库，不说是方便用户使用，应该是在功能上逐渐拥有传统数据的功能：跨表事务、多表联查等\n<!-- more -->\n互联网应用往往可以根据用户进行拆分，比如Email系统、相册系统、广告投放效果报表系统、购物网站商品存储系统等。同一用户内部的操作需要保证强一致性，比如要求支持食物，多个用户之间的操作往往只需要最终一致性，比如用户之间发Email不要求立刻收到。因此，可以根据用户将数据拆分为不同的子集分布到不同的机器上。\n> 根据业务将系统进行拆分为独立的子系统，并将数据也随之拆分。这就是现在的微服务。\n\n谷歌进一步从互联网应用特性中抽取实体组（Entity Group）概念，从而实现可扩展性和数据库语义之间的一种权衡，同时获得NoSQL和RDBMS的优点。\n\n什么是实体组？用户定义了User和Photo两张表，主键分别为<user_id>和<user_id, photo_id>,每一个用户的所有数据构成一个实体组。其中User表是实体组根表，Photo表是实体组子表\n> 对现实事物的抽象就是一个实体组？每一个用户的所有数据构成了这个用户本身。用户实体组当然要以用户为核心。\n\n实体组根表中的一行成为根实体（Root Entity），对应Bigtable存储系统中的一行。根实体除了存放用户数据，还需要存放Megastore事物及复制操作所需的元数据，包括操作日志。\n<img src=\"/img/202104/001.png\" width=\"50%\" height=\"50%\">\n<img src=\"/img/202104/002.png\" width=\"50%\" height=\"50%\">\n表6-1中有两个实体组分别是101和102:\n\n> 从字段可以看到，实体组是一个抽象的概念，在数据库中并不会按照一个实体组一行数据这样的形式保存，而是将一个“用户”的概念拆分成多张表，并在逻辑层面将这些表的数据拼装成一个实体（Entity），证据就是101的用户，在实体组中记录了它多次对Photo进行操作的记录，同时并没有一张拥有name与time的表。\n\n其中101,500是Photo的主键<user_id, photo_id>。\n\nBigtable通过单行事务保证根实体操作的原子性，而一个实体组就是一行，因此对同一个实体组的操作都会是原子性的。\n\n又因为同一个实体组的数据都是连续存放，因此一个用户的数据往往会存在同一张子表中，分布在同一台Tablet Server上，从而提供较高的扫描性能和事务性能，当然，也能够将实体组分不到多台服务器上。\n\n## 系统架构\n<img src=\"/img/202104/003.png\" width=\"50%\" height=\"50%\">\n\nMegastore由三个部分组成：\n* 客户端库：提供Megastore到应用程序的接口，应用程序通过客户端操作Megastore的实体组。Megastore系统大部分功能集中在客户端，包括映射Megastore操作到Bigtable，事务及并发控制，基于Paxos的复制，将请求分送给复制服务器，通过协调者实现快速读等。\n* 复制服务器：接受客户端的用户请求并转发到所在机房的Bigtable实例，用于解决跨机房连接数过多的问题。\n* 协调者：存储每个机房本地的实体组是否处于最新状态的信息，用于实现快速读。\n\nMegastore的功能主要分为三个部分：映射Megastore数据模型到Bigtable，事务及并发控制，跨机房数据复制及读写优化。\n\n\t1. Megastore首先解析用户通过客户端传入的SQL请求\n\t2. 接着根据用户定义的Megastore数据模型将SQL请求转化为对底层Bigtable的操作。\n\n在上表中，假设用户（user_id = 101)往Photo表格中插入photo_id分别为500和502的两行数据。这就意味着需要向Bigtable写入主键分别为<101, 500>和<101, 502>这两行数据。这两行数据在Bigtable属于同一个版本，客户端要么读到全部行，要么一行也读不到。\n> 同一个版本应该是属于同一次操作吧，即一次操作同时插入两行。这一次操作就是原子性的。\n\n## 实体组\n<img src=\"/img/202104/004.png\" width=\"50%\" height=\"50%\">\n\n> GFS在底层负责保存数据，并维持数据的高可用与高容错；Bigtable在GFS之上抽象出了“表格”，并保持表格数据的一致性，但对外只有对表格数据的增删改查这些简单的服务；Megastore在Bigtable之上又添加了事务与并发支持，并同时增加高度灵活的SQL语句解析器，将SQL语句解析为对Bigtable的增删改查\n\n从图中看到，数据拆分成不同的实体组，每个实体组内的操作日志采用基于Paxos的方式同步到多个机房，保证强一致性。\n\n实体组之间通过分布式队列的方式保证最终一致性或者两阶段提交协议的方式实现分布式事务。\n<img src=\"/img/202104/005.png\" width=\"50%\" height=\"50%\">\n单集群实体组内部：同一个实体组内部支持满足ACID特性的事务。数据库系统事务实现时总是会提到REDO日志和UNDO日志，在Megastore系统中通过REDO日志的方式实现事务。同一个实体组的REDO日志都写到这个实体组的根实体中，对应Bigtable系统中的一行，从而保证REDO日志操作的原子性。客户端写完REDO日志后，事务操作成功，接下来只需要回放REDO日志。如果回放REDO日志失败，比如某些行所在的子表服务器宕机，事务操作也可以成功返回客户端，后续的读操作如果要读取最新的数据，需要先回放REDO日志。\n\n> 用户这一行数据包含他的REDO操作日志，通过这列可以获取到这行数据所表示的实体执行的REDO操作。\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;REDO日志：记录事务修改后的状态。\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bigtable保证单行数据的原子性\n\n单集群实体组之间：实体组之间一般采用分布式队列的方式提供最终一致性，子表服务器上又定时扫描线程，发送跨实体组的操作到目标实体组。如果需要保证多个实体组之间的强一致性，即实现分布式事务，只能通过两阶段提交协议加锁协调。\n\n## 并发控制\n### 读事务\nMegastore提供了三种读模式：最新读取（current read）、快照读取（snapshot read）、非一致性读取（inconsistent read）。\n\n最新读取和快照读取总是在单个实体组内完成。\n\n在开始最新读取前，要确保之前的所有写操作都已经提交并全部生效，然后读取最后一个版本的数据。\n\n对于快照读取，系统取出一个已知的已经完全生效的最新版本并读取。与最新读取不同，他在读取时不要求之前的写操作是否全部生效，即快照读取只读取当前时间点最新的一个已经完全提交并生效的版本数据。（REDO日志同步成功但没有回放完成）。\n\n非一致性读取忽略日志的状态而直接读取Bigtable中最新的值，可能读取到不完整的事务。\n> 最新读取之后 ，如果没有最新的写入，则之后的最新读取与快照读取的结果都将是同一个版本的数据\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;提交：同步REDO日志\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;生效：回放REDO日志\n\n### 写事务\nMegastore事务中的写操作采用了预写式日志（REDO日志），只有当所有操作都在日志中记录下来后，写操作才会对数据执行修改。\n\n在写事务开始前，要执行一个最新读取，用于将之前提交的写事务生效并获取下一个可用的日志位置，将用户操作聚集到日志缓冲区，分配一个更高的时间戳，最后通过Paxos复制协议提交到下一个可用的日志位置。\n\nPaxos协议使用了乐观锁机制：尽管可能有多个写操作同时试图写同一个日志位置，但最后只有一个会成功。其他失败的写操作都会观察到成功的写操作，然后终止并重试。\n> 同一时间针对同一个id的多个提案，最终只会有一个提案被通过。\n\n写事务大致流程如下：\n\t1. 读取：获取最后一次提交的事务的时间戳和日志位置（最新读取）\n\t2. 应用逻辑：从Bigtable读取并且将写操作聚集到日志缓冲区\n\t3. 提交：将缓冲区中的操作日志追加到多个机房的Bigtable集群，通过Paxos协议保证一致性\n\t4. 生效：应用操作日志，更新Bigtable中的实体和索引\n\t5. 清理：删除不在需要的数据\n\n执行方式如下:\n<img src=\"/img/202104/006.png\" width=\"50%\" height=\"50%\">\n> 事务从Megastore的SQL语句，会被解析成针对BIgtable每个子表的操作，并生成针对每个子表的REDO日志，将这些日志提交到对应子表所在的Tablet Server上后由Tablet Server进行回放操作。如果有一张子表回放失败，则所有子表的回放都判定为失败，并将数据恢复成REDO日志之前的数据。\n\n有这样两个操作：\n```bash\nT1: Read a; Read b; Set c = a + b;\nT2: Read a; Read d; Set c = a + d;\n```\n假设事务T1和T2对同一个实体组并发执行，T1执行时读取a和b，T2读取a和d，接着T1和T2同时提交。Paxos协议保证T1和T2中有且只有一个事务提交成功。\n\n假如T1提交成功，T2将重新读取a和b后再次通过Paxos协议提交。对同一个实体组的多个事务被串行化，Megastore之所以能够提供可串行化的隔离级别，得益于定义的实体组数据模型，由于同一个实体组同时进行更新往往很少，事务冲突导致重试的概率很低。\n> 为什么定义的数据模型能够提供可串行化的隔离级别呢？是因为Bigtable保证了对一行的操作只能是原子性吗？但是这已经涉及到生效步骤了，如果是事务提交阶段保证串行化的话只是通过Paxos来保证同一时间针对同一个实体组只有一个事务提案能够被通过。\n\n## 复制\n对于多个集群之间的操作日志同步，主要有两个方案：基于中心节点的强一致性同步、基于Paxos的同步。\n\n基于中心节点：保证了日志同步的强一致性。但是当中心节点宕机时，Slave需要确认中心节点宕机后才能够切换为Master继续提供服务，在这一时间段内是需要停止写服务的。\n\n基于Paxos：Slave可以在假设Master宕机的情况下作为Master发起同步操作，虽然会出现多个节点同时操作的情况，但是Paxos通过选举机制保证了同一时间针对同一个id的提案只能通过一个。\n\nMegastore通过Paxos协议将数据复制到多个数据中心，而且机器故障自动切换不停写服务，保证了高可靠与高可用性。\n\n## 索引\n<img src=\"/img/202104/007.png\" width=\"50%\" height=\"50%\">\n\nMegastore的索引分为两大类：\n* 局部索引（local index）：局部索引是单个实体组内部的，用于加速单个实体组内部的查找。也就是子表的索引。实体组内，数据和局部索引的更新操作是原子的。\n在某个实体组上执行事务操作时，先记录REDO日志，回放REDO日志时，原子地更新实体组内部的数据和局部索引。PhotosByTime就是一个局部索引，映射到Photo表中的(user_id, time)主键\n* 全局索引（global index）：全局索引横跨多个实体组。在Bigtable中相当于一张索引表。PhotosByTag就是一个全局索引。映射到Bigtable就是一张新的索引表，主键为(tag, user_id, photo_id)，即索引字段+Photo数据表主键。\n\n> 横跨多个实体组，那也可能横跨多个存储节点？甚至横跨数据中心\n\n除了这两类之外，Megastore还有其他一些额外的索引特性：\n* STORING子句：通过在索引中增加STORING子句，系统可以在索引中荣誉一些常用的列字段，从而不需要查询基本表，减少一次查询操作。PhotosByTag索引表中冗余了thumbnail_url字段。\n> 全局索引会创建一个新的索引表，查询时先从索引表中查询到数据的主键，然后根据主键查询基本表。如果在这个索引表中添加一些列，可以在根据索引查询这些列的时候减少一次查询基本表的过程，但是要注意不能太多，且冗余的列必须属于热点数据。\n* 可重复索引：Megastore数据某些中某些字段是可重复的，相应的索引就是可重复索引。这就意味着，一行数据可能对应多行索引。PhotosByTag就是重复索引，每个photo可能有不同呢的tag，分别对应不同的索引行。\n> 即tag的部分一行会有多个tag，每个tag都在索引表中有一行，所以一行tag会对应多行索引表。\n\n## 协调者\n### 快速读\nPaxos协议要求读取最新的数据至少需要经过一半以上的副本，然而，如果不出现故障，每个副本基本都是最新的。也就是说，能够利用你本地读取实现快速读取，减少读取延时和跨机房操作。\n\nMegastore引入协调者来记录每个本机房Bigtable实例中的每个实体组的数据是否最新。如果实体组的数据最新，读取操作只需要本地读取，没有跨机房操作。\n\n实体组有更新操作时，写操作需要将协调者记录的实体组状态更新为无效，如果某个机房的Bigtable集群写入失败，需要首先使得相应的协调者记录的实体组状态是无效后写操作才可以成功返回客户端。\n> 有点像Learner，即paxos提案通过之后将这部分信息发送给Learner。协调者作为一个全局的数据版本保存节点，保存的是当前集群中数据的最新版本编号，由于只保留版本号而不保存数据值，因此协调者的IO压力不会很大。\n\n### 协调者的可用性\n每次写操作都涉及协调者，如果协调者出现故障就会导致整个系统不可用。因此当协调者不可用时，需要检测到他的故障并且将其隔离。\n> 通常的故障检测都是心跳，但是在Bigtable中对Tablet Server的故障检测使用的Chubby的互斥锁机制。因此Megastore采用的也是这个方法。\n\n协调者在启动时从数据中心内获取Chubby锁。为了处理请求，协调者必须持有Chubby所。一旦因为出现问题导致锁失效，协调者就会恢复到一个默认的保守状态：认为所有它能看到的实体组都是失效的。\n\n如果协调者的锁失效，写操作可以安全地将其忽略；但是从协调者失效到检测到锁释放有一个短暂的过期时间，这个时间段写操作都会失败，所有的写操作都必须等待协调者的Chubby锁锁过期。\n> 心跳与Chubby互斥锁的使用：心跳主要用于检测服务器节点是否生效，但是Chubby在检测节点是否失效的同时还有一个同一类节点的单一权限问题。因此，Chubby通常用于那些接收处理没有备份数据的节点，即所有的数据都发往这一个节点，它的备节点只是用来提供服务的高可用，而不是用于提高数据的可靠性。\n\n> 例子：Chunk Server通过心跳告知Master自己的存活状态，因为当GFS在写入时，同一份数据能够发往Chunk Server的备节点，并不会都发给他一个。而Tablet Server通过Chubby来告知Master存活状态，因为每个Tablet Server中的子表在整个Bigtabl集群中是唯一的，Tablet Server拥有对子表的完全操作权限，因此当Tablet Server宕机时Master需要获取到这个Tablet Server的互斥锁用于解除它对子表的操作权限，否则就会出现数据不一致性。\n\n> 简单地说：心跳不涉及对数据的任何操作权限，只用于提供存活检测。而互斥锁与租约，在提供存活检测的同时还有对这个节点的权限控制。因为GFS在写入的时候数据不是非得写入到这个Chunk Server上。而Bigtable对子表的操作时只能在对应的Tablet Server上操作。\n\n#### 竞争条件\n除了可用性之外，对于协调者的读写协议必须满足一系列的竞争条件。\n\n失效操作总是安全的，但是生效操作必须谨慎处理。\n\n在异步的网络环境中，消息可能乱序到达协调者。每条生效消息和失效消息都带有日志位置。如果协调者先收到较晚的失效操作再收到较早的生效操作，生效操作将被忽略。\n\n协调者从你启动到退出为一个生命周期，每个生命周期用一个唯一的序号标识。生效操作只允许在最近一次对协调者进行读取操作依赖序号没有发生变化的情况下修改协调者的状态。\n\n> 提交数据最频繁的节点成为Leader。\n\n## 读取流程\n<img src=\"/img/202104/008.png\" width=\"50%\" height=\"50%\">\n\nMegastore的读取流程如下：\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.本地查询。查询本地副本的协调者来决定这个实体组上数据是否已经最新。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.发现位置。确认一个最高的已经提交的操作日志位置，并选择最新的副本，具体操作如下：\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a. 本地读取：如果本地查询确认本地副本已经是最新的，直接读取本地副本已经提交的最高日志位置和响应的时间戳，即快速读取。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b. 多数派读取：如果本地副本不是最新的（或者本地查询、本地读取超时），从多数派副本中读取最大的日志位置，然后从中选取一个响应最快或者最新的副本。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.追赶。一旦某个副本被选中，就采用如下方式使其追赶到已知的最大位置处：\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a. 获取操作日志：对于所选副本中所有不知道Paxos共识值的日志位置，从其他副本中读取。对于所有不确定共识值的日志位置，利用Paxos发起一次误操作的写（Paxos中的no-op）。Paxos协议将会促使大多数副本达成一个共识值：要么是无操作写，要么是以前已提交的一次写操作。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b. 应用操作日志：顺序地应用所有已经提交但还没有生效的操作日志，更新实体组的数据和索引信息。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.使实体组生效。如果选取了本地副本且原来不是最新的，需要发送一个生效消息以通知协调者本地副本中这次读取的实体组已经最新。生效消息不需要等待应答，如果请求失败，下一个读取操作会重试。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.查询数据。在所选副本中通过日志中记录的时间戳读取指定版本数据。如果所选副本不可用了，重新选去一个替代副本，执行追赶操作，然后从中读取数据。\n\n> 什么是共识值？写事务的操作日志。\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;追赶：将多数派读取中那些没有达到最新值的副本的操作日志更新对最新。\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第五步之前的查询都是查询Bigtable中的操作日志，只有到第五步确定了所要查询实体组的时间戳后才会从Bigtable中查询子表数据。因为Bigtable保存的数据是带有时间戳的，只用通过前4步获取到所要查询数据的最新提交生效的时间戳才能获取的正确的数据。\n\n## 写入流程\n<img src=\"/img/202104/009.png\" width=\"50%\" height=\"50%\">\n执行完一次完整的读流程后，下一个可用的日志位置，最后一次写操作的时间戳，以及下一次的主副本（Leader）都知道了。在提交时刻所有的修改操作都被打包，同时还包含一个时间戳、下一次主副本提名，作为提议的下一个日志位置的共识值。如果该值被大多数副本通过，它将被应用到所有的副本中，否则整个事务将中止从读操作开始重试。\n\n> 为什么要重试？此时采用的paxos算法是改进后的multi-paxos，该算法的核心在于第一次accept之后如果成功，则下一次不需要执行prepaer阶段。而在这里，直接进入accept，这个过程中会出现数据不一致，但是只有当协调者的数据更新后，这次修改才会对客户可见\n\n写入过程包括如下几个步骤：\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.请求主副本接受：请求主副本将提议的共识值（写事务的操作日志）作为0号提案。如果想成功，跳转到步骤3.\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.准备：对于所有的副本，运行Paxos协议准备节点，即在当前的日志位置上，用一个比以前所有提议都更高的提议号进行选举。将提议的共识值替换为已知的拥有最高提议号的副本的提议值。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.接收：请求剩余的副本接受主副本的提议，如果大多数副本拒绝这个值，返回步骤2。Paxos协议大多数情况下主副本不会变化，可以忽略准备阶段直接执行这个阶段，这就是Megastore的快速写。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.使实体组失效：如果某些副本不接受多数派达成共识，将协调者记录的实体组状态标记为失效。协调者失效操作返回前写操作不能返回客户端呢，从而防止用户的最新读取得到不正确的结果。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.应用操作日志：将共识值在尽可能多的副本上应用生效，更新实体组的数据和索引信息。\n\n> 这里既然提到将共识值尽可能多的在副本上应用生效，则共识值就是REDO日志记录了。\n\n## 讨论\n分布式存储系统有两个目标：一个是可扩展性，最终目标是线性可扩展；另外一个是功能，最终目标是支持全功能SQL。Megastore是一个介于传统的关系型数据库和分布式NoSQL系统之间的存储系统呢，融合了SQL和NoSQL两者的优势。\n\nMegastore的主要创新点在于：\n* 提出实体组的数据模型。通过实体组划分数据，实体组内部位置关系数据库的ACID特征，实体组之间维持类似NoSQL的弱一致性，有效地融合了SQL和NoSQL两者的优势。另外，实体组的定义方式也在很大程度上规避了影响性能和可扩展性的Join操作。\n* 通过Paxos协议同时保证高可靠性和高可用性，即把数据强同步到多个机房，又能做到发生故障时自动切换不影响读写服务。另外，通过协调者和优化Paxos协议是的读写操作都比较高效。\n\n但是Megastore也有一些问题：来源于Bigtable的单副本服务，SSD支持较弱，整体架构过于复杂，协调者对读写服务和韵味复杂度的影响。因此后续又有一套Spanner架构用于解决这些问题。\n\n> 以用户作为分组的方式，如果某个查询需要查询所有用户的用户信息，则这个查询会横跨多个实体组，甚至是多个网络集群。","tags":["存储"]},{"title":"Gossip 协议","url":"/2021/03/27/Gossip-协议/","content":"Gossip protocol 也叫 Epidemic Protocol （流行病协议）。从名字就可以猜到这个算法的作用主要在服务器集群中进行数据的传播，这种传播方式类似于人群中的流行病一样一传十十传百，随着时间推移接收请求的服务器节点将会越来越多。\n<!-- more -->\n> Gossip protocol 最早是在 1987 年发表在 ACM 上的论文 《Epidemic Algorithms for Replicated Database Maintenance》中被提出。主要用在分布式数据库系统中各个副本节点同步数据之用，这种场景的一个最大特点就是组成的网络的节点都是对等节点，是非结构化网络，这区别与之前介绍的用于结构化网络中的 DHT 算法 Kadmelia。\n\n> 我们知道，很多知名的 P2P 网络或区块链项目，比如 IPFS，Ethereum 等，都使用了 Kadmelia 算法，而大名鼎鼎的 Bitcoin 则是使用了 Gossip 协议来传播交易和区块信息\n\n> 实际上，只要仔细分析一下场景就知道，Ethereum 使用 DHT 算法并不是很合理，因为它使用节点保存整个链数据，不像 IPFS 那样分片保存数据，因此 Ethereum 真正适合的协议应该像 Bitcoin 那样，是 Gossip 协议。\n\nGossip执行过程：Gossip过程由某个种子节点发起，当该节点由状态需要更新到网络中的其他节点时，他会随机选择周围几个节点散播消息，收到消息的节点也会重复这个过程，直至最终网络中所有的节点都收到了消息。这个过程要有一段时间，而且无法保证某个时刻所有的节点都能收到消息，但是能保证最终整个集群中的节点都会收到消息，因此他是一个最终一致性协议。\n\n类似水滴，在水滴滴下后往周围传播波。\n\n由于需要进行大量的网络请求，因此对于Gossip节点来说，他们的网络请求都应该是异步的，而且发出请求后不需要等待响应；因此会产生大量的消息冗余。这就类似于图的广度优先搜索，一圈一圈往外传播。\n\n由于是异步的网络请求，因此Gossip可以十分频繁地进行传播。通常一个请求周期为一秒。Gossip在传播的过程中会选择相邻的节点，通常选择3个节点进行散播。每次收到消息的节点都选择尚未发送过的节点进行散播。收到消息的节点不再往发送节点散播，即A -> B，B在散播的时候不会再发往A。\n\n## 演示如下：\n* 首先节点1发起传播：\n<img src=\"/img/202103/033.png\" width=\"50%\" height=\"50%\">\n传播到2和7后，由2、7继续这一过程\n<img src=\"/img/202103/034.png\" width=\"50%\" height=\"50%\">\n直到整个集群的节点都收到消息\n<img src=\"/img/202103/035.png\" width=\"50%\" height=\"50%\">\n\n## Gossip的特点\n1. 扩展性\n\t网络可以允许节点的任意增加和减少，新增加的节点的状态最终会与其他节点一致\n2. 容错\n\t网络中任何节点的宕机和重启都不会影响Gossip消息的传播，Gossip协议具有天然的分布式系统容错特性\n3. 去中心化\n\tGossip协议不要求任何中心节点，所有节点都可以是对等的，任何一个节点无需知道整个网络状况，只要网络是联通的，任意一个节点就可以把消息散播到全网\n4. 一致性收敛\n\tGossip协议中的消息会以一传十、十传百一样的指数级速度再网络中快速传播，因此系统状态的不一致可以在很快的时间内收敛到一致。消息传播速度达到了logN\n5. 简单\n    Gossip协议的过程及其简单，实现起来几乎没有太多复杂性\n\n## Gossip的缺点\n1. 消息延迟：\n\t节点随机像少数几个几点发送消息，消息最终是通过多个轮次的散播而到达全网，不可避免地造成消息延迟\n2. 消息冗余：\n\t节点定期随机选择周围节点发送消息，而收到消息的节点也会重复该步骤，因此不可避免地引起同一节点多次接收同意消息，增加消息处理的压力。一次通信会对网络带宽、CPU资源造成很大的负载，而这些负载又受限于通信频率，该频率又影响着算法收敛的速度。\n3. 拜占庭问题：\n    如果有一个恶意传播消息的节点，Gossip协议的分布式系统就会出现问题\n\n> 由于没有中心节点，所以没有一个同一的消息校验规则。只能区分消息的正确与否，无法区分消息的好坏。\n\n上诉缺点的本质是因为Gossip是一个带冗余的容错算法，是一个最终一致性算法，无法保证在某一时刻所有节点状态一直，但可以保证“最终所有节点一致”，但这个最终时间是一个理论无法明确的时间点。所以适合AP场景的数据一致性处理，常见的有：P2P网络通信、Apache Cassandra、Redis Cluster、Consul\n\nMárk Jelasity 在它的 [Gossip](http://publicatio.bibl.u-szeged.hu/1529/1/gossip11.pdf)一书中对其进行了归纳：\n<img src=\"/img/202103/036.png\" width=\"50%\" height=\"50%\">\nGossip模块由两个线程组成：活动线程与被动线程。一个用于发起Gossip请求，一个用于接收其他节点传递过来的Gossip请求。\n\n在这两段伪代码中:\n* push表示发起信息交换的节点A随机选择联系节点B，并向其发送自己的信息，节点B在收到信息后更新比自己新的数据，一般有新信息的节点才会作为发起节点。\n* pull表示发起信息交换的节点A随机选择联系节点B，并从对方获取信息。一般无新信息的节点才会作为发起节点\n* push/pull表示发起信息交换的节点A像选择的节点B发送信息，同时从对方获取数据，用于更新自己的本地数据\n\n## 活动线程\n在一个循环中，每经过T段时间就发起一次Gossip请求。\n1. 首先从selectPeer()函数中随机获取一个联系节点p\n2. 如果是push的通信方式\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a. 发送方根据自身的网络信息以及跳数（默认0）生成描述符\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b. 将自身描述符与自身信息合并\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;c. 将合并后的信息发送给p\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;d. 如果没有新消息要发给p则直接发送一个空信号（为什么直接不发送呢？）\n3. 如果是pull的通信方式\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a. 从p中获取他的信息\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b. 给他的跳数递增\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;c. 将p的信息与本地的信息合并\n\n## 被动线程\n1. 从网络队列中获取节点p的消息\n2. 将这个消息的跳数递增\n3. 如何这个消息类型是pull\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a. 接收方根据自身的网络信息以及初始跳数（默认0）生成自身的描述符\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b. 将描述符与自身数据结合后返回给p\n4. 将p的数据跟自身的数据合并\n\n## 总结\n从上面两种方式（pull、push）可以发现Gossip主要有两种请求方式：\n* 将最新的变更传播出去\n* 与其他节点进行数据交换，并将其他节点中的最新数据合并到本地，同时与其交换的结点也会更新到最新数据","tags":["算法"]},{"title":"raft成员变更","url":"/2021/03/24/raft成员变更/","content":"<hr>\n配合[此文](https://mp.weixin.qq.com/s?__biz=MzIzOTU0NTQ0MA==&mid=2247502807&idx=1&sn=c4978ad2968c69d3a9c2f4573d78263d&chksm=e92af6d8de5d7fce822edd4ffbe86a7a92c98ec8cf8c580d4d634ed8c9bcb6a319b0441ad308&scene=178&cur_album_id=1409150425835831296#rd)食用。也是一种读后感吧，既然学习分布式存储了，raft早晚都是要接触的。\n<hr>\n在分布式系统中，如果不是昂贵的设备的话，一般硬件的服务器集群中，节点会时常异常下线。因此大部分的分布式存储系统都要求支持节点动态增删，并在此基础上保证系统服务的稳定性以及数据的一致性。\n<!-- more -->\n成员变更是在集群运行过程中改变运行一致性协议的节点，如增加、减少节点、节点替换等。成员变更过程不能影响系统的可用性。\n\n成员变更过程中对常用的一致性算法，比如paxos、raft会产生一定的影响， 他们加入集群后会扩大集群的多数派集合，但是此时他们的状态与原先节点还没有保持一致，当新加入或者产生变更的节点数量大于等于原有节点数量的时候，就会对集群一致性算法产生影响。\n<img src=\"/img/202103/031.png\" width=\"50%\" height=\"50%\">\n问题如下：当处于画框的时间点并触发选举后，节点1和节点2还是旧的配置文件，因此他们会从节点1～节点3中选出Leader；而节点3～节点5已经是新的配置文件了，他们会从节点3～节点5中选出新的Leader。此时就会出现两个Leader，即脑裂。\n\nraft提出了两阶段的成员变更方法：Joint Consensus。\n# 成员变更\n## Joint Consensus（联合共识模式）成员变更\n新成员配置：C<sub>new</sub>\n\n旧成员配置：C<sub>old</sub>\n\n当Leader需要进行成员变更的时候，首先往所有的Follower发送联合共识日志（Cnew，Cold），所有follower收到后即可生效，不需要等待Leader发送commit命令。\n\n在配置变更的过程中，保留Cold，由于分布式系统中每次元数据变更都会记录对应的版本ID，因此可以很方便的对Cnew与Cold进行区分。\n\nLeader在这期间提交的日志会有两种情况，一种属于原有日志提交，这种时候需要Cnew与Cold两个配置都确认后才能提交；另一种就是只提交Cnew，在这之后系统全部切换为Cnew，所有提案不再经过Cold，而且这一步也是直接生效不需要Leader提交commit。\n\n当Leader宕机的时候，集群中选举出来的新Leader根据自身配置的情况来选择哪种成员变更方式。这里有个问题，在成员变更途中Leader下线后如何选举出全局唯一的Leader？此时集群中的节点会有两种状态Cold，（Cnew，Cold），而为了保证不会出现脑裂现象，当节点中的配置为（Cnew，Cold）时，采用的是Cold配置。\n\n主要的问题：Cnew与Cold节点任意多数派不相交，导致选举出两个Leader。如果存在相交，那么Cnew与Cold节点就无法各自形成多数派。\n\n## 单步成员变更\n造成上诉问题的原因在于同时对所有节点进行变更配置的时候由于网络先后等因素导致一部分节点配置已变更，另一部分没有变更。\n\n既然如此只要保证每次变更一定抵达，并且在一次变更完成之后再发送下一个变更请求就可以避免新旧配置多数派存在交集。即，每次成员变更只允许增加或删除单个成员。\n<img src=\"/img/202103/032.png\" width=\"50%\" height=\"50%\">\n上图中，颜色不同的框表示应用了不同成员配置的节点，可以看到，每次变更一个成员后，两种配置成员中必定会有一个交集。\n\n如果集群中有大量的成员需要变更，那这个时间就十分漫长。\n\n# Raft单步成员变更的问题\n## Raft单步成员变更的正确性问题\n这个问题在于上一个Leader没有同步之后重新上线同步日志，会导致这期间选出来的新Leader的相关日志被抹除，但是如果采用操作追加的话不就没有这个问题了吗？Leader上线后如果检测到变更没有同步到其他节点，那么将变更的操作追加到其他节点的操作日志上，由其他节点进行恢复，如果恢复过程中发现Follower的同步日志与Leader的同步日志不一致，就将其同步回Leader。\n\n这里还有个原因就是集群中Leader下线后如果重新上线，那他的身份还是属于Leader，可以继续执行下线前的操作。\n\nraft的解决办法是新Leader在同步成员变量前要往集群中发送一条no-op日志，用于发现上一任Leader未提交的成员变更日志。这步操纵属于多数派写，则根据单步成员变更原则，必然能够与上一任Leader未提交的成员变更日志至少有一个交集，有了这个标记后，当上一任Leader重新上线并尝试同步日志的时候会发现他所属的节点中存在no-op记录，则使得新上线的Leader凭证失效，重新成为普通节点。\n\n## Raft单步成员变更的可用性问题\n在进行单步成员变更的时候，如果集群中某个节点出现问题需要被新的节点替换，那“删除旧的节点”与“添加新的节点”也会是独立成两个操作。\n\n如果出现二分网络分区的情况，由于两个节点位于同一机房，另外两个节点和他们不在同一个网络拓扑结构上，会导致位于同一机房的两个节点与另外的两个节点状态不一致，此时跟上述问题一样，两种状态的节点集合他们的多数派有可能不相交导致脑裂问题。\n\n这种情况是由于新旧节点同时存在集群中导致的，因此在进行单步成员变更的时候先删除旧节点再添加新节点即可，原因就是新旧两种类型的多数派一定会有相交。\n\n另一种方法就是采用Joint Consensus，使得集群中存在中间状态，也能保证不会出现脑裂问题。\n\n# Raft成员变更的工程实践\n在工程上多数采用Joint Consensue，可能是因为采用另一种方法，如果删除了旧节点但是新节点加入失败的时候会造成数据丢失？\n\n至于后面的工程实践，我没工程经验只能看看了눈_눈","tags":["分布式"]},{"title":"分布式存储笔记3-4 分布式表格系统（Google Bigtable）","url":"/2021/03/22/分布式存储笔记3-4-分布式表格系统（Google-Bigtable）/","content":"<hr>\n这是一篇在阅读《大规模分布式存储系统：原理解析与架构实战》时的阅读笔记，由于长时间碎片阅读的关系导致在做这种读书笔记的时候接近复制粘贴。虽然其中会有一小部分自己的想法但都十分零碎，希望后续能改进。\n<hr>\n\n分布式表格系统对外提供表格模型，每个表格由很多行组成，通过主键唯一标识，每一行包含很多列。整个表格在系统中全局有序。\n\nGFS+Bigtable双层架构是一种里程碑式的架构，但是Bigtable对外结构不够丰富，因此后续又推出了构建在Bigtable上的Megastore以及支持跨多个数据中心的数据库事务的Spanner。\n<!-- more -->\n# Google Bigtable\nBigtable是Google开发的基于GFS和Chubby的分布式表格系统。他存储了大量结构化和半结构化数据。\n\nBigtable系统由很多表格组成，每个表格包含很多行，每行通过一个主键（Row Key）唯一标识，每行又包含很多列（Column）。某一行的某一列构成一个单元（Cell），每个单元包含多个版本的数据。\n> 需要和常用的关系型数据库区分，在关系型数据库中“某一行的某一列”表示的是数据的值，而在Bigtable中则表示某个值的多个版本对象。\n\n整体上看，Bigtable是一个分布式多维映射表。\n<img src=\"/img/202103/026.png\" width=\"50%\" height=\"50%\">\n\n> 其中的时间戳就是某一行某一列的各个版本号，通过版本号获取这个值当时的信息。即通过<行，列>确定数据的对象，通过时间戳确定数据的版本。\n\nBigtable将多个列组织成列族（column family），这样，列由两个部分组成：（column family，qualifier）。列族是Bigtable中访问控制的基本单元，也就是说，访问权限的设置是在列族这一级别上进行的。\n\nBigtable中的列族在创建表格的时候需要预先定义好，个数不允许过多；但是列族中包含哪些qualifier是不需要预先定义的，qualifier可以任意多个，适合表示半结构化数据。\n* 结构化数据：结构化数据可以使用关系型数据库来表示和存储，如MySQL、Oracle、SQL Server等，表现二维形式的数据。可以通过固有键值获取相应信息。一般特点是：数据以行为单位，一行数据表示一个实体的信息，每一行数据的属性是相同的。结构化的数据的存储和排列是很有规律的，这对查询和修改等操作很有帮助。但是，显然，它的扩展性不好（比如，我希望增加一个字段）。\n* 非结构化数据：非结构化数据,就是没有固定结构的数据，包含全部格式的办公文档、文本、图片、XML、HTML、各类报表、图像和音频/视频信息等等。一般直接整体进行存储，而且一般存储为二进制的数据格式\n* 半结构化数据：半结构化数据可以通过灵活的键值调整获取相应信息，且数据的格式不固定，如json，同一键值下存储的信息可能是数值型的，可能是文本型的，也可能是字典或者列表。半结构化数据，属于同一类实体可以有不同的属性，即使他们被组合在一起，这些属性的顺序并不重要。常见的半结构数据有XML和JSON。\n\nBigtable中的行主键可以是任意的字符，最大不超过64KB。Bigtable表中的数据按照行主键进行排序，排序使用的是字典序。\n\n对于域名`www.cnn.com`，在存储的时候将其域名修改为com.cnn.www，这样的好处是使得所有`www.cnn.com`下的子域名在系统中连续存放。\t\n> 即com.cnn.www、com.cnn.A.www、com.cnn.B.www这样的排布方式，使得主域名作为排序的前缀，由于主域名不会重复，因此在字典序下主域名下所有的子域名都连续存放。\n\n<img src=\"/img/202103/027.png\" width=\"50%\" height=\"50%\">\n这一行包含两个列族：“contents”和“anchor”，其中，列族“anchor”又包含两个列，qualifier分别为“cnnsi.com”和“my:look.ca”。\n\n谷歌的很多服务，比如Web检索和用户的个性化设置，都需要保存不同时间的数据，这些不同的数据版本必须通过时间戳来区分。\n> 那应该会有一个时间戳列表或者索引，不然怎么知道是哪个时间戳才保存了数据。\n\n途中contents的t1、t2、t3分别保存了三个时间点获取的网页。为了简化不同版本呢的数据管理，Bigtable提供了两种设置：一种是保留最近的N个不同版本，另一种是保留限定时间内的所有不同版本，比如可以保存最近10天的所有不同版本的数据。失效的版本将会由Bigtable的垃圾回收机制自动删除。\n\n## 架构\nBigtable构建在GFS之上，为文件系统增加一层分布式索引层。另外，Bigtable依赖Google的Chubby（分布式锁服务）进行服务器选举及全局信息维护。\n<img src=\"/img/202103/028.png\" width=\"50%\" height=\"50%\">\n\nBigtable将大表划分为大小在100～200MB的子表（tabler），每个子表对应一个连续的数据范围。\n\nBigtable主要由三个部分组成：客户端程序库（Client）、一个主控服务器（Master）、多个子表服务器（Tablet Server）\n* Client：提供Bigtable到应用程序的接口，应用程序通过客户端程序库对表格的数据单元进行增、删、查、改等操作。客户端通过Chubby锁服务获取一些控制信息，但所有表格的数据内容都在客户端与子表服务器之间传送。\n* Master：管理所有子表服务器，包括分配子表给子表服务器，指导子表服务器实现子表的合并，接受来自子表服务器的子表分裂消息，监控子表服务器，在子表服务器之间进行负载均衡并实现子表服务器的故障恢复等。\n* Tablet Server：实现子表的装载/卸出、表格内容的读和写，子表的合并和分裂。Tablet Server服务的数据包括操作日志以及每个子表上的sstable数据，这些数据存储在底层的GFS中。\n\nBigtable依赖于Chubby锁服务完成如下功能：\n    1. 选取并保证同一时间内只有一个主控服务器\n    2. 存储Bigtable系统引导信息\n    3. 用于配合主控服务器发现子表服务器加入和下线\n    4. 获取Bigtable表格的schema信息及访问控制信息\n\n> 1可以理解，通过锁服务进行选举得到唯一的Master；但是2，锁服务是如何配合Bigtable存储Bigtable系统的引导信息的呢？；3，Tablet Server不是通过心跳与Master维持通信来告知存活的吗？这个节点Chubby是如何加入进来的呢？难道是在Chubby存储当前主Master的信息，而Tablet Server的心跳信息是直接发放Chubby然后再由Chubby转发到当前生效的主Master中的吗？；4，锁服务可以执行访问控制。\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PS:2、在Chubby服务器上存储元数据服务器的元数据信息。3、TabetServer通过持有互斥锁来告知Master是否在线。4、同2\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为什么不用心跳。因为互斥锁比心跳更能判断是否存活，心跳如果接收不到可能是网络原因，如果Master将其判断为下线并执行表的复制，但实际上该TabletServer并没有下线，因此会导致有两张相同的子表同时对外提供服务，会造成数据的不一致性\n\nChubby是一个分布式锁服务，底层的核心算法为Paxos。Paxos算法的实现过程需要一个“多数派”就某个值达成一致，进而才能得到一个分布式一致状态。只要半数以上的节点不发生故障，Chubby就能够正常提供服务。\n\nBigtable包含三种类型的表格：用户表（User Table）、元数据表（Meta Table）和根表（Root Table）。\n* 用户表：存储用户实际数据\n* 元数据表：存储用户表的元数据，如子表位置信息、SSTable及操作日志文件编号、日志回放点等\n* 根表：用来存储元数据表的元数据\n\n根表的元数据，也就是根表的位置信息，又称为Bigtable引导信息，存放在Chubby中。客户端、主控服务器以及子表服务器执行过程中都需要依赖Chubby服务，如果Chubby发生故障，Bigtable系统整体不可用。\n\n## 数据分布\n<img src=\"/img/202103/029.png\" width=\"50%\" height=\"50%\">\n客户端在查询时先查询Chubby获取根表信息，然后从根表获取元数据表最后查询到用户表获取数据。为了加快查询速度使用了缓存和预取技术。子表的位置信息被缓存在客户端，客户端在寻址时首先查找缓存，如果不存在则向上请求元数据表的缓存，如果也不存在则继续向上请求根表的缓存，还是不存在的话就按照原先顺序冲Chubby中开始向下查询。\n\n## 复制与一致性\nBigtable系统保证强一致性，同一时刻同一子表只能被一台Tablet Server服务，也就是说，Master将子表分配给某个Tablet Server服务时需要确保没有其他Tablet Server正在服务这个子表。这一过程可以通过互斥锁来实现，且为了防止死锁，互斥锁也会有一个过期时间\n> 这点就类似于租约机制，同一时刻只能有一台Chunk Server提供服务，当拥有租约的服务器宕机后，Master会在租约过期后重新进行选举\n\nBigtable写入GFS的数据分为两种：\n* 操作日志：当Tablet Server发生故障时，它上面服务的子表会被集群中的其他Tablet Server加载继续提供服务。加载子表可能需要回放操作日志，每条操作日志都有唯一的序号，通过它可以去除重复的操作日志。\n* 每个子表包含的SSTable数据。如果写入GFS失败可以重试并产生多条重复记录，但是Bigtable只会索引最后一条成功写入的数据。\n\n第二条解决了GFS遗留的一致性问题，它只会获取一致性写入后的一条数据，将那些重复与填充数据全部排除。\n\n## 容错\nBigtable中Master对Tablet Server的监控是通过Chubby完成的，Tablet Server在初始化时都会从Chubby中获取一个独占锁。通过这种方式所有的Tablet Server基本信息被保存在Chubby中一个成为服务器目录（Server Director）的特殊目录之中。Master通过检测这个目录随时获取最新的Tablet Server信息，包括目前正在活跃的Tablet Server，以及每个Tablet Server上现已经分配的子表。\n> 这个独占锁也和租约很像，为了防止死锁，Tablet Server的独占锁肯定会有一个过期时间，而为了减少Chubby的工作开销，肯定是要快过期的时候Tablet Server往Chubby请求申请续租。\n\nMaster通过定期往Chubby检查Tablet Server独占锁的状态来判断Tablet Server的存活状态，这时会有两种情况：Chubby出问题、Tablet Server出问题。Master首先会尝试获取这个锁，如果获取失败则表示Chubby出问题，否则就是Tablet Server出问题。Chubby 出问题表示整个Bigtable都无法提供服务所以只能由人工维护；Tablet Server出问题的话，Master将终止这个Tablet Server并将其上的子表全部迁移到其他Tablet Server。\n\n每个子表持久化的数据包含两部分：操作日志以及SSTable。\n\n为了提高性能，Tablet Server不会为每一份子表都维护一份独立的操作日志，而是将所有子表的操作都写进GFS中，每条日志通过〈表格编号，行主键，日志序列号〉来唯一表示。\n\n当某个Tablet Server宕机后，Master将该Tablet Server服务的子表分配给其他Tablet Server。为了减少Tablet Server从GFS读取的日志数据量，Master 将选择一些Tablet Server对日志进行分段排序。排好序后，同一个子表的操作日志连续存放，Tablet Server恢复某个子表时只需要读取该子表对应的操作日志即可。Master需要尽可能选择负载低的Tablet Server执行排序，并且需要处理排序任务失败的情况。\n> Tablet Server在写入操作日志的时候为了性能可以一股脑一起写入，但是在恢复Tablet Server数据并写需要重现日志的时候是按照子表来恢复的，因此需要将写入的混合日志分离出来并排序，并且以子表为单位进行恢复。这样做的原因是进行子表转移分配的时候它的基本单位也是子表，而不是多条操作日志。同时这是对追求写入所产生的后果进行的填补。\n\n> 如何排序？在GFS中是没有排序逻辑的，因此需要将操作日志全部取出来并且分配到各个空闲的Tablet Server来为这部分数据排序，这里要求排序要将数据拆分后排序，可以在排好序后建立一个子表与排序集合的映射表方便组合最终的结果。\n\nMaster也会和Tablet Server一样，在启动时获取一个独占锁，如果锁过期则表示Master宕机，那么Chubby将会选择备Master恢复服务。\n\nMaster如Chubby的不同：\n* Chubby主要用于保存元数据表的元数据，分布式锁服务，检测各个成员状态等功能。\n* Master主要用于执行在分布式存储系统中遇到的各种问题，比如数据备份、容错恢复等操作。\n\n一个偏向于存储与管理生命周期，一个偏向于管理集群日常事务。\n\n## 负载均衡\n子表是Bigtable负载均衡的基本单位，与GFS的chunk一样。\n\nTablet Server定期向Master汇报状态。\n\n当Master的状态检测时发现某个Tablet Server上的负载过重时，Master会自动对其进行负载均衡，即执行子表迁移工作。\n\n子表迁移分两步：\n\t1. 请求原有的Tablet Server卸载子表；\n    2. 选择一台负载较低的Tablet Server加载子表。\n\n> 由于所有的数据都是构建在GFS之上的，所有Bigtable对子表的迁移实际上迁移的就是表结构以及GFS中数据索引与偏移量相关的数据，并不会执行普通数据库一样的大量数据迁移。\n\nMaster发送命令请求原有的Tablet Server卸载子表时需要首先获取互斥锁。如果原有Tablet Server发生故障，新的Tablet Server需要等待原有Tablet Server加载子表上的互斥锁过期。\n> 这样来看Bigtable中的互斥锁有两种，一种就是锁定整个节点的锁，Master用这种互斥锁来确认Tablet Server是否存活；另一种就是锁住整个子表，Tablet Server通过这种锁来获取对子表操作的唯一权限。\n\n在迁移前会对原有Tablet Server执行Minor Compaction操作，将内存中的更新操作以SSTable文件的形式转出到GFS中\n\nTablet Server不需要回放操作日志。\n\n由于不需要回放操作日志，因此在子表的迁移过程中将会停止该子表的服务，为了尽可能减少停服务的时间，Bigtable内部采用两次Minor Compaction的策略：\n    1. 原有Tablet Server对子表执行一次Minor Compaction操作，操作过程仍然允许写操作\n    2. 停止子表的写服务，对子表再执行一次Minor Compaction操作。由于第一次Minor Compaction过程中写入的数据一般比较少，第二次Minor Compaction的时间会比较短。\n\n> 从“暂停并将内存中的所有数据写入”简化为了“暂停服务，并将上一次Minor Compaction执行期间的操作日志写入”。明显是后者的时间短。\n\n由于子表迁移过程会停止一段时间的服务，因此负载均衡策略不宜过于激进。涉及的因素有：TabletServer读、写个数、磁盘、内存负载等信息。\n\n## 分裂与合并\n随着数据的写入与删除，某些子表可能太大，某些子表可能太小，需要执行子表 分裂与合并操作。\n> 又不改动GFS中的数据，应该就是对Tablet Server中保存的子表的元数据进行一个分裂。\n\n顺序分布与哈希分布的区别在于哈希分布往往是静态的，而顺序分布是动态的，需要同构分裂与合并操作动态调整。\n> 哈希分布很难排序与遍历，因此不方便对数据进行分裂\n\nBigtable每个子表的数据分为内存中的MemTable和GFS中的SSTable。\n\n同一个子表只能被一台TabletServer服务，因此不会存在子表的备份。如果子表宕机，将会从元数据节点中查询子表的元数据并从GFS中读取恢复。\n\nBigtable上执行分裂操作不需要进行实际的数据拷贝工作，只需要将内存中你的索引信息分为两份，比如分裂前子表的范围为（起始主键，结束主键]，在内存中将索引分为（起始主键，分裂主键]和（分裂主键，结束主键]两个返回。\n\n分裂以后两个子表各自写不同的MemTable，等到执行Compaction操作时再根据分裂后的子表范围生成不同的SSTable，无用的数据被回收。\n\n分裂操作由Tablet Server发起，需要修改元数据（包括元数据表、根表）。Bigtable保证在元数据中添加一行为原子性事物。只要修改元数据成功，分裂操作就算成功。\n\n分裂成功后Tablet Server向Master报告，如果出现Tablet Server故障，Master可能会丢失汇报分裂的消息。但是当Tablet Server重新上线并从磁盘与元数据表中重新构建子表元数据时会通过定时的汇报将这个修改告知Master。\n\n合并由Master发起，比起分裂要更加复杂，因为可能涉及到多个Tablet Server。所以合并的第一步需要将待合并的子表迁移到同一台Tablet Server上，然后通知Tablet Server执行子表合并。\n> 子表合并一个最大的问题就是主键不连续。而随着子表的合并，在元数据表中按道理说要删除n-1行数据，由此，如何分配并管理合并后的子表主键就是一个问题。\n[论文原文](https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/bigtable-osdi06.pdf)\n\n> 2021/03/25追加：合并或许不需要修改根表，只需要修改根表到子表的映射即可，跟Facebook Haystack一样，修改的是映射关系，即一张子表由两段不相交的连续的主键组成，在根表中有两个引用指向该表。\n\n## 单机存储\n<img src=\"/img/202103/030.png\" width=\"50%\" height=\"50%\">\nBigtable采用Merge-dump存储引擎。数据写入是需要先写操作日志，成功后应用到内存中的MemTable中，写操作日志是往磁盘中的日志文件追加数据，很好地利用了磁盘设备的特性。\n\n> 写日志也是往GFS中写，这样可以方式Tablet Server永久下线导致的数据丢失，实际上在Tablet Server中已经不会有独属于它自身的持久化数据，MemTable与SSTable的数据都通过操作日志的形式在GFS中留有备份，他的元数据信息也是存在GFS中，而元数据的索引保存在上级元数据节点中。\n\n类似于LevelDB的写入方法。当内存中的MemTable过大时，将MemTable转储（Dump）成SSTable文件保存到GFS中。\n\n由于数据同时存在MemTable与多个SSTable中，读取操作需要按从旧到新的时间顺序合并SSTable和内存中的MemTable数据。数据在SSTable中连续存放，硬刺可以同时满足随机读取和顺序读取两种需求。\n\n为了方式SSTable数量过多，需要通过compaction过程将其合并为一个SSTable，从而减少后续读操作需要读取的文件个数。\n\n> 这个SSTable是保存在GFS中，但是在保存的时候GFS会将数据的副本放置在tabletserver本地磁盘中，因此这种合并操作可以是在Tablet Server本地执行完成后统一写入到GFS中。通过Compaction直接在GFS中合并SSTable。\n\n增删改查插入等操作在Merge-dump引擎中都看成一回事，除了最早生成的SSTable外，SSTable中记录的只是操作，而不是最终的结果，需要等到读取时才合并得到最终结果。\n\nBigtable中包含三种Compaction策略：Minor Compaction、Merging Compaction、Major Compaction。\n* Minor Compaction：把内存中的MemTable转储到GFS中\n* Merging Compaction：合并GFS中的多个SSTable文件生成一个更大的SSTable。\n* Major Compaction：与Merging Compaction执行的操作一样都是将GFS中的多个SSTable合并成一个，但是不同的是他会合并所有的SSTable文件和内存中的MemTable，生成最终结果，而Merging Conpaction生成的SSTable文件可能包含一些操作，比如删除、增加等。\n\n经过Major Compaction之后，Tablet Server中的数据会按照主键有序存储。每个SSTable由若干个大小相近的数据块（Block）组成，每个数据块包含若干行。\n\n数据块的大小一般在8 ～ 64KB之间，可以后期配置。\n\nTablet Server的缓存包含两种：块缓存（Block Cache）和行缓存（Row Cache）。其中，块缓存的单位为SSTable中的数据块，行缓存的单位为一行记录。\n\n随机读取时，首先查找行缓存；如果读取的数据行在SSTable中不存在，可以通过布隆过滤器发现，从而避免一次读取GFS文件的操作。\n\n## 垃圾回收\nCompaction后生成新的SSTable，原有的SSTable成为了垃圾需要被回收掉。每个子表正在引用的SSTable文件保存在元数据中。因此这个任务需要由Master执行或者参与。\n\nMaster定期执行垃圾回收任务，这是一个标记删除的过程。首先扫描GFS获取所有的SSTable文件，接着扫描所有子表与元数据获取他们正在引用的SSTable文件，如果GFS中的SSTable没被任何一个子表使用，说明可以被回收掉。\n> 如果刚好遇到Tablet Server下线并且Master还没有获知这一情况下，此时一部分有效SSTable将没有被任何子表引用。这个问题的解决办法在于子表对SSTable的引用需要有一个过期时间，而不是实时引用，这个过期时间需要设置的比Tablet Server持有的互斥锁所过期的时候要长或者相同。这样，发生这种情况的时候这部分SSTable的引用依旧存在，而当引用过期的时候Tablet Server已经下线的情况早就被Master所得知了，也不会随便开启垃圾回收，或者在开启垃圾回收的时候将这部分SSTable判断为非垃圾数据。\n\n由于Tablet Server执行Compaction操作生成一个全新的SSTable与修改元数据这两个操作不是原子的，垃圾回收需要避免删除刚刚生成但还没有记录到元数据中的SSTable文件。一种简单的做法就是垃圾回收只删除至少一段时间，比如1小时没有被使用的SSTable文件，这样上面的那个问题也同时被解决了。\n\n## 讨论\nGFS+Bigtable两层架构以一种很优雅的方式兼顾系统的强一致性和可用性。\n\n底层文件系统GFS是弱一致性，可用性和性能好，但是多客户端追加可能出现重复记录等数据不一致问题；\n\n表格系统Bigtable通过多级分布式索引的方式使得系统对外整体表现为强一致性。\n\nBigtable最大的优势在于线性可扩展，单台机器出现故障可以将服务迅速迁移到整个集群。但同时Bigtable也面临一些问题：\n* 单副本服务。Bigtable架构非常适合离线或者半线上应用，然而，TabletServer节点出现故障时部分数据短时间内无法提供读写服务，不适合实时性要求特别高的业务，比如交易类业务\n    * 不过谷歌最开始用Bigtable就是为了存储网页信息\n* SSD使用。谷歌整体架构的设计理念为通过廉价机器构建自动容错的大集群，然而，随着SSD等硬件技术的发展，机器宕机的概率变得更小，SSD和SAS混合存储也变得比较常见，存储和服务分离的架构已经不太适应当前时代\n* 架构的复杂性导致BUG定位难。Bigtable依赖GFS和Chubby，这些依赖系统本身比较复杂，另外，Bigtable多级分布式索引和容错等机制内部实现都非常复杂，工程量巨大，使用的过程中如果发现问题很难定位。\n\n","tags":["存储"]},{"title":"分布式存储笔记笔记3-3 分布式键值系统（Amazon Dynamo & Tair）","url":"/2021/03/19/分布式存储笔记3-3-分布式键值系统（Amazon-Dynamo-Tair）/","content":"<hr>\n这是一篇在阅读《大规模分布式存储系统：原理解析与架构实战》时的阅读笔记，由于长时间碎片阅读的关系导致在做这种读书笔记的时候接近复制粘贴。虽然其中会有一小部分自己的想法但都十分零碎，希望后续能改进。\n<hr>\n\n分布式键值模型可以看成是分布式表格模型的一种特例。由于它只支持针对单个key-value的增删改查（随机查找）操作，因此适用哈希分布算法。\n\n学习Dynamo的设计对学习分布式系统理念很有帮助。但是这个系统的主要价值在学术层面，从工程的角度来看，它牺牲了一致性，却没有换来什么好处。\n<!-- more -->\n# Amazon Dynamo\nDynamo以很简单的键值方式存储数据，不支持复杂的查询。Dynamo存储的是数据的原始形式，不解析数据的具体内容。Dynamo是一个P2P结构的分布式存储模型，而不是常用的中心节点模型。\n<img src=\"/img/202103/018.png\" width=\"50%\" height=\"50%\">\n\n## 数据分布\nDynamo采用一致性哈希算法将数据分布到多个存储节点中。\n\n考虑到节点的异构性，不同节点之间处理能力的差别会很大，Dynamo使用了改进的一致性哈希算法：每个物理节点根据其性能的差异分配多个token，这样，性能高的节点他的哈希选中率也会提高。\n<img src=\"/img/202103/019.png\" width=\"50%\" height=\"50%\">\n如果新增了存储节点，只需要将对应的token分配到该节点上即可。\n\n为了找到数据所属的节点，要求每个节点维护一定的集群信息用于定位。Dynamo系统中每个节点维护整个集群你的信息。\n> 因为是P2P结构，所以要求一个节点必要时能够和其他所有节点通信。\n\n所有节点每隔固定时间（比如1s）通过Gossip协议的方式从其他节点中任选一个与之通信的节点。如果连接成功，双方交换各自保存的节点信息。\n\n[Gossip协议](https://zhuanlan.zhihu.com/p/41228196)用于P2P系统中自治的节点协调对整个集群的认知，比如集群的节点状态、负载情况。\n* A告诉B其管理的所有节点的版本（包括Down状态和Up状态的节点）\n* B告诉A那些节点的版本比较旧了，哪些版本它有最新的，然后把最新的节点状态发给A（处于Down状态的节点由于版本没有发生更新则不会被关注）\n* A将B中比较旧的节点告诉B，同时将B发送来的最新节点信息在本地更新\n* B收到A发来的最新节点后更新本地的元数据\n\n>  双方各自比较对方的节点版本并用对方的新版本替换自身的旧版本。\n\n由于种子节点的存在，新节点的加入可以做的比较简单。新节点加入时首先与种子节点交换集群信息，从而对集群有了认识。DHT（一致性哈希表）环中原有的其他节点也会定期和种子节点交换集群信息，从而发现新节点的加入。\n\n> 这样看来种子节点起到了一个类似中心的作用，不过它的功能不在于完成存储业务，而是负责节点集群之间信息的交互。\n\n每个节点需要定期通过Gossip协议同其他节点交换集群信息，如果发现某个节点很长时间状态都没有更新，则认为该节点已经下线了。\n\n> 这里对于节点的存活与否依赖于其他节点的检查，而不是与种子节点建立心跳通信。这样的好处就是减少了种子节点的业务复杂度与网络压力。\n\n## 一致性与复制\n为了处理节点失效的情况（DHT环中删除节点），需要对节点的数据进行复制。\n> 这种需要中心节点处理的流程也是交给存储节点吗？还是交给种子节点？\n\n假设数据存储N份，DHT定位到的数据所属节点为K，则数据存储在节点K，K+1，…，K + N上。如果第K + i（0 ≤ i ≤ N-1）台机器宕机，则往后找一台机器K+N临时替代。如果第K+i台机器重启，临时替代的机器K+N能够功过Gossip协议发现，他会将这些临时数据归还K+i，这个过程在Dynamo中叫做数据回传。\n> 这里一个问题就是“临时数据”属于什么数据？当第K + N台机器接入的时候，它本身是没有数据的，因此除非接入的时候就进行一次数据的复制工作，否则机器K+N的缓存无法命中，如果不进行数据复制，那么这里的数据将会是后续新添加的缓存，但是既然数据已经分多台机器存储备份，那么当K+i号机器重新上线后也可以直接从临近的机器上同步数据。\n\n在这台机器下线的时间段内，所有的读写均落入到机器[K, K + i - 1]和[K + i + 1, K + N]中，如果机器K + i永久失效，机器K+N需要进行数据同步操作。这个过程通过Merkle树对机器的数据文件进行快速同步。\n\nNWR是Dynamo中的一个亮点，其中N表示复制的备份数，R指成功读操作的最少节点数，W指成功写操作的最少节点数。只要满足W + R > N，就可以保证当存在不超过一台机器故障的的时候，至少能够读到一份有效数据。\n> 像什么？多数派写！当写数量 + 读数量 > 节点总数量则能够保证一定有一次成功的写入被读取到。但是这里的数据必须要有一个版本号或者携带时间戳用于区分数据的新旧。\n\n在P2P这样的集群中，由于每个节点存储的集群信息有所不同，可能出现同一条记录被多个节点同时更新的情况，无法保证多个节点之间的更新顺序，为此Dynamo引入向量时钟（Vector Clock）的技术手段来尝试解决冲突。\n<img src=\"/img/202103/020.png\" width=\"50%\" height=\"50%\">\nDynamo中的向量时钟用一个[nodes, counter]对表示。其中nodes表示节点，counter是一个计数器，初始为0，节点每次更新操作加1。\n\n**例如：**\n首先，Sx对某一个对象进行一次写操作，产生一个对象版本D1([Sx, 1])，接着Sx再次操作，counter值更新为2，产生第二个版本D2（[Sx， 2]）；之后，Sy和Sz同时对该对象进行写操作，Sy将自身的信息加入向量时钟产生了新的版本D3（[Sx，2]，[Sy，1]），Sz同样产生了新的版本信息D4（[Sx，2]，[Sz，1]），这时系统中就有了两个冲突的版本。最常见的冲突解决方法有两个：一种是通过客户端逻辑来解决，比如购物车应用；另外一种常见的策略是“last write wins”，即选择时间戳最新的副班，然而这个策略以来集群内节点之间的时钟同步算法，不能完全保证准确性。\n> 对每一个线程对同一个对象的操作都生成一个独立的版本，这样就会导致N个线程同时写入就会有N个版本。这里就跟paxos不一样了，在paxos中，如果要执行写入的rnd小于last_rnd，那么就拒绝写入，而在Dynamo中都是先写入后执行解决冲突策略。\n\n> 存储节点之间的时钟必然会有误差，如果误差过大比如超过5秒，那么一致性就很难保证。\n\n向量时钟不能完美解决冲突，即使N + W > R，Dynamo也只能保证每个读取操作能读到所有的更新版本，这些版本可能冲突，需要进行版本合并。Dynamo只保证最终一致性，如果多个节点之间的更新顺序不一致，客户端可能读取不到期望的结果。这个不一致的问题影响到了应用程序的设计和对整个系统的测试工作。\n\n> 从上面会产生版本冲突的问题来看，在Dynamo中，一个数据被分布到多个节点上，并不会使得某一个节点成为主节点用于提供服务，而是所有的节点都会提供对这个数据的增删改服务，这能够极大程度增加系统承压能力，但是也导致了同一条数据多个不同节点之间数据的不一致性。\n\n## 容错\nDynamo把异常分为两种类型：临时性的异常和永久性异常。\n\n这种分类是依据异常的持续时间划分，比如机器假死属于临时性异常；硬盘报修或者机器报废就是永久性异常。\n\n### 数据回传\n同上文，当下线的机器恢复后第K+N台机器会通过Gossip协议发现并启动传输任务将暂存的数据回传给机器K+i。\n\n### Merkle树同步\n如果超过了时间T机器K+i还是处于宕机状态，这种异常被认为是永久性的。这时需要借助Merkle树机制从其他副本进行数据同步。\n> 应该要从多台机器中获取备份并将冲突解决后作为自身的最终数据，因为Dynamo无法保证强一致性，因此同一份数据不同备份之间可能不一致，所以K+N在获取备份的时候应该要“货比三家”。\n\nMerkle树中每个非叶子结点对应多个文件，为其所有子节点组合以后的哈希值；叶子结点对应单个数据文件，为文件内容的哈希值。这样，任何一个数据文件不匹配都将导致从该文件对应的叶子结点到根结点的所有节点值不同。每台机器对每一段范围的数据维护一颗Merkle树，机器同步时首先传输Merkle树信息，并且只需要同步从根到叶子的所有节点值均不相同的文件。\n> 即，每个非叶子结点的哈希值都是它下属所有文件组合后的哈希值，每个叶子结点都是其对应的文件内容的哈希值。这样，当某个文件不同步时，必然会有一条从根结点到叶子结点的树与其他Merkle树不一致，此时只需要同步这条路径上的哈希值与叶子结点的文件即可。\n> 这种方法在于能够高效快速地筛选出两颗Merkle之间的不同，但是如何同步，或者说当两条路径不一致时该选用哪一条路径为主就需要另外的解决冲突逻辑。通常如同上文说的一样可以由客户端代码实现，也可以选用最新修改的数据。\n\n### 读取修复\n假设N=3，W=2，R=2，机器K宕机，可能有部分写操作已经返回客户端成功了但是没有完全同步到所有副本，如果机器K出现永久性异常，导致三个副本之间的数据不一致。客户端的读取操作如果发现了某些副本版本太老，则启动异步的读取修复任务。该任务会合并多个副本的数据，并使用合并后的结果更新过期的副本，从而使得副本之间保持一致。\n> 也就是上文所说，Dyamo无法保证所有节点中同一数据的一致性，但是能保证最新的数据必定已被写入成功，因此当R+W>N时，会读取到多个不同版本的数据，这时候需要合并冲突并得到最终最新的值，并将这个值更新到所有机器中。相当于一个保底机制：第一次写入没有一致的情况下，在第一次读取的时候会进行冲突合并，并重新写入，保证数据不会在多次读取后还是处于不一致的状态。\n\n## 负载均衡\nDynamo的负载均衡取决于如何给每台机器分配虚拟结点号，即token。由于集群环境的异构性，每台物理机器包含多个虚拟结点。一般有以下两种分配方式：\n<img src=\"/img/202103/021.png\" width=\"50%\" height=\"50%\">\n\n> 随机分配导致token分布零散，体现在Merkle树中就是同一台机器的不同token位于不同非叶子结点下（或者说，一台机器拥有的token在Merkle树中的路径十分宽大）使得新节点加入/离开系统时需要对Merkle树中的大量节点进行修改。且由于是随机分布没有规律，也无法将数据进行归档/备份。\n数据范围等分+随机分配：先将数据等分，然后按顺序将token分配给不同的结点，这样就使得当节点新增/下线时对Merkle树的影响最小，且由于的顺序分配，使得归档/备份变得容易许多。\n\n由于Dynamo中同步操作、写操作重试等后台任务比较多。为了不影响正常读写服务，需要对后台任务能够使用的资源做出限制。\n\nDynamo维护一个资源授权系统。该系统将整个机器的资源切分成多个片，监控60秒内的磁盘读写响应时间，事务超时时间及锁冲突情况，根据监控信息算出机器负载从而动态调整分配给后台任务的资源片个数。\n\n## 读写流程\n<img src=\"/img/202103/022.png\" width=\"50%\" height=\"50%\">\n> 这里W就是表示“大部分”，即不需要等所有副本都返回写入成功，只要大部分副本都写入成功就认为这次写入是成功的。就是多数派写。\n\n由于没有中心节点，因此客户端会从所有存储节点中选出一个作为协调者，这个协调者就起到了主存储节点的作用。客户端只需要把写入请求发给协调者即可，后续的所有节点的写入与同步问题都交给协调者处理。\n\n在读取时也是一样，根据一致性哈希算法计算出所有副本存储的节点，并选出其中一个作为协调者，通过协调者读取所有（实际上不需要所有，只需要根据负载均衡策略计算出的R个副本即可）存储副本以及自身的数据，汇总之后返回给客户端。这里可能会产生冲突，默认情况下是使用最新的数据，用户也可以自定义冲突解决策略。在将最终的数据返回给客户端后，协调者还会异步地将最终数据返回给所有副本用于更新最新结果，用于修复错误副本。\n<img src=\"/img/202103/023.png\" width=\"50%\" height=\"50%\">\n\n## 单机实现\nDynamo的存储节点包含三个组件：请求协调、成员和故障检测、存储引擎。\n\nDynamo设计支持可插拔的存储引擎，比如BerkerlyDB（BDB），Mysql InnoDB等。\n\n> 也就是说Dynamo类似于一个分布式存储的上层解决方案，而最终将数据持久化的任务交给其他存储引擎？\n\n请求协调组建采用基于事件驱动的设计，每个客户端的读写请求对应一个状态机，系统根据发生的事件及状态机中的状态决定下一步的操作。比如读取操作对应的状态包括：\n* 协调者发送读请求到其他节点\n* 等待其他节点返回读取结果，最少需要R-1个（加上协调者自己就是R个）\n* 如果请求其他节点返回失败，需要按照一定的策略重试\n* 如果到达时间限制成功的节点仍然小于R-1个，返回客户端请求超过\n\n合并协调者及其他R-1个节点的读取结果，并返回客户端，合并的结果可能包含多个冲突版本；如果设置了冲突解决方法 ，协调者还需要解决冲突。\n\n读操作返回客户端后状态机不会立刻被销毁，而是会等待一段时间，等待其他节点将过期的数据返回，并将最新的数据更新到这些节点。\n\n> 这样来看R只是一个阈值。实际请求的时候应该是请求所有的节点，然后当R-1个节点返回时候就判断读取成功执行后续的合并与返回操作，但是后续还有一些返回的节点只需要更新到最新的值即可。\n\n## 讨论\nDynamo采用无中心节点的P2P设计，增加了系统的可扩展性，但同时带来了一致性问题，影响上层应用。\n\n> 这里就差不多可以总结出中心节点的优劣了：好处是由于所有元数据都交给/都经过中心节点，可以最大程度上保证数据的一致性，就算暂时不一致，在后续存储节点与中心节点的通信中也可以被中心节点检测出来并及时修正；缺点是扩展性不如P2P结构，在P2P结构中，新加入节点只需要通知一下种子节点然后通过Gossip协议对接上其他存储节点即可，而中心节点架构需要联系上中心节点并通过修改元数据与负载均衡来让新节点加入。\n\nDynamo在Amazon的使用场景优先，主流的分布式系统一般都带有中心节点，这样能够简化设计且中心节点只需要维护少量的元数据，一般不会称为性能瓶颈。\n\nDynamo及其开源实现Cassandra在实践中受到的关注逐渐减小，但是它应用了各种分布式技术，在实践过程中也可以借鉴。\n\n# 淘宝Tair\nTair分为持久化和非持久化两种使用方式：非持久化可以看成是一个分布式缓存，持久化的Tair将数据存放于磁盘中。持久化的容错机制与现有的分布式存储备份机制一样。\n\n## 系统架构\n<img src=\"/img/202103/024.png\" width=\"50%\" height=\"50%\">\nTair作为一个分布式系统，是由一个中心控制节点和若干服务节点组成。其中，中心控制节点被称为Config Server，服务节点称为Data Server。Data Server以心跳的方式将自身状况汇报给Config Server。\n\n> 实际上心跳与租约两者并无冲突，心跳可以检测系统是否时常在线，租约负责下放权限。\n\n## 关键问题\n### 数据分布\n根据数据的主键计算哈希值后，分布到Q个桶中，桶是负载均衡和数据迁移的基本单位。Config Server按照一定的策略把每个桶指派到不同的Data Server上。因为数据按照主键计算哈希值，所以可以认为每个桶中的数据基本是平衡的，只要保证桶分布的均衡性，就能够保证数据分布的均衡性。根据Dynamo论文中的实验结论，Q取值需要远大于集群的物理机器数，例如Q取10240.\n\n> 桶分布的均衡性不是平均分配，而是考虑每台机器的硬件配置与运行情况进行的一种非均匀分配，使得每台机器的压力趋于平衡。那么这里的桶可以当作一组token的集合\n\n### 容错\n当某台Data Server故障不可用时，Config Server能够检测到。每个哈希桶在Tair中存储多个副本，如果是备副本，那么Config Server会重新为其指定一台DataServer，如果是持久化存储，还将复制数据到新的Data Server上。如果是主副本，则第一时间启用其他备副本用于提供服务，然后再选择另外一台Data Server称为备副本，确保数据的备份数。\n\n> 通过主备副本的方式来实现容错，并保证数据的一致性。但针对请求的吞吐量弱于Dynamo。\n\n### 数据迁移\n机器加入或者负载不均衡可能导致桶迁移，迁移的过程中需要保证对外服务。某个Data Server中存放三个副本A、B、C，如果A尚未迁移完成，B还在迁移当中，C已经迁移完成。当请求读写A的时候，依旧是原有Data Server提供服务，当请求读写B的时候，原有Data Server提供服务，并将修改操作记录到日志中，当迁移完成时将日志也同步过去使得新的节点能够重现迁移期间进行的修改操作，当读写C的时候直接讲请求转到新的Data Server上。\n> 这样看来，迁移的时候会复制一份新的数据，原有的数据继续对外提供服务，新复制的数据连同期间的修改日志一同迁移到新的机器并通过日志与原有数据进行同步。\n\n### Config Server\n客户端缓存路由表，大多数情况下，客户端不需要访问Config Server，Config Server宕机也不影响客户端正常访问。每次路由的变更，Config Server都会将新的配置信息推给Data Server。在客户端访问Data Server的时候，会发送客户端缓存的路由表版本号。如果Data Server发现客户端的版本号过旧，则会通知客户端去Config Server获取一份新的路由表。如果客户端访问某台Data Server发生了不可达的情况，客户端也会主动去Config Server获取新的路由表。\n> 当主Config Server宕机的情况下如何继续提供服务：通常来说中心节点都是一主一备的配置，如果主Config Server宕机，且备Config Server还没上上线的情况下，客户端通过读取自身缓存的路由表可以在一定程度上减少对Config Server的依赖，为Config Server切换主备提供时间，并在此时间内能够正常对外服务。\n\n> 当路由表发生改变时客户端才会往Config Server请求新的路由表。当Config Server检测到路由表发生改变的时候并不会直接通知客户端，而是将最新的版本号发送给所有Data Server，当客户端用旧的路由表请求DataServer的时候由DataServer通知客户端往Config Server请求新的路由表；或者当客户端请求Data Server失败的时候表示现有的路由表不是完全准确的了，因此也会主动请求新的路由表。这样的设计最大程度减少了Config Server的压力，使得Config Server不会成为性能的瓶颈。\n\n> 如果Data Server一个一个依次下线/上线，那么就会导致Config Server频繁更新路由表，这或许会使得Config Server达到性能瓶颈。\n\n### Data Server\nData Server负责数据的存储，并根据Config Server的要求完成数据的复制和迁移工作。Data Server具备抽象的存储引擎层，可以很方便地添加新的存储引擎。Data Server还有一个插件容器，可以动态加载/卸载插件\n<img src=\"/img/202103/025.png\" width=\"50%\" height=\"50%\">\nTair存储引擎有一个抽象层，只要满足存储引擎需要的接口，就可以很方便地替换Tair底层的存储引擎。Tair默认包含两个存储引擎：Mdb和Fdb，另外还支持Berkerly BD、Tokyo Cabinet、Inno DB、Level DB等各种存储引擎。\n\n## 讨论\nDynamo采用P2P架构，而在Tair中引入了中心节点Config Server，这种方式很容易处理数据的一致性问题，因为所有的数据都要经过中心节点方便管理。\n> P2P相关技术：向量时钟、数据回传、Merkle树、冲突处理\n\n由于Tair的复制是异步的，所以当有DataServer发生故障时，客户端有可能在一定时间内读不到最新的数据，甚至发生最新修改的数据丢失的情况。\n> 比如数据在迁移过程中主Data Server永久性异常了，那么节点在迁移期间内的所有修改操作全部丢失，因为此时数据还是保存在主Data Server的操作日志中，并没有备份。而同步过去的数据只是该节点在迁移前某个时间点的快照。\n\n","tags":["存储"]},{"title":"分布式存储笔记3-2 分布式文件系统（TFS&FH）","url":"/2021/03/09/分布式存储笔记3-2-分布式文件系统（TFS-FH）/","content":"<hr>\n这是一篇在阅读《大规模分布式存储系统：原理解析与架构实战》时的阅读笔记，由于长时间碎片阅读的关系导致在做这种读书笔记的时候接近复制粘贴。虽然其中会有一小部分自己的想法但都十分零碎，希望后续能改进。\n<hr>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TFS = Taobao File System\n\nFH = Facebook Haystack\n\nBlob文件系统的特点是数据写入后基本都是只读，很少出现更新操作。\n<!-- more -->\n# Taobao File System\nTFS架构设计时需要考虑如下两个问题：\n<img src=\"/img/202103/001.png\" width=\"50%\" height=\"50%\">\n因此TFS的设计思路是：多个逻辑图片共享一个物理文件\n\n## 系统架构\nTFS于GFS的不同点：\n* TFS内部不维护文件目录树，每个小文件使用一个64位的编号表示；\n* TFS是一个读多写少的应用，相比GFS，他的写流程可以做的更加简单高效\n<img src=\"/img/202103/002.png\" width=\"50%\" height=\"50%\">\n一个TFS集群由两个NameServer和多个DataServer节点组成，NameServer通过心跳对DataServer的状态进行检测。\n\n> 不是租约，说明不需要对DataServer下放写权限?\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 答：当要写入的DataServer宕机时，Client告知NameServer，由NameServer重新进行分配。因为TFS读多写少，不需要支持并发写，因此不会有同一个文件对多个DataServer同时写入的情况。\n\n当主NameServer宕机的时候，可以被心跳守护检测出来并将服务切换到备NameServer。\n\n每个DataServer上会运行多个dsp进程，一个dsp对应一个挂载点，这个挂载点一般对应一个独立磁盘，从而管理多块磁盘。\n\n在TFS中，将大量的小文件合并成一个大文件，这个大文件称为块（Block），每个Block拥有在集群内唯一的编号（块ID），通过<块ID，块偏移>这样的映射关系，可以唯一确定一个文件。\n\n与GFS相同，每个块的大小约为64MB，并默认保存三份。且在客户端也缓存NameServer的元数据信息。\n\n## 追加流程\nGFS为了减少对Master的压力，引入了租约机制，从而将修改权限下放到主ChunkServer。TFS是读多写少，且写多是追加写而不是修改。（浏览用户多，商户修改不频繁），因此每次写操作都需要经过NameServer，从而减少系统复杂度。\n\nTFS也不需要支持多客户端并发写，同一时刻每个Block只能有一个写操作，多个客户端的写操作会被串行化\n> 是因为进行写操作的多是商家/管理人员，而这些用户对响应的要求比浏览商品的客户要低很多\n\n<img src=\"/img/202103/003.png\" width=\"50%\" height=\"50%\">\n\n> 与GFS不同的地方在于：写入操作只有一种，即写入一个文件，而GFS有追加写和大文件写入这两种情况；NameServer参与度比GFS要高很多，在GFS中，在ChunkServer写入完成后chunk的变化不会立刻同步到master，而是在chunkserver定时发往master的信息中携带。但在TFS中写入完成后需要立刻通知NameServer元数据的修改。这应该是要考虑到商家上传好图片之类的信息后需要立马生效看到效果这样实际的业务需求来设计的。即读多写少的情况，这种情况下会有客户端还没有收到写入成功的消息但是其他用户就能够看到这张图片了，用最快的速度提高图片的生效时间。\n\n在写入完成后DataServer会返回客户端两个信息：小文件在TFS中的block编号以及在block中的偏移。引用系统在读取图片的时候能保证在所有block中他的偏移量都是有效的。\n\n> 这里的一致性要求就比GFS高了，GFS只是保证数据至少有一次写入，且就算是同一个chunk的不同备份，数据的偏移量都可能不一致，因为他们对每一个chunk块都维护了一个元数据结构。而TFS则为了保证应用层面的简单，强制要求任何一个block备份块都能使用同一个小文件的偏移量。即主备block之间所有字节都一致。\n\n## NameServer\n<img src=\"/img/202103/004.png\" width=\"50%\" height=\"50%\">\n\n> 之所以不需要维护文件与Block之间的映射关系是因为block与文件的关联关系已经返回给客户端另行保存了。\n\nDataServer掉线以及新加入的操作都跟GFS一样。\n\n## 讨论\n图片应用有几个问题：\n* 图片去重\n* 图片更新与删除\n<img src=\"/img/202103/005.png\" width=\"50%\" height=\"50%\">\n<img src=\"/img/202103/006.png\" width=\"50%\" height=\"50%\">\n\n> 保证每个block都能使用同一个物理偏移带来架构简化的同时也意味着无法随便对block做垃圾回收、合并等操作。\n\n# Facebook Haystack\nFacebook相册后端早期采用基于NAS的存储，通过NFS挂载NAS中的照片文件来提供服务。后台出于性能考虑，自主研发了Facebook Haystack。\n\n## 系统架构\n<img src=\"/img/202103/007.png\" width=\"50%\" height=\"50%\">\nFH的思路与TFS类似，也是多个逻辑文件共享一个物理文件。\n\nHaystack系统包括三个部分：目录、存储、缓存。\n\nHaystack存储是物理节点，以物理卷轴（physical volume）的形式组织存储空间，每个物理卷轴一般都很大（100GB），这样10TB的数据也只需要100个物理卷轴。\n\n每个物理卷轴对应一个物理文件，因此，每个存储节点上的物理文件元数据都很小。\n> 将基础的数据块变大了，相应的对块的管理就减少了\n\n多个物理存储节点上的物理卷轴组成一个逻辑卷轴，用于备份。\n\nHaystack目录存放逻辑卷轴和物理卷轴的对应关系，以及照片id到逻辑卷轴之间的映射关系。\n\nHaystack缓存主要用于解决对CDN提供商过于依赖的问题，提供最近增加的照片缓存服务。\n<img src=\"/img/202103/008.png\" width=\"50%\" height=\"50%\">\n\n## 写流程\n<img src=\"/img/202103/009.png\" width=\"50%\" height=\"50%\">\n\n> 这样的话存储端需要跟Haystack目录有一个通信过程，这样才能保证目录的准确性，且相关的元数据位于Hatstack目录。这里的Web服务器就承担了GFS与TFS中的客户端的功能：他先从Haystsck目录中寻找有效的逻辑目录以及对应的物理存储信息，然后将生成id的图片写入存储端。\n\nHaystack的一致性模型只保证写操作成功，逻辑卷轴对应的所有物理卷轴都存在一个有效的照片文件，但有效照片文件在不同物理卷轴中的偏移量可能不同。\n> 这个就类似GFS的一致性模型，即保证数据被写入，但不能保证写入的地方是一致的。这样就不能用TFS一样一个文件ID查询所有副本，需要在另外维护一个文件ID与数据在各个块中的偏移量信息的映射结构。\n\nHaystack只能追加不能更改照片，如果有修改，则新增一条照片ID并将旧的ID替换。如果新的照片和原有照片不在同一个逻辑卷轴，Haystack目录的元数据会更新为最新的逻辑卷轴；如果新增照片和原有的照片在相同的逻辑卷轴，Haystack存储会以偏移更大的照片文件为准。\n\n> 按照用户-逻辑卷轴-物理卷轴。这样的分层结构，对照片修改后如果在同一个逻辑卷轴内，则只需要修改用户-逻辑卷轴部分对于<图片id, 逻辑卷轴>这个映射关系即可。如果不在同一个逻辑卷轴内，则需要修改整体的元数据结构。 ？\n\n### 容错处理\nHaystack存储节点容错：\n<img src=\"/img/202103/010.png\" width=\"50%\" height=\"50%\">\n\nHaystack目录容错：\n<img src=\"/img/202103/011.png\" width=\"50%\" height=\"50%\">\n也就是说Haystack实际上是将元数据持久化到数据库中，将保证元数据写入磁盘的工作交给了数据库。\n\n## Haystack存储\nHaystack存储保存物理卷轴，每个物理卷轴对应文件系统中的一个物理文件，每个物理文件格式如下：\n<img src=\"/img/202103/012.png\" width=\"50%\" height=\"50%\">\n\n多个照片存放在一个物理卷轴中，每个照片文件是一个Needle，包含实际数据以及逻辑照片文件的元数据。部分元数据需要装载到内存中用于照片查找，包括Key（照片ID，8字节），Alternate Key（照片规格，4字节），照片在物理卷轴的偏移Offset（4字节），照片的大小（4字节），每张照片的信息需要占用20字节空间。假设每张照片大小为80KB。则一台可用磁盘为8TB的机器可以保存8TB/80KB=1亿张照片，占用内存1亿x20字节=2GB。\n> 这样的存储设计是基于机器的硬件而生，以硬件上的物理卷轴为基本块用于存储信息。好处是数据集中且由于写入都是在一块物理卷轴中写入，不会有磁盘指针跨磁道的开销，写入快速。\n\n存储节点宕机时，需要恢复内存中的逻辑照片查找表，扫描整个物理卷轴耗时太长，因此对每个物理卷轴维护了一个索引文件，保存每个Needle查找相关的元数据。\n> 这部分元数据既持久化到Needle中和图片数据放一起，又单独持久化为一个索引文件。说明元数据的维护可能并不需要Haystack目录保存到自己本地，只需要等待存储节点将自身的物理卷轴元数据加载到内存中后发送给Haystack目录即可。\n\n由于更新索引文件的操作是异步的，所以可能出现索引文件和物理卷轴文件不一致的情况，不过由于对物理卷轴文件和索引文件的操作都是追加操作，只需要扫描物理卷轴文件最后写入的几个Needle，然后补全索引文件即可，这只能在只有追加写入的系统中才能使用，也很常用。\n> 系统保证会将文件写入，并在写入完成后修改索引文件，但这一步就已经属于异步操作，所以只会出现索引文件缺失而不是索引文件中存在，但是物理卷轴中没有的情况。索引文件缺失的这部分通过对操作日志的重现可以恢复。\n\n<img src=\"/img/202103/013.png\" width=\"50%\" height=\"50%\">\n\n> 既然不需要跟TFS一样保证一个ID都能从所有副本中得到一样的数据，那么就一定会有一个文件ID与物理卷轴偏移量的映射数据结构，这样就可以对存储节点采用垃圾回收策略。而他的策略和其他系统一样都是删除已删除的和重复的数据\n\n## 讨论\nHaystack与TFS最大的不同就在与他可以进行垃圾回收。\n\nHaystack使用RAID6，并且底层文件系统使用性能更好的XFS，TFS不使用RAID机制，文件系统使用Ext3，由应用程序负责管理多个磁盘。\n\n> 看起来就像Facebook不差钱一样，无论是GFS还是TFS都是考虑系统建立在不稳定廉价的硬件基础上，因此他们对于存储节点出现故障的频率是考虑很高的，所以在对文件块的设计上，偏向于小巧轻便，这样当存储节点宕机无法恢复的时候可以快速地对其他副本进行复制备份，且不会对现有业务造成影响 。而Haystack的大块设计，既简化了元数据与磁盘管理开销，但同时增加了磁盘成本与维护成本，每当存储节点宕机切无法恢复时，数据迁移备份的时间会长很多。\n\n# 内容分发网络(CDN)\n<img src=\"/img/202103/014.png\" width=\"50%\" height=\"50%\">\n<img src=\"/img/202103/015.png\" width=\"50%\" height=\"50%\">\n\n## CDN架构\n<img src=\"/img/202103/016.png\" width=\"50%\" height=\"50%\">\n\n图片存储在后台的TFS集群中，CDN系统将这些图片缓存到离用户最近的边缘节点。\n\nCDN采用两级Cache：L1-Cache以及L2-Cache。\n\n用户访问淘宝网的图片时，通过全局调度系统调度到某个L1-Cache节点。如果L1-Cache节点命中，那么直接讲图片数据返回用户；否则，请求L2-Cache节点，并将返回的图片数据缓存到L1-Cache节点；否则，请求源服务器的图片服务器集群。\n\n每台图片服务器是一个运行着Nginx的Web服务器，他还会在本地缓存图片，只有当本地缓存也不命中时才会请求后端的TFS集群。\n\n对于每个CDN节点，其架构如图所示:\n<img src=\"/img/202103/017.png\" width=\"50%\" height=\"50%\">\n\n每个CDN节点内部通过LVS+Haproxy的方式进行负载均衡。\n\n* [LVS](https://blog.csdn.net/weixin_40470303/article/details/80541639)：该项目在Linux内核中实现了基于IP的数据请求负载均衡调度方案。\n\n* [HAProxy](http://www.ttlsa.com/linux/haproxy-study-tutorial/)：是一个提供高可用性、负载均衡，以及基于TCP和HTTP的应用程序代理。\n\n其中LVS是四层负载均衡软件，性能好；Haproxy是七层负载均衡软件，能够支持更加灵活的负载均衡策略。通过有机结合两者，可以将不同的图片请求调度到不同的Squid服务器。\n\nSquid服务器用来缓存Blob图片呢数据。用户的请求按照一定的策略发送给某台Squid服务器，如果缓存命中则直接返回；否则Squid服务器首先会请求源服务器获取图片缓存到本地，接着再将图片数据返回给用户。\n\n相比起分布式存储系统，分布式缓存系统的实现要容易得多，这是因为缓存系统不需要考虑数据持久化，如果缓存服务器出现故障，只需要简单地将其从集群中删除即可。\n\n## 分级存储\n\n分级存储是淘宝CDN架构的一个很大创新。\n\n由于缓存数据有较高的局部性，在Squid服务器上使用SSD+SAS+SATA混合存储，图片随着热点变化而迁移，最热门的部署到SSD，中等热门的部署到SAS，轻度热门的存储到SATA。\n> 结合数据的访问频繁程度与存储介质的访问速度分多级存储，能够加快数据访问速度。\n\n## 低功耗服务器定制\n淘宝CDN架构的另一个亮点是低功耗服务器定制。\n\nCDN缓存服务是IO密集型服务而不是CPU密集型，因此可以选用CPU功耗相对较低的服务器。\n\n## 讨论\nBlob存储系统读访问量大，更新和删除很少，特别适合通过CDN技术分发到离用户最近的节点。\n\nCDN也是一种缓存，需要考虑与源服务器之间的一致性。如果源服务器删除了Blob数据，需要能够比较实时地推送到CDN缓存节点，否则只能等待缓存节点中的对象被自然淘汰，但是对象的有效期往往很长，热门对象很难被淘汰。\n\n> 可以专门维护一个专门用于接收这种修改与删除操作的程序。但是如果边缘节点很多的情况，光修改一次图片就要使得TFS应用或者上层的应用通知每一个squid服务器，这些工作量会很耗时且浪费资源。或许可以采用GFS中的那种串行化设计，TFS只需要通知离他最近的几台squid服务器，然后让这几台squid服务器继续往外分发修改事件，缓存系统也不需要担心重复删除/修改等问题。\n\n随着硬件技术的发展，SSD价格的下降，新上线的CDN可以全部配制成SSD。","tags":["存储"]},{"title":"Paxos Made Simple笔记","url":"/2021/03/04/Paxos-Made-Simple笔记/","content":"原文：[Paxos Made Simple](https://www.microsoft.com/en-us/research/uploads/prod/2016/12/paxos-simple-Copy.pdf) 翻译：[点击查看](https://www.cnblogs.com/j-well/p/7056951.html)\n\n一致性算法需要在一组服务器节点或者进程中保证一个提案能够在最终结果上达成一致。这里的提案可以是对一次操作的认定，也可以衍生为对一个值的最终结果。\n<!-- more -->\npaxos算法中，由三个角色组成：Proposers(提议者)、Acceptors(接受者)、Learners(学习者)。在一个实现中，单个进程可以充当多个角色。\n\n通常在节点之间的通信采用异步模型。\n* 每个参与者（Proposer,Acceptor,Learner）都有可能因为不同的原因导致执行失败、进程停止。当一个提案被选定后，如果所有参与者都同时下线并重启，必须要保证提案结果能被恢复，即需要将结果持久化到各个参与者的磁盘中。\n* 参与者之间传递的信息可能会超时、丢失，但不会被修改也允许被修改\n\n从安全角度来说：\n* 只有从提案发起者发起的提案才能被选定是否通过\n* 在最终结果没有出来前，任何节点/进程不能假设某个提案已经通过或者某个提案会有很大概率通过\n\n一个分布式算法，有两个重要属性：Safety和Liveness\n* Safety：指那些需要保证永远都不会发生的事\n* Liveness：指哪些最终一定会发生的事情\n\n## 提案的选择：\n选定提案最简单的方式就是只有一个Acceptor存在，这样不会存在多个不同的提案情况，但是一旦唯一的Acceptor宕机，则整个系统不可用。\n\n对上一种问题唯一的解决办法就是增加多个Acceptor。此时Proposer向一个Acceptor集合发送提案，某个Acceptor可能会通过这个提案。当有足够多的Acceptor通过它时，我们就认为这个提案被选定了。\n\n这里一个重要问题就是如何判断“足够多”是多少个Acceptor？为了保证数量，这个集合需要包含所有Acceptor成员。\n\n因为集合中任意取两个“大多数”子集，他们一定会存在交集，即用于判断提案是否通过的Acceptor数量至少要超过集合的一半，这就是多数派读写一致性要求。\n\n再要求所有Acceptor只能通过一个提案，那么就可以保证至少有一个提案会被通过。\n\n但是上面并没有考虑实际使用过程中节点不可用的情况。而这种情况在实际问题中往往无可避免，属于Liveness。\n\n在假设没有Acceptor下线的情况下，我们可以设置如下需求：\n> P1：一个Acceptor必须通过它收到的第一个提案\n\n上面所有的假设都在只有一个Proposer的情况下。如果当有多个Proposer同时提出不同的提案，那么可能会发生所有提案都无法满足“足够多”的情况。\n\n* 由于网络延时等原因，提案送达的时间会有先后，可能会出现两个提案各自通过的Acceptor数量相等的情况。但这种情况可以用奇数个数量的Acceptor节点修复。\n* 但是考虑到实际情况中会出现的Accptor下线等问题，会有5个Acceptor节点，其中2个通过提案A，3个通过提案B。当他们要将结果返回给Proposer时，有一个通过提案B的节点下线了，此时Proposer收到的有2个节点通过提案A，两个节点通过提案B。\n\n上面的问题，暗示了一个Accpetor必须要能够通过不止一个提案。\n\n通过对每一个提案分配一个编号来记录一个Acceptor通过的那些提案，于是一个提案就包含一个提案编号以及他的value值。为了不产生混淆，需要保证不同的提案具有不同的编号。当一个具有value值的提案被多数Acceptor通过后，我们就认为该value被选定了。\n\nWe can allow multiple proposals to be chosen, but we must guarantee that all chosen proposals have the same value.  这句无法理解，允许通过多个提案，但是必须保证所有的提案都具有相同的值？\n> P2：如果具有value值v的提案被选定了，那么所有比它编号高的提案的value值也必须是v。\n\n也就是说，如果有两个存在先后顺序的提案同时发出，当最先发出的提案通过并确定好值后，后判断的提案上的值必须时上一步已经确定好的值。\n\n一个提案能否被选定，至少需要有一个Acceptor通过。\n> P2a：如果一个具有value值v的提案被选定了，那么被Acceptor通过的所有编号比他高的提案的value值也必须时v。\n\n即，Acceptor需要保存上一次通过的提案，这个通过的提案不是单个Acceptor通过的提案，而是经过一次paxos流程后最终选定的这个提案。那样的话在一个提案通过后Proposer需要将通过的提案重新传回给Acceptor并让其留档，当同样的提案再次被发送过来的时候可以根据Acceptor保存的上一个提案来对当前要通过的提案执行判断。\n\n由于通信是异步的，一个提案可能会在某个Acceptor c还没收到任何提案的时候就被选定了。此时有另一个Proposer提出了一个具有不同value值的更高编号的提案，根据P1，需要c通过这个提案，但是这样又与P2a矛盾，因为对整个集群来说提案已经通过了，但是对于c来说这个提案根本没有发给他。因此需要对P2进行强化：\n> P2b：如果具有value值v的提案被选定了，那么所有比他编号更高的被Proposer提出的提案value值也必须是v\n\n### 如何证明P2b成立：\n假设某个具有编号m和value值v的提案被选定了，需要证明任意具有编号n（n > m）的提案都具有value值v。我们可以通过对n使用数学归纳法来简化证明，在额外的假设下：即编号在[m, n-1]之间的提案具有value值v，来证明编号为n的提案具有value值v，其中[i, j]表示从i到j的集合。因为编号为m的提案已经被选定了，这就意味着存在一个多数Acceptor组成的集合C，C中的每个成员都通过了这个提案。结合归纳的假设，m被选定意味着C中的每个Acceptor都通过了一个编号在[m, n - 1]之间的提案，并且每个编号在[m, n-1]之间的被Acceptor通过的提案都具有value值v。\n\n由于任何包含多数Acceptor的集合S都至少包含一个c中的成员，我们可以通过保持如下不变性来确保编号为n的提案具有value值v：\n* P2c:对于任意v和n，如果一个编号为n，value值为v的提案被提出，那么肯定存在一个由多数Acceptor组成的集合S满足以下条件中的一个：\n   * S中不存在任何Acceptor通过了编号小于n的提案\n   * v是S中所有Acceptor已经通过的编号小于n的具有最大编号的提案的value值\n\n只要维护P2c的不变性就可以满足P2b了。\n\n当一个提案被通过后，所有通过改提案的Acceptor都要保存这个提案到本地，这样，在所有Acceptor集合中，必然存在一个表示“大多数”的集合，这个集合中一定存在有通过了之前提案的Acceptor。但是这个Acceptor不一定在这个“大多数”集合中占据大多数。比如100和Acceptor，第一次提案时有60台通过了提案，然后进行第二次提案，此时这个新的集合中包含60个Acceptor，但是其中只有10台属于上次通过了提案的Acceptor。此时其他50台的相同提案的编号肯定小于那10台，因此这里又产生一个约束条件：保证当提案通过后在新的提案到达时如果有两种不同编号的同一种提案，则采取编号最新的提案的value。\n\n这种情况下，当一个Proposer在提出一个编号为n的提案时，如果存在一个将要或者已经被多数Acceptor通过的编号小于n的最大编号提案，Proposer需要知道他的信息。在将提案发往Acceptor的时候需要由Acceptor判断出当前自身以通过或者正在执行的小于n的最大编号提案，如果存在则将这个信息发给Proposer。\n单单返回已经通过的提案很简单，但是那种正在执行判断的很难预测它是否会被通过（虽然大部分情况下都是accept的），为了避免陷入预测未来这种困境，Proposer通过提出承诺不会有那样的通过情况来控制它。换句话说，Proposer会请求哪些Acceptor不要再通过任何编号小于n的提案了。即如果某个小于n的提案正在执行中，此时Proposer发送编号为n的提案给Acceptor，Acceptor会保证这个小于n的提案必定不会被通过。\n这就导致了如下提案生成算法：\n\t1. Proposer选择一个新的提案编号n，然后向某个Acceptor集合中的成员发送请求，要求他做出如下回应：\n\t\ta. 保证不再通过任何编号小于n的提案\n\t\tb. 返回它当前已经通过的编号小于n的最大编号提案，如果存在的话\n\t我们把这样的请求称为编号n的prepare请求。\n\t2. 如果Proposer收到来自集合中多数成员的响应结果，那么它可以提出编号为n，value值为v的提案，\n    这里v时所有响应中最大编号提案的value值，如果响应中不包含任何提案，那么这个值就由Proposer自由决定。\n\nProposer通过向某个Acceptor集合发送需要被通过的提案请求来产生一个提案（这里的Acceptor集合不一定是响应前一个请求的集合）。这个过程叫做accept请求。\n\n## Acceptor\nAcceptor会收到两种请求：prepare、accept。Acceptor可以忽略任意请求而不用担心破环算法的安全性。\n它可以再任何时候响应prepare请求，也可以再不违反现有承诺的情况下响应accept请求。\n   * P1a:一个Accepter可以通过一个编号为n的提案，只要它还未响应任何编号大于n的prepare请求\n\n假设一个Acceptor收到了一个编号为n的prepare请求，但是它已经对编号大于n的prepare请求作出了相应，因此它肯定不会再通过任何新的编号为n的提案，于是我们会让Acceptor忽略这样的prepare请求，我们也会让他忽略那些他已经通过的提案的prepare请求。\n\n通过这个优化，Acceptor只需要记住它已经通过的提案的最大编号以及它已经响应过prepare请求的提案的最大编号。因为必须要在出错的情况下也保证P2c的不变性，所以Acceptor要在故障和重启的情况下也能记住这些信息。\n\nProposer可以随时丢弃提案以及它的所有信息，只要它可以保证不会提出具有相同编号的提案即可。\n\n把Proposer和Acceptor的行为结合起来，我们就能得到算法的两阶段执行过程：\n\n    Phase 1:\n        • Proposer选择一个提案编号n，然后向Acceptor的多数集发送编号为n的prepare请求。\n        • 如果一个Acceptor收到一个编号为n的prepare请示，且n大于它所有已经响应的请求的编号，那么他就会保证不会再通过任意编号小于n的提案，同时将它已经通过的最大编号提案（如果存在的话）一并作为响应。\n    Phase 2:\n        • 如果Proposer收到多数Acceptor对他的prepare请求（编号为n）的响应，那么它就会发送一个编号为n，value值为v的提案的acceptor请求给每个Acceptor，这里v是收到的响应中最大编号提案的值，如果响应中不包含任何提案，那么他就可以是任意值。\n        • 如果acceptor收到一个编号为n的提案的accept请求，只要它还未对编号大于n的prepare作出响应，他就可以通过这个提案\n\n**优化：如果一个Acceptor已经收到一个大于n的prepare请求，那么他应该通知给出编号n提案的Proposer，使得Proposer放弃编号为n的提案。**\n\n### 获取被选定的提案值：\n这里会跟Learner有关，Learner必须要能够知道一个提案已经被多数Acceptor通过了，最直观的算法是，让每个Acceptor再通过一个提案时就通知所有Leaner。但这需要让每个Acceptor与每个Leaner通信，通信次数是二者的乘积。\n\n更一般地，Acceptor可以将信息发送给一个特写的Learner集合，他们中的任何一个都可以在某个value被选定后通知所有Learner。这个集合中的Learner越多，可靠性越好，通信复杂度越高。\n\n如果只通知一次Learner的话消息可能会丢失，Learner可以向Acceptor询问他们通过了那些提案，但是任一Acceptor出错，都有可能导致无法分辨是否有多个Acceptor通过了某个提案。在这种情况下Learner可以由Proposer扮演，Proposer之间负责维持通信。\n\n当一个新的提案被选定时，Learner才能发现被选定的value。如果一个Learner想知道是否已经选定一个value，他可以让Proposer利用上面的算法提出一个提案。\n\n### 进展性：\n会有这种情况，两个Proposer轮流发起prepaer请求，但是永远没有一个Proposer进入plase2阶段。\n为了保证进度，必须选择一个特定的Proposer作为唯一的提案提出者。如果这个Proposer可以和多数Acceptor进行通信，并且可以使用比已用编号更大的编号进行提案的话，那么它提出的提案就可以成功被通过。如果知道有某些编号更高的请求，他可以通过舍弃当前的提案并重新开始，这个Proposer最终一定会选到一个足够大的提案编号。这个Proposer叫做Leader。\n文中是说通过选举得到一个Leader，但是具体如何选呢？\n\n即在所有Proposer中选择一个特殊的Proposer，他可以在accept阶段被拒绝后重新获取一个新的编号并发起提案，而其他的Proposer在accept阶段被拒绝后将无法重新获取新编号，除非最外逻辑重新发起一轮新的提案。\n\n## 实现：\nPaxos算法假设了一组进程网络。在他的一致性算法中，每个进程都扮演着Proposer，Acceptor，以及Learner的角色。\n该算法选择了一个Leader来扮演那个特定的Proposer和Learner。Paxos一致性算法就是上面描述的那样，请求和响应都以普通消息的方式发送（响应消息通过对应的提案编号来标识以免混淆）。使用可靠的存储设备存储Acceptor需要记住的信息来防止出错。\nAcceptor在真正发送响应之前，会将它记录到可靠的存储设备中。\n\n不同的Proposer从不相交的编号集合中选择自己的编号，这样任何两个Proposer就不会用到相同的编号。每个Proposer都记录它使用过的最大编号，然后用这个比这更大的编号的提案开始Phase 1\n\n> 本来后面还有一章状态机的，但是我看不太懂就没放。","tags":["分布式"]},{"title":"分布式存储笔记3-1 分布式文件系统（GFS）","url":"/2021/02/28/分布式存储笔记3-1-分布式文件系统（GFS）/","content":"<hr>\n这是一篇在阅读《大规模分布式存储系统：原理解析与架构实战》时的阅读笔记，由于长时间碎片阅读的关系导致在做这种读书笔记的时候接近复制粘贴。虽然其中会有一小部分自己的想法但都十分零碎，希望后续能改进。\n<hr>\n\n分布式文件系统的主要功能有两个：\n1. 存储文档、图像、视频之类的Blob类型数据\n2. 作为分布式表格系统的持久化层\n<!-- more -->\n<img src=\"/img/202102/063.png\" width=\"50%\" height=\"50%\">\n\n# Google文件系统（GFS）\n<img src=\"/img/202102/064.png\" width=\"50%\" height=\"50%\">\n\n## 系统架构\n<img src=\"/img/202102/065.png\" width=\"50%\" height=\"50%\">\n\nGFS可分为三种角色：GFS Master（主控服务器）、GFS ChunkServer（CS，数据块服务器）、GFS客户端。\n\nGFS的文件被划分为固定大小的文件块，并拥有一个全局唯一的64位chunk句柄（类似于uuid），chunk被以普通linux文件形式保存在服务器中，并在不同机器中复制多份，默认为三份。\n\n主控服务器中维护了系统的元数据（包括文件及chunk命名空间、文件到chunk之间的映射、chunk位置信息）。它也负责整个系统的全局控制（如chunk租约管理、垃圾回收无用chunk、chunk复制等）。主控服务器会定期与CS通过心跳的方式交换信息。\n> 既然是通过租约实现对chunk server的存活检测，那这个心跳应该都是chunk server发往master.\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PS:找了一下论文果然如此，chunk server定时往master发送自身chunk相关信息。这样这部分数据就不需要让master持久化到自身，而是可以通过与chunk server的通信从提交的信息中构建\n\n客户端是提供给应用程序的访问接口，他是一组专用接口，不遵循POSIX规范，以库文件的形式提供。客户端访问GFS时先从master获取对应文件的句柄与chunk的字节范围。然后client与chunk server直接通信。最大程度减少master的负担。\n\nGFS不缓存文件数据，只缓存元数据信息。这是由GFS应用的特点决定的：MapReduce与Bigtable。\n* MapReduce：GFS客户端使用方式为顺序读写，没有缓存文件数据的必要\n* Bigtable：Bigtable作为分布式表格系统，内部已经实现了一套缓存机制。\n\n如何维护客户端缓存与实际数据之间的一致性是一个及其复杂的问题。\n\n## 关键问题\n### 租约机制\nGFS数据追加以记录为单位，每个记录的大小为几十KB到几MB不等，如果每次记录追加都需要请求Master，那么Master显然会成为系统的性能瓶颈，因此，GFS系统中通过租约（lease）机制将chunk写操作授权给ChunkServer。拥有租约授权的ChunkServer称为主ChunkServer，其他副本所在的ChunkServer称为备ChunkServer。\n\n租约授权针对单个chunk，在租约有效期内，对该chunk的写操作都由ChunkServer负责，从而减轻Master的负载。\n\n> Q:如果主ChunkServer在租约有效期内下线该怎么办?\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1: 客户端如果多次请求ChunkServer失败，则重新往Master申请这个文件块的新的ChunkServer，如果Master确认在租约有效期内原先的主ChunkServer依旧无法上线，则在现有备份ChunkServer中要么直接选择要么通过选举的方式重新赋予一个备ChunkServer权限使其对Client服务。\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2: ~~客户端判断在租约有效期内依旧无法联系上ChunkServer，那么在此之前，Master返回文件命名空间以及chunk相关信息的时候顺便携带chunk的备份服务器信息，那么就让Client发起选举~~（如果由Client直接选择，那么如果有多台Client同时读写同一个数据块，那么将会出现复数个Proposer）【这个操作不行，只能有一个Proposer，否则这是没意义的】\n\n一般来说，由于涉及文件读写，所以租约有效期比较长为60秒所有。\n<img src=\"/img/202102/066.png\" width=\"50%\" height=\"50%\">\n> 直接删除并重新备份，减少了运行期间chunk数据重新保持一致的工作量，过于复杂会使得出问题的几率增加。且同步数据的工程量比直接复制要大。\n\n### 一致性模型\nGFS主要是为了追加写（append）而不是改写（overwrite）而设计的，一方面改写的需求比较少，或者可以通过追加来实现，比如可以只使用GFS的追加功能构建分布式表格系统Bigtable；另一方面是因为追加的一致性模型相比改写要更加简单有效。\n<img src=\"/img/202102/067.png\" width=\"50%\" height=\"50%\">\n\n追加写时，如果成功那万事大吉。如果有些chunkserver写入成功而有些写入失败，失败的副本可能会出现一些可识别的填充（padding）记录。GFS客户端追加失败将重试，只要返回用户追加成功，说明在所有副本中都至少追加成功了一次，也有可能一个chunk被追加了多次，即重复记录；也可能出现一些可识别的填充记录，需要在应用层处理这些问题。\n\n> GFS只负责写入数据成功，但不能保证写入数据的完整性以及物理层面的一致性。即它能确保chunk的所有副本都已经被写入数据，但不能确保所有的数据都一致（这里的一致是指所有的数据都是完整一条。有的chunkserver成功写入数据但是会有重复记录，有些chunkserver写入一些可识别的填充记录。）\n\n> 如果要保证强一致性，那么当某个chunkserver长时间无法响应的时候，client的整个写入都会被阻塞，此时应该添加一个超时机制，如果长时间没有响应，则client重新发起写入请求，这种情况下就容易产生重复数据。\n\n<img src=\"/img/202102/068.png\" width=\"50%\" height=\"50%\">\n\n> 但是每次写入都会返回这部分文件位于chunk的那一部分，需要读取的时候只需要根据这些文件偏移量信息读取即可，而且在chunkserver的写入过程中需要保证原子性，即每个请求的写入不能被打断，但是同一client的写入请求可以被打断。\n\n<img src=\"/img/202102/069.png\" width=\"50%\" height=\"50%\">\n\n### 追加流程\n<img src=\"/img/202102/070.png\" width=\"50%\" height=\"50%\">\n<img src=\"/img/202102/071.png\" width=\"50%\" height=\"50%\">\n\n> client在给主chunkserver发送写请求的时候，其他备chunkserver数据还缓存在内存中，如果同时有多个写操作将数据缓存在一台chunkserver服务器中或者client没有发送写请求给主chunkserver或者主chunkserver没有发送写请求给备chunkserver，就会出现内存溢出等问题。\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解决办法就是在chunkserver中维护一条LRU，如果某段数据长时间没有被写入则会排到队列末尾并被移除。\n\n> 客户端这里并没有在写入完成后将修改元数据的请求发往Master，这部分修改是在ChunkServer定时发往Master的信息中携带的。也就是说master的元数据变更会比chunk server实际文件变更延后一段时间。\n\n> 如果在发送写请求的时候备ChunkServer宕机了该怎么办？\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这时候主ChunkServer发起写入请求会失败，则通知Master将下线的ChunkServer中的数据进行垃圾回收，并将其中的数据重新备份。\n\n> 如果在发送写请求的时候是主ChunkServer宕机了该怎么办？\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Client应该通知Master在超过租约时限后重新在备ChunkServer中选出主ChunkServer并重新开始写入流程。\n\n<img src=\"/img/202102/072.png\" width=\"50%\" height=\"50%\">\n> 这样的要求ChunkServer自身需要维护其chunk的所有备份信息。\n<img src=\"/img/202102/073.png\" width=\"50%\" height=\"50%\">\n\n> 如果在追加过程中主ChunkServer租约过期而失去chunk修改操作的权限改怎么办？\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主ChunkServer应该要向Master续租，如果续租失败，为了保证流程规范，此次写入应该判定为失败，并让客户端重新向Master请求，由Master选举出新的主ChunkServer。\n\n> 这里说的是写入主ChunkServer，但是前面又说写入最近的ChunkServer并由最近的ChunkServer转发给其他ChunkServer。他们的区别是什么呢？\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;答：可能是数据量的不同，如果是追加流程且数据量较小的情况下，只需要由客户端发送给最近的ChunkServer并由他进行转发，或者发送给所有的ChunkServer，并将写入命令发给主ChunkServer即可。如果是大文件写入，则需要按照上述流程进行大文件转发。[相关链接](https://zhuanlan.zhihu.com/p/185619042)\n> 2020-03-15追加：就是数据量的不同，最开始的那种数据流与控制流分离的做法适用于数据量大的情况，这可以减少数据传输对整个系统的影响，但会使得写入流程变得更加复杂\n\n### 容错机制\n#### Master容错\nMaster容错与传统方法类似，通过操作日志加checkpoint的方式进行，并且有一台被称为“Shadow Master”的实时热备。\n<img src=\"/img/202102/074.png\" width=\"50%\" height=\"50%\">\n\nMaster需要持久化前两种元数据。对于第三种元数据，可以交给ChunkServer维护，当ChunkServer上线的时候将这些信息发送给Master。\n\nGFS Master的修改操作总是先记录操作日志，然后修改内存。当Master发生故障时，可以通过磁盘中的操作日志恢复内存数据结构。\n\n为了减少Master宕机恢复时间，Master会定期将内存中的数据以checkpoint文件的形式转储到磁盘中，从而减少回放的日志量。\n\nMaster的任何修改元数据的操作都必须在实时热备中完成后才能生效。Shadow Master通过日志的形式接收Master的操作步骤，并在自己的内存中回放这些元数据操作。\n\n为了保证同一时刻只有一台Master工作，是否为主Master需要通过选举得到。\n\n> 如果热备服务器宕机了导致Master始终无法写入成功该怎么办？\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果允许也可以部署两台热备服务器。另一种方法就是Master将日志同步记录保存下来，当热备服务器上线后将之前的操作重新发给热备服务器。\n\n#### ChunkServer容错\nGFS采取复制多个副本的方式实现ChunkServer的容错，每个chunk由多个存储副本，分别存储在不同的ChunkServer上。\n\n对于每个chunk，必须都要写入成功才算整个写入流程成功。如果相关副本丢失或者服务器下线的情况，Master会将其上的副本重新复制，并且当这台服务器重新上线的时候他的数据会被垃圾回收，因为此时它上面的数据都有可能与其他副本不一致。\n<img src=\"/img/202102/075.png\" width=\"50%\" height=\"50%\">\n> 这部分数据也将会被视为垃圾，并在合适的时候进行回收。\n\n> 这样来看chunk中会存储很多垃圾数据：重复写入的数据、可读取的填充数据、错误数据、已删除的数据。如果放任这些数据积累，可能会造成空间的浪费，GFS是否有类似于Bitcask的垃圾回收机制或者LevelDB的SSTable合并机制用于清理垃圾？\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;答：有，GFS采用惰性回收的方式进行垃圾回收。在删除文件的时候会将文件的元数据做一个标记，master会在合适的时候扫描所有chunk的namespace并重新整理chunk。这里就应该会将已删除的数据、重复写入的数据、可读取的填充数据、错误数据清除。\n\n> 看文章说的都是已删除数据，其他垃圾数据不知道会不会清楚，不过我想应该会的吧，毕竟成本放那里，能省一点是一点。\n\n### Master设计\n#### Master内存占用\n内存是Master的稀有资源。chunnk的原信息包括全局唯一的ID、版本号、每个副本所在的chunkserver等。\n<img src=\"/img/202102/076.png\" width=\"50%\" height=\"50%\">\n\n#### 负载均衡\n<img src=\"/img/202102/077.png\" width=\"50%\" height=\"50%\">\n系统中需要创建chunk副本的情况有三种：chunk创建、chunk复制、负载均衡。\n<img src=\"/img/202102/078.png\" width=\"50%\" height=\"50%\">\n> 第二点就是为了后续的文件写入预留出空间，否则都用来存储chunk而没有空间写入，就使得添加新的服务器对分布式存储系统的性能毫无提升。\n\n当chunk的数量低于一定的数量以后，Master会尝试重新复制一个chunk副本。可能的原因包括：ChunkServer宕机、ChunkServer报告Chunk副本损坏，ChunkServer磁盘故障等。\n\n每个chunk复制任务都有优先级，按照优先级从高到低在Master队列排队等待执行。\n\n> 这个优先级应该是按照chunk的热度与数量来划分的，如果某个chunk的数据访问量高，那么为了保证后续的写操作，则优先将其备份。或者某一个chunk只剩一个副本，为了保证数据不丢失，则也要优先将其备份。而热度与数量之间我认为以数量为优先。因为chunk整块丢失是无法自动化恢复的，且无法保证完全恢复，而热度方面还有其他备用块分担压力。\n\n当client要写入到一个只有一个副本的chunk块的时候，GFS将会阻塞其操作，并规定必须要有两个副本才可以，此时chunkserver会将这个情况报告给master用于提高这个chunk块的复制优先级。\n\n> 就跟我上面猜想的一样，一个副本的时候已经很危险了，为了防止操作对现有chunk块造成不可逆的影响，需要优先保证副本的数量。\n\nMaster会定期扫描当前副本的分布情况，如果发现磁盘使用量或者机器负载不均衡，将会执行重新负载均衡操作。\n\n> 这个操作应该就是将磁盘压力大的chunk复制到磁盘使用量少的服务器上，并将原来的chunk删除。\n\n<img src=\"/img/202102/079.png\" width=\"50%\" height=\"50%\">\n\n#### 垃圾回收\n<img src=\"/img/202102/080.png\" width=\"50%\" height=\"50%\">\n\n对于那些下线后的没有执行删除的chunkserver，master会维护一个递增的版本号，当检测到版本号过期时一样会将chunk的空间内释放。\n\n#### 快照（Snapshot）\n快照操作是对源文件 / 目录进行一个“快照操作”，生成该时刻源文件根目录的一个瞬间状态存放于目标文件根目录中。\n\nGFS采用标准的写时复制技术，“快照”只是增加GFS中chunk引用计数，表示这个chunk被快照文件引用 了，等到客户端修改这个chunk时，才需要在chunkserver中拷贝chunk的数据生成新的chunnk，后续的修改操作落到新生成的chunk上。\n\n> 也就是说这里的快照不仅仅是对master元数据的快照，还是对当前目录下所有文件做的快照，当快照完成后后续的读写工作都将会在新的chunk中执行。类似于做了一个完整的目录下的所有文件备份工作。\n<img src=\"/img/202102/081.png\" width=\"50%\" height=\"50%\">\n<img src=\"/img/202102/082.png\" width=\"50%\" height=\"50%\">\n<img src=\"/img/202102/083.png\" width=\"50%\" height=\"50%\">\n\n> 快照相当于给所有chunk做一个标记（体现在引用计数大于1），并当gfs需要对这个chunk进行操作的时候再复制该chunk进行备份，这里可能会有一个情况就是后续只写入C3，那么foo的chunk将会有C1、C2、C3、C3’这四种。此时如果C1丢失，那么快照也随之失效了。\n\n### ChunkServer设计\n<img src=\"/img/202102/084.png\" width=\"50%\" height=\"50%\">\n\n> 新建的之后直接在旧的chunk上覆盖写即可。这样也就解决了上面提到的如何回收除了删除文件之外的那种重复数据、填充数据、错误数据等办法。不需要将其做具体区分，只需要按chunk进行整块删除，并在后续新数据写入的时候直接覆盖即可。\n\n<img src=\"/img/202102/085.png\" width=\"50%\" height=\"50%\">\n\n## 总结\n谷歌用GFS证明了单总控节点在面对现代数据处理的情况下是可行的，既保证了原子性又容易实现。这里的原子性属于“保证至少有一次写入/记录至少原子性追加一次”，即一次写入数据可能会有重复写入的问题，但这个问题交给应用层面解决，浪费的空间在当chunk块删除的时候会被直接覆盖。\n\n通过租约的方式将对chunk的修改授权下放到ChunkServer从而减少了Master的负载，通过流水线的方式复制多个副本以减少延时，追加流程复杂繁琐。需要设计高效简介的元数据模型，并且要支持快照操作。支持写时复制的B数能够满足Master日常管理元数据所需，这里的实现相当复杂。\n","tags":["存储"]},{"title":"分布式存储笔记3 总揽","url":"/2021/02/24/分布式存储笔记3-总揽/","content":"<hr>\n这是一篇在阅读《大规模分布式存储系统：原理解析与架构实战》时的阅读笔记，由于长时间碎片阅读的关系导致在做这种读书笔记的时候接近复制粘贴。虽然其中会有一小部分自己的想法但都十分零碎，希望后续能改进。\n<hr>\n\n架构设计之初需要估算系统的性能从而权衡不同的设计方法\n> 我感觉最难的就是估算性能与权衡不同的设计方法了。估算性能需要对系统的业务与公司的用户群体有一个清晰的认识，权衡不同的设计方法则需要大量的技术积累，毕竟量多了才能选\n<!-- more -->\n接下来首先会有分布式系统相关的基础概念和性能估算方法。接着才是分布式系统的基础理论知识，包括数据分布、复制、一致性、容错等。最后就是常见的分布式协议。\n\n## 主要协议：##\n* paxos协议：paxos选举协议用于多个节点之间达成一致，往往用于实现总控结点选举\n* 两阶段提交协议：用于保证跨多个结点操作的原子性\n\n## 基本概念：\n### 异常：\n\n在分布式存储系统中，往往将一台服务器或者服务器上运行的一个进程称为一个节点。\n\n大规模分布式存储系统的一个核心问题在于自动容错，由于网络与硬件往往是不可靠的，因此会出现各种异常情况。\n\n#### 异常类型：\n1. 服务器宕机\n可能原因：内存错误、服务器停电\n服务器宕机可能随时发生，此时节点无法正常工作，称为不可用（unavailable），服务器重启后节点失去所有内存信息。需要读取持久化介质中你数据来恢复内存信息，从而快速恢复到宕机前的某个一致状态。进行退出同理。\n> 所以需要在节点重启/启动的时候快速构建内存信息，通常可以通过持久化内存信息作为索引文件来实现\n\n2. 网络异常\n可能原因：消息丢失、消息乱序（如果采用UDP通信）、网络包数据错误、数据分区（通常是不同网络服务商之间无法正常通信的现象）\n设计时就要假定网络永远不可靠，只有收到接收方确认消息后才能认定这条消息发送成功。\n\n3. 磁盘故障\n可能原因：磁盘损坏、磁盘数据报错\n磁盘损坏会导致数据丢失，因此需要将数据备份到多台服务器上，如果出现损坏则快速地从其他服务器恢复数据。\n磁盘数据错误可以采用校验和（checksum）机制来解决，这个机制既可以在OS层面上实现，也可以在应用程序层面实现\n\n#### 超时：\n在分布式系统调用中，通过RPC对远端服务器函数进行调用会有三种状态：成功、失败、超时。与本地调用的两种状态相比（成功、失败）多了一个超时，这是因为网络延时/故障等原因引起。这三个状态也称为分布式存储系统的三态。\n<img src=\"/img/202102/025.png\" width=\"50%\" height=\"50%\">\n\n> 之所以将“超时”独立与失败，是因为调用方无法确定是在发起请求的时候网络异常还是在调用成功后响应时网络异常。\n\n当超时的时候客户端不能简单地认为服务器端处理失败。通常这种情况可以将服务端暴露出来的接口设计为冥等，这样当出现失败/超时的时候客户端可以一直用相同的参数发起请求，直到成功。\n\n### 一致性：\n由于异常的存在，分布式存储系统设计时往往会将数据冗余存储多份，每一份称为一个副本，当某一个节点出现故障时可以从其他副本读取数据。\t\n\n副本是分布式存储系统容错技术的唯一手段。\n\n而由于多个副本的存在，如何保证多个副本间数据的一致性就是整个分布式系统理论的核心。\n\n两个方面理解一致性：\n* 从客户的角度：客户的读写时对他们来说数据是否一致\n* 从服务端的角度：整个存储系统中所有副本是否一致，更新顺序是否相同\n\n> 两个不同的角度对于一致性的要求也不一样。如果从客户端角度出发，只要求客户端在读写数据的时候保证一致性，那么在存储系统中这个数据的某些副本可能并不处于一致状态，只不过在客户端读的过程中会将这些副本给排除掉；从服务端的角度出发则要求整个存储系统的副本达成高度的统一。\n\n定义场景\n<img src=\"/img/202102/026.png\" width=\"50%\" height=\"50%\">\n从客户端的角度出发一致性包含三种情况：\n<img src=\"/img/202102/027.png\" width=\"50%\" height=\"50%\">\n<img src=\"/img/202102/028.png\" width=\"50%\" height=\"50%\">\n<img src=\"/img/202102/029.png\" width=\"50%\" height=\"50%\">\n> 最终一致性，需要等待一段时间达成一致性。通常来说分布式存储系统采用的都是强一致性和最终一致性。而具体要用哪一个需要看业务的具体需要，如果是读取频率高的场景则要保证强一致性，如果类似于分布式文件系统的场景则可以采用最终一致性\n\n最终一致性具有其他变体：\n<img src=\"/img/202102/030.png\" width=\"50%\" height=\"50%\">\n> 读写一致性通常需要等待数据在存储系统中同步完成；会话一致性可能是为了提高吞吐量从而将一部分写入数据保存在内存中，类似于LevelDB的方式，这样当一个失效的时候可能会丢失内存中的数据，导致新创建的会话无法读取这部分数据；\n\n### 衡量指标：\n#### 性能：\n常见的性能指标有：系统的吞吐能力、系统的响应时间\n\n这两个指标往往是矛盾的，追求高吞吐的系统呢往往很难做到低延迟；追求低延迟的系统，吞吐量也会受限制。\n> 前文讲过，分布式存储系统是在保证低延迟的基础上提高吞吐量\n\n##### 可用性：\n系统可用性指系统在面对各种异常时可以提供正常服务的能力。可用性可以用系统停服务的时间与正常服务的时间的比例来衡量。\n* 某个系统的可用性为4个9（99.99%），他的允许停服务时间最大不能超过365 x 24 x 60 / 10000 = 52.56分钟。\n\n##### 一致性：\n前文有过说明，一般来说，越是强的一致性模型，用户使用起来越简单。\n\n大部分存储系统都倾向强一致性\n> 因为如最终一致模型，对用户来说可能无法区分是同步失败还是在等待同步当中\n\n##### 可扩展性：\n指分布式存储系统通过扩展集群服务器规模来提高系统存储容量、计算量和性能的能力。\n\n良好的分布式存储系统的扩展性体现在系统性能与服务器数量呈线性关系。\n\n#### 性能分析：\n需要在系统设计之初就估算存储系统的性能。\n\n系统设计之初通过性能分析来确定设计目标，防止出现重大设计失误。等系统运行后通过性能优化方法找到系统的瓶颈点并消除。\n\n性能分析的结果是不精准的，但是不会相差一个大的数量级。\n\n设计之初需要分析整体架构，然后重点分析可能成为瓶颈的单机模块。系统的资源（CPU、内存、磁盘、网络）是有限的，性能分析就是需要找出可能出现的资源瓶颈。\n\n##### 分析实例：\n\n**Q:生成一张有30张缩略图（假设图片原始大小为256KB）的页面需要多少时间？** \n<img src=\"/img/202102/031.png\" width=\"50%\" height=\"50%\">\n<img src=\"/img/202102/032.png\" width=\"50%\" height=\"50%\">\n\n**Q:1GB的4字节证书，执行一次快速排序需要多少时间？**\n<img src=\"/img/202102/033.png\" width=\"50%\" height=\"50%\">\n\n**BigTable系统性能分析**\n<img src=\"/img/202102/034.png\" width=\"50%\" height=\"50%\">\n<img src=\"/img/202102/035.png\" width=\"50%\" height=\"50%\">\n\n> 还是要在实践中积累经验\n\n### 数据分布：\n数据分布的方式有两种：哈希分布、顺序分布\n\n数据分散到多台机器后，需要尽量保证多台机器之间的负载时比较均衡的。衡量机器负载均衡的因素很多：机器Load值、CPU、内存、磁盘、网络等资源的使用情况；读写请求数及请求量等。\n\n分布式存储系统需要能自动识别负载高的节点，当某台机器的负载较高时，将它服务的部分数据迁移到其他机器，实现自动负载均衡。\n\n> Q:为什么不将流量转移到其他的备份机器中呢？\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A:GFS的备份最多不会超过10台，如果流量大到10台机器都满了那就没地方迁移了，而直接将热点数据转移到其他空闲的服务器可以保证扩容的灵活性，在流量继续增加的情况下还可以添加新的服务器并将热点数据转移到这台服务器上。\n\n分布式存储系统的一个基本要求就是透明性，包括数据分布透明性、数据迁移透明性、数据复制透明性、故障处理透明性。\n\n#### 哈希分布：\n通常根据数据主键（hash(key) % N）或者数据所属的用户id（hash(user_id) % N）计算哈希值来决定将数据映射到哪台服务器。\n\n哈希分布的难点在于如何找到一个好的散列特性。\n* 如果按数据主键分布，会使得一个用户下的数据分布到多台服务器，当要操作同一个用户下的多条数据的时候要同时修改多台服务器的数据十分困难，此时大量的网络开销就是性能瓶颈。\n* 如果按照用户id分布，容易出现呢“数据倾斜”的问题，即某些大用户的数据量很大，无论集群的规模多大，这些用户始终由一台服务器处理。\n\n处理大用户问题一般有两种方式：手动拆分、自动拆分\n* 自动拆分：根据数据分布算法实现动态调整，自动就爱哪个大用户的数据拆分到多台服务器上。\n* 手动拆分：线下标记系统中的大用户（例如运行一次MapReduce作业），并根据这些大用户的数据量将其拆分到多台服务器。\n\n传统的哈希还有一个问题，当系统中的服务器上线下线的时候N值发生变化，数据映射被完全打乱，几乎所有数据都需要重新排布，这将带来大量数据迁移。\n\n**解决办法：**\n<img src=\"/img/202102/036.png\" width=\"50%\" height=\"50%\">\n> 但这样又需要一台新的服务器用于保存哈希值与服务器的对应关系，同时必须要保证这个模块高可用，这将提高整个系统的复杂性\n\n<img src=\"/img/202102/037.png\" width=\"50%\" height=\"50%\">\n> 这只是缓解了数据重新排布的情况，不能完全避免，依旧会有数据要重新排布。\n\n<img src=\"/img/202102/038.png\" width=\"50%\" height=\"50%\">\n加入node-5后只影响了node-3的数据分布。但node-3需要迁移的数据如果过多，整个集群的负载也会不均衡。一种方法是将需要迁移的数据分散到各个集群当中，没台服务器只需要迁移1/N的数据量，为此引入了虚拟节点的概念。\n\n#### 顺序分布：\n哈希散列破坏了数据的有效性，只支持随机读，不支持顺序扫描。\n> 不过如果按照之前说的添加一台哈希-元数据服务器的话可以通过这台服务器进行扫描。\n\n一种方法是按用户id进行哈希分布，这样就保证一个用户的数据位于同一节点下，可以进行顺序扫描。但是也会带来数据倾斜的问题，无法发挥分布式存储系统的多机并行处理能力。\n\n因此，顺序分布常用于分布式表格系统。将大表按顺序划分为连续的范围，每个范围成为一个子表，总控服务器负责将这些子表按照一定的策略分配到存储节点上。\n<img src=\"/img/202102/039.png\" width=\"50%\" height=\"50%\">\nRoot用来维护数据的分布情况。\n\n随着子表数据插入与删除，有些子表会变得很大，某些变得很小，数据分布不均匀。如果采用顺序分布，在设计时就需要考虑子表的分裂与合并，这将会增大系统的复杂度。\n\n* 子表分裂：当一个子表太大超过一定阈值的时候，需要分裂为两个子表，从而有机会通过系统的负载均衡机制分散到多个存储节点。\n* 子表合并：一般由数据删除引起。当相邻的两个子表都很小时，可以合并为一个子表。\n\n一般来说，单个服务节点能够服务的子表数量是有限的，比如4000～10000个，子表合并的目的是为了防止系统中出现过多太小的子表，减少系统中的元数据\n> 减少Root的维护成本\n\n#### 负载均衡：\n分布式存储系统中的每个集群一般有一个总控节点，其他节点为工作节点你，由总控节点根据全部负载信息进行整体调度。\n\n工作节点刚上线时，总控节点需要将数据迁移到该节点\n> 采用数据迁移而不是直接追加写的原因可能是为了防止大量的写入请求冲击这台服务器。\n\n系统运行过程中也要随时不断的进行数据迁移，将数据从负载较高的工作节点迁移到负载较低的工作节点。\n\n工作节点通过心跳包将节点负载相关信息（CPU、内存、磁盘、网络等资源利用率，读写次数以及读写数据量）发送给主控节点。主控节点计算出工作节点的负载以及需要迁移的数据量。生成迁移任务放入迁移队列中等待执行。\n\n负载均衡需要掌握节奏，如果新加入一台机器，主控节点立刻将大量的数据同时迁移到这台新的机器，整个系统在新增机器的过程中服务能力就会大幅降低。一般来说，从新增机器加入到集群负载达到比较均衡的状态需要较长一段时间，比如30分钟到一个小时，\n\n> 可以将负载量大的工作节点中的数据优先迁移，即根据负载量进行排序，最高的几台工作节点优先迁移数据。\n\n工作节点的数据往往会有多个副本，对外提供服务的称为主副本。当发生数据迁移的时候，可以将服务切换到其他副本然后进行迁移，这种无缝操作对用户来说完全透明。\n<img src=\"/img/202102/040.png\" width=\"50%\" height=\"50%\">\n<img src=\"/img/202102/041.png\" width=\"50%\" height=\"50%\">\n\n#### 复制：\n数据备份了多份，但通常只会有一份用于提供服务，当无法提供服务时，主控系统会自动切换到备份，这个操作称为自动容错。\n\n数据的副本有两种：主副本（Primary）、备副本（Backup）\n\n复制协议分为两种：强同步复制、异步复制\n* 强同步复制：可以保证主备副本之间的一致性，但是当备副本出现故障时，也可能阻塞存储系统的正常写服务。影响系统整体可用性。\n* 异步复制：可用性相对较好，但是一致性得不到保障，主副本出现故障时还有数据丢失的可能。\n\n两者的区别在于用的鞋请求是否需要同步到备副本才可以返回成功。加入备副本不止一个，复制协议还会要求写请求至少需要同步到几个备副本。\n> 不管是强同步还是异步，都要求主副本写入完成后才可以返回，即保证了主副本数据完整。但是这里难道说异步复制连主副本是否需要写入完成都不需要判断了吗？\n\n##### 复制的概述：\n复制常见的做法是将数据写给主副本，由主副本确定操作的顺序并复制到其他副本。\n<img src=\"/img/202102/042.png\" width=\"50%\" height=\"50%\">\n<img src=\"/img/202102/043.png\" width=\"50%\" height=\"50%\">\n> 备副本阻塞无法返回的情况下可以参考多数派写方法，当超过一半的备副本返回写入成功响应后就视本次写入成功。（超过一半 => 至少1个）\n<img src=\"/img/202102/044.png\" width=\"50%\" height=\"50%\">\n> 本地写入完成并且还没备份到其他服务器的时候，如果发生不可恢复的故障会导致本次写入数据完全丢失，而对于客户端来说本次写入是已经完成了的。\n\n强同步复制和异步复制都是将主副本的数据以某种形式发送到其他副本，这种复制协议称为基于主副本的复制协议（Primary-based protocol）。这种方法要求在任何时刻只能由一个副本作为主副本，由它来确定写操作之间的顺序，如果主副本出现故障，需要选举一个备副本称为新的主副本，这步操作称为选举，经典的选举协议为paxos协议。\n\n主备副本之间的复制一般通过操作日志实现。\n\n为了利用好磁盘的顺序读写特性，将客户端的写操作先顺序写入到磁盘中，然后应用到内存中，由此内存是随机读写设备，可以很容易通过各种数据结构有效组织起来。当服务器宕机时，只需要回放操作日志就可以恢复内存状态。为了提高并发能力，系统往往会用成组提交技术写入数据。\n\n> 如果是写入大文件根据HDFS的EditsLog来看也是直接记录数据内容的，然后通过定期合并/删除等方式减少操作日志的大小。\n<img src=\"/img/202102/045.png\" width=\"50%\" height=\"50%\">\n> 跟前面第一次遇到检查点概念的时候我的理解一样，检查点持久化后只需要恢复这个检查点之后的数据即可。\n\n除了给予主副本的复制协议，分布式存储系统中还可能使用给予写多个存储节点的复制协议（Replicated-write protocol）。比如Dynamo系统中的NWR复制协议，其中N为副本数量，W为写操作的副本数，R为度操作的副本数。\n\nNWR协议中多个副本不再区分主副，客户端根据一定的策略往其中的W个副本写入数据，读取其中的R个副本。只要W+ R > N，可以保证读到的副本中至少有一个包含了最新的更新。\n\n但是，这种协议的问题在于不同副本的操作顺序可能不一致，从多个副本读取时可能出现冲突。（不建议使用）\n\n> R或者W设置的越大也会导致整个响应越长，因为最终的响应时间是根据最慢的响应来的。在读取到数据的时候还需要判断哪些数据是最新的，这时就需要向量时钟来配合。\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;只要W + R > N，就能保证读的节点一定会和写的节点产生交集。这样才能保证系统的强一致性。\n\n### 一致性与可用性：\nCAP理论：一致性（Consistency）、可用性（Availability）、分区可容忍性（Tolerance of network Partition）三者不能同时满足。\n<img src=\"/img/202102/046.png\" width=\"50%\" height=\"50%\">\n其中，分区可容忍性是一定要满足的，因此只能从一致性与可用性中权衡利弊。\n<img src=\"/img/202102/047.png\" width=\"50%\" height=\"50%\">\n具体如何采用需要根据系统的业务来权衡。\n\n### 容错：\n容错是分布式存储系统设计的重要目标，只有实现了自动化容错，才能减少人工运维成本呢，实现呢分布式存储的规模效应。\n\n分布式存储系统需要能够检测到机器故障，故障检测往往通过租约（Lease）协议实现。然后需要能够将服务复制或者迁移到集群哪种呢的其他正常的存储节点。\n\n最常出现的故障就是单机故障（通过备份解决），然后是机架故障（备份到其他机架）等\n\n#### 故障检测：\n心跳是一种很自然很常见的故障检测方法。\n\n但是当主从服务器之间的网络出现延时，或者副节点过于繁忙导致长时间无法及时响应心跳等原因，会使得副节点没有宕机，但主节点就开始对他所保存数据开始复制，这会出现多台服务器（原先的副节点与新备份的节点）同时服务同一份数据从而导致数据不一致。\n\n这里的重心是接收心跳方在何种情况下应该被认为已发生故障并停止服务。\n\n> 如果在主控节点那边做判断，一旦副节点无法及时响应心跳并开始转移它数据的服务，那么在逻辑层面将这台服务器下线，后续如果继续上线则将其认定为一台全新的服务器。\n\n由于机器之间会进行时钟同步，且相差不大（比如不会超过0.5秒），那么，我们可以通过租约（Lease）机制进行故障检测。\n\n#### 租约机制：\n租约机制是一种带有超时时间的授权。\n\n假设机器A需要检测机器B是否发生故障，机器A可以给机器B发放租约，机器B持有的租约在有效期内才允许提供服务，否则主动停止服务。机器B的租约快要到期时向机器A主动申请租约。\n\n正常情况下，机器B通过不断申请租约来延长有效期，当机器B出现故障或者与机器A之间的网络发生故障时，机器B的租约将国旗，从而机器A能够确保机器B不再提供服务，机器B的服务也会被迁移到其他服务器。\n\n> 在什么情况下机器A才会认定机器B的服务出现故障呢？如果机器B出现故障，那么机器A是无法检测到的，只能等服务请求机器B的数据的时候才会出现异常，那这种情况应该就是服务请求B的数据的时候如果出现异常则向机器A报告，并由机器A将B的服务迁移到其他服务器中。若是迁移完成后机器B又恢复了服务且他的租约尚未过期，那A不过不在逻辑层面将B下线的话不一样还是会有多台机器对同一份数据提供服务吗？\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上述问题答案：租约加上提前量，假设租约10秒过期，则A需要等到11秒后才能认定B无法提供服务，这样就能保证将B的服务迁移后就能保证B如果在后面恢复之后不会继续提供服务。\n\n#### 故障恢复：\n常见的分布式存储系统分为两种结构：单层结构和双层结构。\n* 单层结构：在系统中对每个数据分片维护多个副本；\n* 双层结构：类似于BigTable系统，将存储和服务分为两层，存储层对曾哥数据分片维护多个副本，服务层只有一个副本提供服务。（服务就是Bigtable，存储依托于GFS）\n<img src=\"/img/202102/048.png\" width=\"50%\" height=\"50%\">\n\n单层结构：当节点1出现故障时，会启用节点2对外提供服务。此时节点1会有两种情况：临时故障、永久故障\n* 临时故障：当节点1重新上线后，会将节点2的数据同步到节点1，此时节点1作为备份节点。\n* 永久故障：当一段时间后节点1还是无法上线则将其认定为永久故障，此时需要对节点1的数据进行复制。\n\n双层结构：数据块不会在服务节点中做备份，当一个服务节点发生故障的时候，系统会从分布式文件系统中重新获取这个节点中的数据块，并将其放置在其他正常的节点并提供服务（这里可以将数据直接加载到内存中）。\n\n节点故障会影响系统服务，在故障检测以及故障恢复的过程中不能提供些服务及强一致性读服务。\n> 为什么？提供写服务，并在后续节点上线后将数据同步过去不可以吗？\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;后续节点上线后其中的数据已经不一致，但是该节点可以参与后续的服务工作，这样会使得客户端读取到过期数据。而且实现也复杂。\n\n故障检测时间分为两种：故障检测时间、故障恢复时间\n\n单层结构的备副本和主副本之间保持实时同步，切换为主副本的时间很短。\n\n两层结构故障恢复往往实现成本只需要将数据的索引重新加载到内存中即可。\n\t\n总控节点也会出现故障，因此也需要总控节点的副节点以及通过实时同步保证数据的一致性。\n\n当故障发生时，通过外部服务器选举某台副本作为新的总控节点，而这个外部服务也得是高可用的，为了进行选主或者维护系统中的重要信息，可以维护一套通过Paxos协议实现的分布式锁服务，比如Chubby或者Zookeeper。\n\n### 可扩展性：\n主流的分布式存储系统大多都带有主控节点，理论上P2P架构更有优势，但实际上主控节点依旧能够满足大部分业务所需。\n\n传统数据库可以通过分库分表进行水平扩展。\n\n分布式存储系统的可扩展性不能简单地通过系统是否为P2P架构或者是否能够将数据分布到多个存储节点来衡量，而应该综合考虑节点故障后的恢复时间，扩容的自动化程度，扩容的灵活性等。\n\n### 总控节点：\n总控节点用于恢复数据分布信息，执行工作机管理，数据定位，故障检测和恢复，负载均衡等全局调度工作。\n\n总控节点使得系统的设计更加简单，并且更加容易做到强一致性，对用户友好。\n\n* 分布式文件系统的总控节点除了执行全局调度，还需要维护文件系统目录树，此时内存容量可能会成为瓶颈。\n* 总控节点只需要维护数据分片的位置信息，这时内存一般不会成为瓶颈。\n<img src=\"/img/202102/049.png\" width=\"50%\" height=\"50%\">\n如果总控节点成为瓶颈，可以在总控机与工作机之间增加一层元数据节点，每个元数据节点只维护一部分而不是整个分布式文件系统的元数据。而总控节点只需要维护少量元数据节点数据即可。\n\n### 数据库扩容：\n数据库可扩展性实现的手段包括：通过主从复制提高系统的读取能力，通过垂直拆分和水平拆分将数据分布到多个存储节点，通过主从复制将系统扩展到多个数据中心。\n<img src=\"/img/202102/050.png\" width=\"50%\" height=\"50%\">\n<img src=\"/img/202102/051.png\" width=\"50%\" height=\"50%\">\n<img src=\"/img/202102/052.png\" width=\"50%\" height=\"50%\">\n\n### 异构系统：\n#### 同构系统：\n<img src=\"/img/202102/053.png\" width=\"50%\" height=\"50%\">\n> 这就有点像GFS的设计，读写部分由客户端直接与存储节点交互\n\n将存储节点分为若干组，每个组内的节点服务完全相同的数据，其中一个节点为主节点，其他节点为备节点。同一组存储节点服务相同的数据，这样的存储系统成为同构系统。\n\n缺点：增加副本需要迁移的数据量太大（如果要增加副本，则要将主节点中的数据全部拷贝到副本节点，假设主节点数据量为1T，节点之间带宽为20MB/s,则完全备份需要1TB/20MB/s=50000s，耗时长且容易出错）\n* 大规模分布式存储系统要求具有线性可扩展性，即随时加入或者删除一个或者多个存储节点，系统的处理能力与存储节点的个数成线性关系。\n\n#### 异构系统：\n异构系统与同构系统的区别在于对数据的划分，同构系统将数据全部放置在同一个节点中，这样会使得数据的备份十分耗时。而异构系统将数据划分为多个数据块，并将这些数据块分布到整个集群中的不同节点当中，当一个存储节点出现问题的时候，将由整个集群来恢复数据服务而不是固定的几台服务器，这样能提高数据的可用性，且备份的时候传输数据块的效率比一个完整的数据要高很多。\n<img src=\"/img/202102/054.png\" width=\"50%\" height=\"50%\">\n当节点1宕机的时候其中的数据服务将由节点2、节点3来支撑，并由其他节点5、节点4来重新备份，这样可以将备份的网络开销与提供服务的网络开销隔离开使得效率更高，且恢复时间短，当集群规模越大的时候，优势越明显。\n\n### 分布式协议：\n\n分布式协议有很多，例如：租约、复制协议、一致性协议。其中两阶段提交协议与Paxos协议最有代表性。\n\n两阶段提交协议用于保证跨多个节点操作的原子性。\n\nPaxos协议用于确保多个节点对某个投票（例如哪个节点为主节点）达成一致。\n\n#### 两阶段提交协议（2PC）：\n常用来实现分布式事务。\n\n在2PC中，系统一般包含两类节点：一类为协调者（coordinator），通常一个系统中只有一个；另一类为事务参与者（participants，cohorts或workers），一般包含多个。\n\n协议中假设每个节点都会记录操作日志并持久化到非易失性存储介质，及时节点发生故障日志也不会丢失。两个阶段如下：\n<img src=\"/img/202102/055.png\" width=\"50%\" height=\"50%\">\n> 如果当一个参与者长时间没有回复，就会导致整个协议过程阻塞。或者协调者通知完参与者后下线了且数据没有做保存，那么也会导致该流程失败。不过可以引入事务超时机制来避免此问题。\n<img src=\"/img/202102/056.png\" width=\"50%\" height=\"50%\">\n\n上面的两个故障总结如下：\n* 事务参与者发生故障。给每个事务设置一个超时时间，如果某个事务参与者一直不响应，到达超时时间后整个事务失败。\n* 协调者发生故障。协调者需要将事务相信信息记录到操作日志并同步到备用协调者，加入协调者发生故障，备用协调者可以接替他完成后续的工作。如果没有备用协调者，协调者又发生了永久性故障，事务参与者将无法完成事务而一直等待下去。\n\t两阶段提交协议是阻塞协议，执行过程中需要锁住其他更新，且不能容错，通常大部分分布式存储系统都会放弃分布式事务的支持。\n\n#### Paxos协议：\nPaxos协议用于解决多个节点之间的一致性问题。\n\n多个节点之间通过操作日志同步数据，如果只有一个节点为主节点，那么很容易确保多个节点之间操作日志的一致性。当主节点出现故障的时候，系统需要选举出新的主节点。\n\n只要保证多个节点之间操作日志的一致性，就能够在这些节点上构建高可用的全局服务，例如分布式锁服务，全局命名和配置服务等。\n> 分布式系统一致性的核心就是保证多个节点之间的操作日志一致性。\n\n当主节点出现故障，备节点会提议自己成为主节点。这里的问题在于：网络分区的时候，可能会有多个备节点提议（Proposer，提议者）自己成为主节点。\n\nPaxos协议执行步骤如下：\n<img src=\"/img/202102/057.png\" width=\"50%\" height=\"50%\">\n如果Proposer第一次发器的accept请求没有被accepter中的多数派批准，即与其他的proposer冲突。则完整执行一轮Paxos协议：\n<img src=\"/img/202102/058.png\" width=\"50%\" height=\"50%\">\n<img src=\"/img/202102/059.png\" width=\"50%\" height=\"50%\">\n> 跟之前那篇paxos文章不同，上一篇通过Paxos保证了多个节点之间数据的一致性。而这里的Paxos协议则是通过在Proposer保证了多个节点之间记录的最后哪个成功修改结果并成为主节点信息一致。\n\n#### Paxos与2PC：\nPaxos协议有两种用法：\n1. 用它来实现全局的锁服务或者命名和配置服务，例如：Chubby、Zookeeper\n2. 用它来将用户数据复制到多个数据中心，例如：Megastore、Spanner\n\n2PC协议最大的缺陷在于无法处理协调者宕机问题，如果协调者宕机，那么2PC协议中的每个参与者都可能不知道事务应该提交还是回滚，整个协议备阻塞。\n\n常见的做法是将Paxos与2PC组合起来，通过2PC保证多个数据分片上的操作的原子性，通过Paxos协议实现同一个数据分片的多个副本之间的一致性。\n\n通过Paxos协议解决2PC协议中协调者宕机问题，当协调者宕机时，通过Paxos协议选举出新的协调者继续提供服务。\n> 这里也可以衍生为当主控节点与其备份节点全部宕机的时候该怎么办？答：通过选举从存储节点中选举出新的主控节点。\n\n### 跨机房部署：\n\n在分布式系统中跨机房是个大问题，机房之间网络延时比较大，且不稳定。\n\n问题主要包含两个方面：数据同步以及服务切换。\n\n跨机房部署方案有三个：集群整体切换、单个集群跨机房、Paxos选主副本\n\n#### 集群整体切换：\n<img src=\"/img/202102/060.png\" width=\"50%\" height=\"50%\">\n\n两个机房的主控节点与数据节点完全相同。并且其中机房1是主机房，机房2是备机房。机房之间的数据同步方式可能为强同步或者异步。如果采用异步模式，那么备机房的数据总是落后于主机房。\n\n当主机房整体出现故障时，有两种选择：\n1. 将服务切换到备机房，忍受数据丢失的风险\n2. 停止服务，直到主机房恢复服务\n\n如果数据同步是异步，那么主备机房切换往往是手工的，允许用户根据业务的特点选择“丢失数据”或者“停止服务”。\n\n如果采取强同步的方式，那么除了手动切换外还可以采取自动切换的方式，通过分布式锁服务检测主机房的服务，当主机房出现故障时，自动将备机房切换为主机房。\n\n#### 单个集群跨机房：\n<img src=\"/img/202102/061.png\" width=\"50%\" height=\"50%\">\n\n跟集群整体切换不同，单个集群跨机房将数据备份分布在两个机房中，总控节点也是跨机房分布，当主总控节点运行中时，他需要跟两个机房的存储节点保持通信。当总控节点出现故障时，由分布式锁服务切换总控节点。\n\n这种方式需要在数据备份的时候尽可能地将副本分布在多个不同的机房。\n\n> 总控节点与其他机房存储节点的通信不包含服务部分，即总控节点只需要维持其他机房存储节点的元数据即可。这样与主服务之间关系不大，可以专门开辟一个线程用于维持链接，不会阻塞服务。\n\n#### Paxos选主副本：\n前两种方法中，总控节点与工作节点之间需要保持租约以维持通信。\n<img src=\"/img/202102/062.png\" width=\"50%\" height=\"50%\">\n\n如果采用Paxos协议选主副本，那么每个数据分片的多个副本构成一个Paxos复制组。\n\n当B1出现故障时，其他副本将尝试切换为主副本，他的优点在于对总控节点的依赖低，缺点则是工程复杂度太高，很难线下模拟所有的异常情况。谷歌的Gegastore和Spanner都采用这种方式。\n> 这里如果出现整个机房故障的话应该和整体切换一样。那么这种方式和上述两种可以结合。将提供服务的基础单位由存储节点细化为数据块。","tags":["存储"]},{"title":"分布式存储笔记2-2 数据库相关","url":"/2021/02/22/分布式存储笔记2-2-数据库相关/","content":"<hr>\n这是一篇在阅读《大规模分布式存储系统：原理解析与架构实战》时的阅读笔记，由于长时间碎片阅读的关系导致在做这种读书笔记的时候接近复制粘贴。虽然其中会有一小部分自己的想法但都十分零碎，希望后续能改进。\n<hr>\n\nNoSQL=Not Only SQL\n<!-- more -->\n关系数据库在海量数据场景面临以下挑战：\n* 事务：关系模型要求多个SQL操作满足ACID特性，但是在分布式系统中，如果要满足该特性，需要用到两段提交协议，这个协议性能很低，且不能容忍服务器故障。\n* 连表：传统数据库设计需要满足范式需求，第三范式规定两张关联的表中除了主键外不允许出现其他冗余字段，但是随着表数据增加，连表的开销也就随之增大。为了避免这个问题往往采用数据冗余的方法。\n* 性能：关系数据库采用B/B+树存储引擎，更新操作的性能不如LSM树这样的存储引擎（在更新了磁盘上数据的同时也要根据新的数据更新索引树,在大量数据的情况下对索引的更新开销会很大），对基于主键的增删改查操作性能不如定制的K-V存储系统\n\nNoSQL系统面临的问题：\n* 缺少统一标准：关系数据库有SQL语言这样的业界标准，并拥有完整的生态链，而NoSQL系统使用方法不同，切换成本高，很难通用。\n* 使用以及运维复杂：NoSQL的使用需要理解系统的实现，关系数据库有完整的运维工具与大量经验丰富的运维人员。\n\n着重理解关系数据库的原理与NoSQL的高可扩展性。\n\n##### 事务与并发控制：\n事务拥有ACID属性，最理想的状态就是每个事务互不干扰，按顺序执行，这被称为可串行化。但可串行化效率低下，商业数据库通常有多种不同的隔离级别。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事务的并发控制通过锁机制来实现，锁会有不同的粒度：行、数据块、表\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;互联网应用中读事务比例远高于写事务，因此使用写时复制或者多版本并发控制技术来避免写事务阻塞读事务。\n\n## 事务\n事务是数据库操作的基本单位，因为他们具有ACID（原子性、一致性、隔离性、持久化）特性。\n* 原子性：使得事务一定全部完成或者一定全部失败，不允许存在中间状态被感知到。一个事务对同一数据项的多次读取结果一定是相同的（如果存在中间状态被感知到，则在读取的时候会读取到中间状态，导致多次读取的结果不一致）\n* 一致性：保证数据符合设定规则，有2个方面来保证。一方面通过数据库内部规则确保数据类型正确，数据的值在给定范围内等；另一方面通过应用程序保证数据的值符合当前场景需求。\n* 隔离性：事务的执行不是一步就完成的，因此要确保事务在执行过程中对外不可见。在并发情况下，一个事务在修改途中插入一个查询事务，这个查询事务是感知不到修改事务的中间状态，对他来说数据形式是原始数据，而不是执行过程中修改了一部分的更新事务中的数据。\n* 持久性：事务完成/失败后，对数据库的影响是永久性的。（成功的数据修改与失败的错误日志记录）\n<img src=\"/img/202102/009.png\" width=\"50%\" height=\"50%\">\n<img src=\"/img/202102/010.png\" width=\"50%\" height=\"50%\">\n四种隔离级别会产生不同的读写异常\n<img src=\"/img/202102/011.png\" width=\"50%\" height=\"50%\">\n\n## 并发控制\n### 数据库锁\n数据库的锁分为两类：读锁、写锁\n\n通常只允许对一个元素加一个写锁，可以对一个元素加多个读锁。\n\n写事务通常会阻塞读事务。\n\n多个事务并发执行可能会出现死锁，解决办法有两种：\n* 给每个事务设置超时时间\n* 设置死锁检测，死锁的原因在于事务之间资源的互相依赖，检测到死锁后可以通过回滚其中某个事务来消除死锁\n### 写时复制：\n在执行写操作时复制一份索引树，并在该索引树上操作。\n<img src=\"/img/202102/012.png\" width=\"50%\" height=\"50%\">\n在整棵索引树中只是复制需要修改的部分结点，不会复制整棵索引树。第三步完成的时候旧的索引树中与复制出来的索引树相关的结点指针都指向新的被修改后的部分索引树。\n\n**操作顺序：**\n1. 从索引树A中获取到写操作锁涉及的结点B\n2. 复制B得到C\n3. 对C进行修改操作\n4. 提交后，原子性地将A中原本指向B的指针指向C\n\n为了避免内存浪费，需要每个结点都维护一个引用计数器，当计数器为0的时候，该结点被垃圾回收。但是：**写时复制操作成本高，且多个写操作的结点如果相交，则是互斥的。**\n<img src=\"/img/202102/013.png\" width=\"50%\" height=\"50%\">\n\n### 多版本并发控制（MVCC）：\n是除了写时复制之外另一个实现读事务不加锁的方案。\n\n为每一行数据维护多个版本，无论事务的执行时间多长，都为事务提供事务一开始一致的数据，即当事务开始的时候受影响的这部分数据就独立为一个全新的版本，其他任何事务的执行都无法干扰到这条分支。\n\nInnoDB维护了两个隐藏的列：行被修改的时间、行被删除的时间。这里的时间不是绝对的时间，而是与时间相对应的数据库系统版本号。\n\n每次执行事务的时候都会得到一个递增的版本号，那么事务在执行的时候只需要对比数据原始版本号与自身得到的最新的版本号进行比较然后根据不同的隔离级别来判断是否返回。\n<img src=\"/img/202102/014.png\" width=\"50%\" height=\"50%\">\n<img src=\"/img/202102/015.png\" width=\"50%\" height=\"50%\">\n但是这样就需要多余的存储开销，且需要定期清除过早的版本号。\n\n### 故障恢复：\n当事务执行到一半的时候系统故障，此时系统重启后需要将事务恢复到最初状体或继续执行下去。（要么commit要么rollback）\n\n数据库系统与其他分布式存储系统一般采用操作日志（有时也被称为提交日志，commit log）技术来实现故障恢复。\n\n**操作日志分类：**\n* 回滚日志（UNDO Log）：记录事务修改前的状态\n* 重做日志（REDO Log）：记录事务修改后的状态\n* 回滚/重做日志（UNDO/REDO Log）\n\n#### 操作日志：\n为了保证数据库的一致性而采用操作日志，将对数据库的操作持久化到磁盘。如果每执行一次操作就写入磁盘，那么写入的数据是随机写入，每条数据之间都是随机指针；而如果将操作先存入操作日志中，当数量达到一定大小之后/定期统一写入磁盘，那么这次写入磁盘的数据都将是顺序写入。\n\n> 在操作日志中的写入都是追加写，包括删除操作\n\n<img src=\"/img/202102/016.png\" width=\"50%\" height=\"50%\">\n\n事务开始时，先在回滚日志中记录数据的原始状态。\n\n在事务结束后，在重做日志中记录数据的完成状态，并且在回滚/重做日志中记录这个事务操作。\n\n在事务提交后，执行的是REDO日志中的记录，如果是回滚，则执行UNDO日志中的操作，即事务执行完成后写入REDO中，此时尚未提交，也就尚未持久化进磁盘。\n\n针对存储模型可以做的简化：\n<img src=\"/img/202102/017.png\" width=\"50%\" height=\"50%\">\n\n通过查看mysql的操作日志，可以看到，用户做的所有操作都会被原样记录。\n<img src=\"/img/202102/018.png\" width=\"50%\" height=\"50%\">\n\n#### 重做日志：\n流程如下：\n<img src=\"/img/202102/019.png\" width=\"50%\" height=\"50%\">\n若要进行修复操作，只需要从头到尾读取一遍REDO日志并将其应用到内存中即可。\n\n如果先应用到内存再持久化入磁盘，那么当在内存中修改完成后其他程序就可以立刻读取到相关修改，而此时磁盘还未记录该操作，如果此时系统重启，那么数据恢复后的结果是修改前的数据，但是其他程序已经读取并使用了修改后的数据，这样就造成了数据的不一致性。\n\n#### 优化手段：\n##### 成组提交\n如果每次执行事务后都立刻写入磁盘会导致系统吞吐量下降，因此系统往往有一个“是否立刻刷入磁盘”的选项，对于一致性要求高的系统可以将其打开。\n\n对于上述情况可以将事务对REDO的记录缓存在内存中，当满足一定条件后统一写入日志中，并在内存中体现数据的更改。这样牺牲了事务的写延时但是能够提高系统的吞吐量。但是这种做法会导致当系统宕机时缓存中还没有写入的事务操作丢失。\n\n> 说是先保存在内存中，实际上为了防止数据丢失，在每一次写入的时候都要记录到操作日志当中，操作日志与数据块不同。同时为了减少数据丢失的危害，在写入条件部分会特别严格。\n\n当达成以下任何一项条件后写入磁盘：\n<img src=\"/img/202102/020.png\" width=\"50%\" height=\"50%\">\n\n> 可以看到写入的条件是十分严格的，精确到毫秒级，可以最大程度地方式数据丢失。\n\n##### 检查点\n如果所有的数据都保存在内存中会有两个问题：\n<img src=\"/img/202102/021.png\" width=\"50%\" height=\"50%\">\n类似于索引文件，将内存中的数据结构保存为一个索引文件，系统重启后直接从索引文件构建索引即可，不需要扫描所有数据。\n\n检查点流程如下：\n<img src=\"/img/202102/022.png\" width=\"50%\" height=\"50%\">\n\n这里应该是如下流程：\n<img src=\"/img/202102/023.png\" width=\"50%\" height=\"50%\">\ncheckpoint保存的是当前内存中数据的状态，可能会有一些修改操作，但由于REDO日志存储的是修改后的结果，就算再执行这些修改操作，数据依旧是以REDO中的为准，不会出现多次执行后结果变化的问题。\n\n加法操作、追加操作不具有冥等性，因此在checkpoint的时候不应该保存这些操作的状态，为此有两种方式：\n<img src=\"/img/202102/024.png\" width=\"50%\" height=\"50%\">\n要么停止服务（进行the world）要么在状态完美的瞬间创建一个快照，持久化的就是这个快照。","tags":["存储"]},{"title":"分布式存储笔记2.1 单机存储系统","url":"/2021/02/21/分布式存储笔记2-1-单机存储系统/","content":"<hr>\n这是一篇在阅读《大规模分布式存储系统：原理解析与架构实战》时的阅读笔记，由于长时间碎片阅读的关系导致在做这种读书笔记的时候接近复制粘贴。虽然其中会有一小部分自己的想法但都十分零碎，希望后续能改进。\n<hr>\n\n## 单机存储引擎与单机存储系统：\n单机存储引擎就是哈希表、B树等数据结构在机械磁盘、SSD等持久化介质上的实现。单机存储系统就是单机存储引擎的一种封装，对外提供文件、键值、表格或关系模型。\n<!-- more -->\n数据库将一个或多个操作组成一组，称作事务，事务必须满足原子性、一致性、隔离性以及持久性，简称ACID特性。\n\n### 网络拓扑：\n传统网络拓扑分为三层结构：接入层、汇聚层、核心层\n<img src=\"/img/202102/002.png\" width=\"50%\" height=\"50%\">\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这样的问题在于，同一接入层之间的服务器之间的带宽为1GB，不同接入层之间的服务器之间的带宽小于1GB，而同一接入层往往放在同一个机架中，因此设计系统的时候需要考虑到服务器是否在一个机架内，减少跨机架拷贝大量数据。\n\n> 不同核心层之间的服务器进行通信需要经过接入层、汇聚层、核心层，而核心层、汇聚层服务器在同时处理多台接入层的设备，使得网络带宽资源被占用。\n\nhadoop的三个存储副本中，有两个都处于同一个机架内。\n\n> 即两个副本付出同一接入层下，保证了两个副本之间带宽为1Gb\n\n新型的一种网络拓扑结构是谷歌推出的扁平化拓扑结构，即三级CLOS网络，同一个集群内最多支持20480台服务器，且任何两台之间都有1Gb带宽。但是这需要投入更多的交换机，不过好处就是设计系统时不需要考虑底层网络拓扑，从而很方便地将整个集群做成一个计算资源池。\n### 常用硬件参数：\n<img src=\"/img/202102/003.png\" width=\"50%\" height=\"50%\">\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;存储系统的性能瓶颈主要在于磁盘的随机读写，设计存储引擎的时候需要根据磁盘的特性做出相应的处理，比如将随机写操作转化为顺序写，通过缓存你减少磁盘随机读操作。\n\n### 存储介质对比：\n<img src=\"/img/202102/004.png\" width=\"50%\" height=\"50%\">\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于SSD价格高，但是在随机读写方面性能好的特定，可以将SSD作为缓存使用。\n\n### 存储层次架构：\n不同机架之间的服务器互相访问需要有额外的网络传输协议与网络协议栈的开销。\n> 传统网络拓扑结构如果要保证跨层间服务器网络带宽最大，需要为上层服务器开通更大的带宽\n\n存储系统的性能主要包括两个维度：\n* 吞吐量（系统的吞吐能力指系统在某一段时间可以处理的请求总数，常用美妙处理的读操作数QPS或者写操作数TPS来衡量）\n* 访问延时（指从某个请求发出到接收到返回结果消耗的时间，通常用平均延时或者99.9%以上请求的最大延时来衡量）\n\n设计系统时要求能够在保证访问延时的基础上，通过最低的成本尽可能实现高的吞吐量。优先保证访问延时，又有余力的情况下提高吞吐量\n> 优先保证访问延时，又有余力的情况下提高吞吐量\n\n磁盘与SSD的访问延时差别很大，但是带宽差别不大，因此磁盘适合大块顺序访问的存储系统，SSD适合 随机访问较多或者对延时比较敏感的关键系统。（SSD就算读取出了大块数据，最终还是要等网络带宽将其传输出去。\n\n二者也可以组合成混合存储：热数据（频繁访问）存储到SSD中，冷数据（访问不频繁）存储到磁盘中。\n> 就算IO性能再强，也要等待网络将数据发送出去，对外来说服务器理论最大的吞吐量就是服务器的满速带宽。\n\n### 单机存储引擎：\n存储系统的核心是存储引擎，它直接决定了存储系统的性能与功能。\n\n存储系统的基本功能：增删改读\n\n读操作又分为：随机读取、顺序扫描\n\n#### 哈希存储引擎 - 键值存储系统：\n是哈希表的持久化实现，支持增删改查，其中的查询操作属于随机读取，不支持顺序扫描。\n\nBitcask是基于哈希表结构的键值存储系统，用于日志存储。\n\n因此它只支持追加操作。\n> 日志对时间顺序要求很高不允许随机写入，而且作为记录不允许修改\n\n由于日志会长时间连续不断地写入，因此日志文件需要在到达一定大小之后分块，否则将无法读取。\n> 单个文件过大无法打开\n\n为了保证日志写入的时间顺序，在任意时刻，只有一个文件是可写的，老的文件只读不写。这个文件被称为活跃数据文件，其他已经达到大小限制的文件称为老数据文件。\n##### 数据结构：\n<img src=\"/img/202102/005.png\" width=\"50%\" height=\"50%\">\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bitcask数据文件中的数据是一条一条的写入操作。每条数据项分别为主键（Key）、value内容（value），主键长度（key_sz）、value长度（value_sz）、时间戳（timestamp）以及crc校验值。\n\n数据删除不会物理删除数据，而是通过将value设定为一个特殊的值用于标识的逻辑删除。\n\n> 将删除的开销整合到垃圾回收当中\n\n内存中采用基于哈希表的索引数据结构，哈希表的作用是通过主键快速地定位到value的位置。（如果只需要获取数据，从索引中得到的信息可以直接从文件中获取。如果要获取到这条记录的详细信息，如：时间戳、crc等，就需要从value位置读取再解析。这样在索引部分冗余了value长度等信息，可以加快对内容本身的获取速度）\n\n系统中的记录删除或者更新后，原来的记录成为了垃圾数据。Bitcask定期会执行垃圾回收的合并操作。将所有老数据扫描一遍生成新的数据文件，由于旧数据不允许更新，因此对同一个Key的更新操作也是会新追加一条记录，因此在垃圾回收的时候，对于同一个Key的多条数据，以最新的一条数据为准，对已经删除的数据则直接抛弃。\n\n为了保证系统宕机时对索引的快速恢复，系统的索引也需要一个持久化的索引文件并定时更新索引。\nBitcask在进行合并的时候会产生一个索引文件，这个索引文件跟内存中的索引完全一样。\n\n#### B树存储引擎 - 关系数据库：\n是B树的持久化实现，支持单条记录的增删改查操作，还支持顺序扫描。也能够实现键值存储系统。\n<img src=\"/img/202102/006.png\" width=\"50%\" height=\"50%\">\n\nInnoDB中常用的是B+树。\n\n##### 缓冲区管理：\n缓冲区管理是在内存中分配的一个额外空间，与页面同等大小。磁盘块的内容可以传送到缓冲区中。常用的缓冲区替换策略有两种：\n1. LRU：\n\tLRU算法淘汰最长时间没有读或者写过的块。这种方法要求缓冲区管理器按照页面最后一次被访问的时间组成一个链表，每次淘汰链表的尾页。但是有一个问题：加入某一个查询做了一次全表扫描，将导致缓冲池中的大量页面被替换（全局查询的结果将整个链表全部替换），从而污染缓冲池。\n2. LIRS：\n\t现代数据库一般采用LIRS算法，即将缓冲池分为两级，数据首先进入第一级，如果数据在短时间内被访问两次或者以上，则称为热点数据进入第二级。每一级都是由LRU组成。\n\n#### LSM树存储引擎 - 分布式表格系统：\n支持增删改查（包括随机读取与顺序扫描）通过批量转储技术规避磁盘随机写入问题。\n\n将对数据的修改增量保持在内存中，达到指定大小限制后将这些修改操作批量写入磁盘，读取时需要合并磁盘中的历史数据和内存中最新的修改操作。\n\n该算法的优势在于有效地规避了磁盘随机写入问题，但读取时可能需要访问较多的磁盘文件。\n> 如果服务器突然宕机可能会丢失部分在内存中的数据\n\n##### LevelDB存储引擎：\n<img src=\"/img/202102/007.png\" width=\"50%\" height=\"50%\">\n\n该引擎主要包括：\n* 内存中：MemTable、不可变MemTable\n* 磁盘上：当前（Current）文件、清单（Manifest）文件、操作日志（Commit Log，又叫提交日志）、SSTable文件\n\n当应用写入一条记录时，LevelDB首先将修改操作写入到操作日志文件中，成功后再将修改操作应用到MemTable，这时候就完成了写入操作。\n> 这样当服务器宕机的时候也可以从操作日志中恢复MemTable数据，不会使得数据丢失\n> 为什么写了日志缺不写入数据块呢？因为日志属于追加写操作，对磁盘的开销小。而对数据块的操作会有增删改查这4个操作，这些操作的开销比追加写要大很多\n\n当MemTable占用的内存达到一个上限值后，需要将内存的数据转储到对外文件中（即持久化到磁盘）这时会先将使用中的MemTable冻结，后续的操作由一个新创建的MemTable接收，LevelDB将不可变MemTable中的数据排序后转储到磁盘，形成一个新的SSTable文件，这个操作称为Compaction。\n> 感觉Compaction之后应该也要将这个操作写入操作日志当中，因为宕机恢复的话一部分数据是会从操作日志恢复的，如果不做记录会将已经持久化的数据也恢复到内存中。不过这样看来问题也不大。\n\nSSTable文件是内存中的数据不断进行Compaction后形成的，且SSTable的数据是一种层级结构，第0层是Level 0。\n\nSSTable的数据是按照主键大小顺序排列的，因此一个SSTable的范围可以由两端的最大、最小主键所确定。而清单文件就记录了这些信息，包括属于哪个层级、文件名称、最小主键、最大主键。\n\n随着SSTable的产生，清单文件也会越来越大，必然会进行拆分并且设置出哪个清单文件是当前生效的，因此当前文件（Current）就是用来指出当前是哪个清单文件在生效。\n\n因此一次查询将会查询多个文件，对此LevelDB做了一个优化：查询时首先查询MemTable，如果没有结果则往不可变MemTable查询，如果还没有则从新往旧查询SSTable。\n##### 合并操作：\nLevelDB的Compaction操作有两种，分别为minor和major。Minor compaction指的是将内存中的MemTable持久化到磁盘中的操作，major Compaction指的是将磁盘中多个SSTable合并的操作。当某个层级下的SStable文件数目超过一定设置值后，LevelDB会将这个层级的SSTable文件与高一级的SSTable文件合并。Major compaction按照主键顺序依次迭代出所有SSTable文件记录，如果没有价值，则将其抛弃。\n\n> 这样的话这部分操作应该跟bitcask的垃圾回收是一样的，LevelDB中对数据的更新操作也不会直接修改数据块中的数据，而是在后面追加一条。删除数据也是在数据块中做一个标记表示删除。这样在major Compaction的时候可以回收大量的磁盘空间。\n> 补充1:具体的需要根据源码确认，目前只是猜测，这里这篇文章应该会有所收获：[LevelDB的Compaction](https://zhuanlan.zhihu.com/p/46718964)\n> 补充2:看来上面的猜测是正确的，major compaction所作的工作除了减少SSTable的文件数量外，还做了跟Bitcask一样的垃圾回收操作。\n> 从上文可知：\n> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Major compaction的目的是：均衡各个level的数据，保证 read 的性能；合并delete数据，释放磁盘空间，因为leveldb是采用的延迟（标记）删除；合并update的数据，例如put同一个key，新put的会替换旧put的，虽然数据做了update，但是update类似于delete，是采用的延迟（标记）update，实际的update是在compact中完成，并实现空间的释放。\n> PS：跟我上面的猜想一样\n\n#### 数据模型：\n存储系统的数据模型包括三类：文件、关系、键值\n关系模型描述能力强，产业链完整。但是新的键值模型更能满足当前大型系统需要，但是没有同一的业界标准。\n\n##### 文件模型：\n\t以系统以及目录树的形式组织文件。POSIX是应用程序访问文件系统的API标准，主要包括：\n* Open/Close：打开/关闭一个文件，获取文件描述符\n* Read/Write：读取一个文件或者往文件中写入数据\n* Opendir/Closedir：打开或关闭一个目录\n* Readdir：遍历目录\n\nPOSIX还定义了读写操作语义。他要求读写并发时能够保证操作的原子性，即读操作要么读到所有结果，要么什么都读不到；\n\nPOSIX适合单机系统，在分布式文件系统中，出于性能考虑往往不会严格遵守，NFS文件系统允许客户端缓存文件数据，多个客户端并发修改同一个文件时可能出现不一致的情况。但是会出现A、B同时修改了文件C并先后提交的情况，这样会导致A的修改被B的修改所覆盖。\n\n对象模型与文件系统类似，但是弱化了目录树的概念，对象模型要求对象一次性写入到系统中，只能删除整个对象，不允许修改一部分。\n就是平时使用的文件系统\n\n##### 关系模型：\n平时数据库的表结构就是关系模型\n每个关系都是一个表格，由多个元组（行）构成，每个元组包含多个属性（列）。关系名、属性名以及属性类型称作该关系的模式（schema）。\n数据库语言SQL用于描述查询以及修改操作。SQL还有个强大的特性是允许在WHERE、FROM、HAVING子句中使用子查询。\nSQL还有两个重要特征：索引、事务。\n索引用于减少SQL执行时扫描的数据量，提高读取性能；事务规定了各个数据库操作的语义，保证了多个操作并发执行时的ACID特性（原子性、一致性、隔离性、持久性）\n\n##### 键值模型：\n大量的NoSQL系统采用了键值模型，有点类似于HashTable，也称为Key-Value模型。每行记录由主键和值两部分组成。\n所有操作都是基于主键，包括：\n* Put：保存一个Key-Value对\n* Get：读取一个Key-Value对\n* Delete：删除一个Key-Value对\nK-V模型过于简单，使用场景有限，NoSQL系统中比较广泛采用的模型是表格模型。\n\n##### 表格模型：\n表格模型弱化了关系模型中的多表关联，支持基于单表的简单操作，典型的有BigTable、HBase。表格模型除了支持简单的基于主键的操作，还支持范围扫描，另外也支持基于列的操作：\n* Insert：插入一行数据，每行包含若干列\n* Delete：删除一行数据\n* Update：更新整行或者其中的某些列的数据\n* Get：读取整行或者其中某些列的数据\n* Scan：扫描一段范围的数据，根据主键确定扫描的范围，支持扫描部分列、支持按列过滤、排序、分组等\n\n### 数据压缩：\n数据压缩分为有损压缩和无损压缩。\n早期的压缩技术以Huffman编码为主，通过统计字符出现的频率计算最优前缀编码。\n1977年后则以LZ77系列压缩算法为主，通过在一个窗口内部找重复并维护数据字典。\n压缩算法的核心是找重复数据，列式存储技术通过把相同列的数据组织在一起。传统的OLAP数据库，如：Sybase IQ、Teradata、Bigtable、HBase等分布式表格系统都实现了列式存储。\n\n#### 压缩算法：\n压缩需要根据数据的特点选择或开发合适的算法，本质是查找数据的重复或者规律，用尽量少的字节表示。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;存储系统在选择压缩算法的时候需要考虑压缩比和效率。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;读操作需要先读取磁盘中的内容呢再解压缩，写操作需要先压缩再将压缩结果写入磁盘中，整个操作的延时包括压缩/解压缩和磁盘的读写。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;压缩比越大，磁盘读写的数据量越小，而压缩/解压缩的时间也会越长，这里是一个权衡点。\n> igTable使用BMADiff以及Zippy两种压缩算法，牺牲一定的压缩比换取算法执行速度的大幅提升。\n\nHuffman编码/LZ系列压缩算法：自己查资料\n\n#### 列式存储：\n传统的行式数据库将完整的数据行存储在数据页中。如果查询时需要用到大量的列，这种方式在磁盘IO上比较高效。一般来说，OLTP（联机事务处理）应用适合采用这种方式。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果一个应用需要查询大量的数据但是只涉及一两列的情况，行式数据库的开销是十分大的。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此时如果采用列式存储就可以有效优化这部分开销，但于此同时，系统常常查询单条记录的情况下，由于列式存储需要从不同行中查询一个对象的不同属性，他查询的开销是比行式存储大。因此可以根据系统具体的业务更换存储方式。\n\t\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;而且，由于同一列的数据重复率很高，因此，列式数据库压缩时有很大的优势。\n> Google 的BigTable列式数据库对网页库压缩可以达到15倍以上的压缩率\n\n同时，可以针对列式存储做专门的索引优化，比如性别（只有两个值：男、女），可以对这列建立位图索引：\n<img src=\"/img/202102/008.png\" width=\"50%\" height=\"50%\">\n100101:分别从左到右的第1、4、6为1表示数据的第1、4、6行为男\n011010:与上面相反，表示2、3、5行为女\n","tags":["存储"]},{"title":"分布式存储笔记1 分布式存储概述","url":"/2021/02/18/分布式存储笔记1-分布式存储概述/","content":"<hr>\n这是一篇在阅读《大规模分布式存储系统：原理解析与架构实战》时的阅读笔记，由于长时间碎片阅读的关系导致在做这种读书笔记的时候接近复制粘贴。虽然其中会有一小部分自己的想法但都十分零碎，希望后续能改进。\n<hr>\n\n## 分布式存储的概念：\n分布式存储系统是大量普通PC服务器通过英特网互联，对外作为一个整体提供存储服务。\n<!-- more -->\n### 分布式存储的几个特征：\n* 可扩展：分布式存储系统可以扩展到几百台甚至几千台的集群规模，而且，随着集群规模的增长，系统整体性能表现为线性增长\n* 低成本：分布式存储系统的自动容错、自动负载均衡机制使其可以构建在普通PC机之上。另外线性扩展能力也使得增加、减少机器非常方便、可以实现自动运维\n* 高性能：无论是针对整个集群还是单台服务器，都要求分布式存储系统具备高性能\n* 易用：分布式存储系统需要能够提供易用的对外借口，另外，也要求具备完善的监控、运维工具，并能够方便地与其他系统集成，例如，从hadoop云计算系统导入数据\n\n主要挑战：数据、状态信息的持久化，要求在自动迁移、自动容错、并发读写的过程中保证数据的一致性。\n\n主要领域：分布式系统、数据库\n\n### 主要需求：\n* 数据分布：如何将数据分布到多台服务器才能保证数据分布均匀？数据分布到多台服务器后如何实现扩服务器读写操作？（数据平衡算法？）\n* 一致性：如何将数据的多个副本复制到脱台服务器，即使在异常情况下，也能够保证不同副本之间的数据一致性？（用paxos等一致性算法）\n* 容错：如何检测到服务器故障？（心跳检测）如何自动将出现故障的服务器上的数据和服务器迁移到集群中的其他服务器？\n* 负载均衡：新增服务器和集群正常运行过程中如何实现自动负载均衡？数据迁移的过程中如何保证不影响已有服务？（主副服务器模式，数据迁移过程中切换到副服务器）\n* 事务与并发控制：如何实现分布式事务？如何实现多版本并发控制？\n* 易用性：如何设计对外借口使得系统容易使用？如何设计监控系统并将系统的内部状态以方便的形式暴露给运维人员？\n* 压缩/解压缩：如何根据数据的特点设计合理的压缩/解压缩算法？如何平衡压缩算法节省的存储空间和消耗的CPU计算机资源\n\n### 面临的数据需求：\n* 非结构化数据：包括所有格式的办公文档、文本、图片、图像、音频和视频信息等\n* 结构化数据：一般存储在关系数据库中呢，可以用二维关系表结构来表示。结构化数据的模式（Schema，包括属性、数据类型以及数据之间的联系）和内容是分开的，数据的模式需要预先定义。\n* 半结构化数据：介于非结构化数据和结构化数据之间，HTML文档就属于半结构化数据。它一般是自描述的，与结构化数据最大的区别在于，半结构化数据的模式结构和内容混在一起，没有明显的区分，也不需要预先定义数据的模式结构。\n\n### 分布式存储的分类：\n* 分布式文件系统\n* 分布式键值（Key-Value）系统\n* 分布式表格系统\n* 分布式数据库\n\n#### 分布式文件系统：\n主要用于存储非结构化数据，这些数据以对象的形式组织。对象之间没有关联，这样的数据一般称为Blob（Binary Large Object， 二进制大对象）数据。\n\n总体上，分布式文件系统存储三种类型的数据：Blod对象、定长块、大文件。而这三者在存储系统内部又被按照数据块（chunk）来组织。一个数据块可以包含多个Blod对象，也可以由多个定长块组成，同时大文件也能拆分为多个数据块。\n<img src=\"/img/202102/001.png\" width=\"50%\" height=\"50%\">\n> Blod对象、定长块、大文件是逻辑上的概念，在存储系统中会转化为物理上的概念数据块（chunk）进行存储。对Blod对象、定长块、大文件的操作最终都映射为对数据块的操作\n\n#### 分布式键值系统：\n分布式键值系统用于存储关系简单的半结构化数据，他只提供基本主键的CRUD功能。从数据结构的角度看，分布式键值系统与传统的哈希表比较类似，不同的是，分布式键值系统支持将数据分布到集群中的多个存储节点。（一个主键对应的数据会被拆分到多个存储节点）\n分布式键值系统是分布式表格系统的一种简化实现，一般作用与缓存。\n一致性哈希是分布式键值系统中常用的数据分布技术。\n\n#### 分布式表格系统：\n分布式表格系统用于存储关系较为复杂的半结构化数据，与分布式键值系统相比，分布式表格系统不仅仅支持简单的CRUD操作，还支持扫描某个主键范围。\n分布式表格系统以表格为单位组织数据，每个表格包括很多行，通过主键标示一行，支持根据主键的CRUD功能以及范围查找功能。\n分布式表格系统借鉴了很多关系数据库的激素，例如事务。\n与分布式数据库相比，分布式表格系统主要支持针对单张表的操作，不支持一些特别复杂的操作，比如多表关联，多表连接，嵌套子查询等。\n在分布式表格系统中，同一个表格的多个数据行也不要求包含相同类型的列，适合半结构化数据。\n分布式表格系统是一种很好的权很，这类系统可以做到超大规模，而且支持较多的功能，但实现比较复杂，有一定的使用门槛。\n\n#### 分布式数据库：\n分布式数据库一般是从单机关系数据库扩展而来，用于存储结构化数据。分布式数据库采用二维表格组织数据，提供SQL关系查询语言，支持多表关联，嵌套子查询等复杂操作，并提供数据库事务以及并发控制。\n","tags":["存储"]},{"title":"Mysql索引小结","url":"/2021/01/31/Mysql索引小结/","content":"索引是帮助Mysql高效获取数据的数据结构。索引可以和数据文件放一起，也可以单独成为一个索引文件。索引通常是B+树结构。\n<!-- more -->\n索引的优势和劣势：\n优势：\n\t• 可以提高数据检索效率，降低IO成本。\n\t• 通过索引列对数据进行排序，降低排序成本，降低CPU消耗。\n\t\t○ 对索引列进行order by速度会快很多，因为索引保存的是数据的地址，而单独对索引与数据地址进行排序的开销比对数据进行排序的开销要少很多。\n\t\t○ 覆盖索引，不需要回表查询。索引列会保存在单独的索引树中，如果要查询的数据在索引树中就存在，则不需要根据数据地址再查询一遍数据。\n劣势：\n\t• 索引会占用磁盘空间\n\t• 索引会降低表更新效率，更新数据的同时还要更新索引。\n\n#### 索引的分类：\n##### 单列索引：\n\t• 普通索引：Mysql中的基本索引类型，没有限制，允许空值与重复\n\t• 唯一索引：允许为空，但不允许重复\n\t• 主键索引：不允许为空，不允许重复\n##### 组合索引：\n\t• 在表的多个字段上创建索引\n\t• 组合索引的使用遵循最左匹配原则\n\t• 一般情况下建议使用组合索引代替单列索引（主键索引除外）\n##### 全文索引：\n\t• 只能在CHAR、VARCHAR、TEXT等字段使用\n\t• 只在MyISAM、InnoDB（5.6以后）才能使用\n\t• 优先级最高，不会执行其他索引\n##### 空间索引：\n\t• 待补充\n\n#### 索引的存储结构：\n\t• 索引是在存储引擎中实现的，因此不同的存储引擎会使用不同的索引\n\t• MyISAM、InnoDB采用的是B+树索引\n\t\n> B树和B+树的主要区别在于子节点是否存储数据，B+树只在叶子阶段存储数据，且叶子结点都在同一层，并且节点之间通过指针关联\n\n#### 非聚集索引（MyISAM）：\n\t• B+树叶子结点存储的是数据行（数据文件）的指针，数据与索引不在一起。\n\t• 非聚集索引包含主键索引和辅助索引都会存储指针的值\n<img src=\"/img/202101/002.png\" width=\"50%\" height=\"50%\">\n通过主键索引查询到叶子结点后，叶子结点中存储的数据是指向数据行的指针，因此会查询两次。索引文件存储在.mdi中，数据文件存储在.ibd中。\n在MyISAN中，主键索引和辅助索引的区别只在于主键索引是唯一的，结构都是一样。\n\n#### 聚集索引（InnoDB）：\n\t• 主键索引的叶子结点会存储数据行，即数据和索引是在一起的\n\t• 辅助索引只会存储主键值。如果要用辅助索引查询，则先根据辅助索引获取到主键索引，然后再根据主键索引获取到数据行。\n\t• 如果没有主键，则使用唯一索引建立主键。如果没有唯一索引，则会按照一定规则自动创建主键，类型为长整型。\n<img src=\"/img/202101/001.png\" width=\"50%\" height=\"50%\">\n辅助索引保存的是主键的值，即引用主键。因此通过辅助索引查询到主键索引之后还要再根据主键索引查询行数据。这种行为叫做回表查询。\n\n因为辅助索引树保存的数据是索引的列的数据（这部分数据是值，而不是地址），所以如果只是需要查询索引相关的列，则这部分行数据已经存在于辅助索引树中，就不需要回表查询。\n\n例如：table1中id是主键、name是辅助索引、age是一般字段。则他的辅助索引树中保存着id和name的信息。\n当：\n> Select * from table1 where name = 'Bob'    需要回表查询，因为*表示所有数据，但是辅助索引树中只有id和name\n\n\n> Select id,name from table1 where name = 'Bob'    此时不需要回表查询，因为id和name的数据都在辅助索引树中就已经存在。\n\n\nQ:为什么不建议用过长的字段作为主键?\n\nA:因为辅助索引引用的都是主键索引，过长的主键会使得辅助索引树过大。\n\nQ:哪些情况下需要创建索引？\n\t1. 主键自动建立唯一索引\n\t2. 频繁作为查询条件的字段应该创建索引\n\t3. 多表关联查询中，关联字段需要创建索引 on 的两边都是。\n\t4. 查询中排序的字段需要创建索引\n\t5. 频繁查找的字段，创建覆盖索引\n\t6. 查询中统计或者分组的字段应该创建索引 group by\n\nQ:哪些情况下不需要创建索引？\n\t1. 表记录太少\n\t2. 经常进行增删改操作的表\n\t3. 频繁更新的字段\n\t4. where 条件中使用频率不高的字段\n\n\n#### 组合索引\nmysql创建组合索引的规则是首先会对组合索引的最左边第一个字段进行排序，并在此基础在再为第二个索引字段进行排序，类似于order by col1,col2这样的规则。\n组合索引相当于将多个列建立成一个辅助索引树，因此比多列创建单列索引更加节省空间。\n两者的区别：\n\t• 多列建立一棵索引树更加节省空间\n\t• 多列分别建立索引更容易实现覆盖索引\n\n组合索引使用时遵循最左前缀原则：\n\t• like语句在使用时使用前缀匹配的场合下会使用索引 like 'a%'\n\t• 当where语句遇到>、<、like、between时中断索引，即当语句where a = 1 and b = 2 and c > 3 and d = 4时，索引在c > 3处结束，不会调用d的索引。此时需要将创建组合索引时的顺序改变即可：将(a,b,c,d)修改为(a,b,d,c)，这样在调用语句的时候会自动将d = 4放到c > 3之前。\n\n#### 索引失败：\nmysql提供了explain命令对select语句进行分析，并输出直接结果。\n\nexplain执行返回的字段说明：\n\t• id: SELECT 查询的标识符. 每个 SELECT 都会自动分配一个唯一的标识符.\n\t• select_type: SELECT 查询的类型.\n\t• table: 查询的是哪个表\n\t• partitions: 匹配的分区\n\t• type: join 类型\n\t• possible_keys: 此次查询中可能选用的索引\n\t• key: 此次查询中确切使用到的索引.\n\t• ref: 哪个字段或常数与 key 一起被使用\n\t• rows: 显示此查询一共扫描了多少行. 这个是一个估计值.\n\t• filtered: 表示此查询条件所过滤的数据的百分比\n\t• extra: 额外的信息\n\n索引失败的几种情况：\n\t• 使用like的时候用后缀匹配，即 like '%a'，此时索引失效\n\t• or的前后没有同时使用索引，这里的索引只能是单列索引，如果or前后属于组合所以，则一样无法生效\n\t• 使用组合索引时没有使用第一列索引，导致索引失效。即组合索引是有顺序要求，他会在where条件中根据组合索引创建时的顺序重新排列查询条件，如果不存在第一列索引，则索引失效。\n\t• 数据类型出现隐式转换，如varchar不加单引号会自动转换为int型，使索引失效。\n\t• 在索引字段上使用了not，!=，<>。!=操作不会使用索引，它只会产生全盘扫描。优化方法：将其拆分为 key > 0 or key < 0\n\t• 对索引字段执行计算操作\n\t• 当全盘扫描比使用索引快时mysql会自动使用全盘扫描，此时索引失效。\n\n> 使用is null，is not null时，依旧会调用索引，mysql在选择是否使用索引时的一个依据就是全盘扫描和使用索引时的开销对比。\n\n","tags":["mysql"]},{"title":"游戏世界中的数学工具","url":"/2020/12/06/游戏世界中的数学工具/","content":"游戏是在计算机上实时模拟虚拟世界的数学模型。\n\n虽然在游戏中会用到几乎所有的数学分支，但最常用的只有两种：三维矢量与矩阵。\n<!-- more -->\n## 点和矢量\n物体在三维世界中的三个要素：\n1. 位置（position）\n2. 定向（orientation）\n3. 比例（scale）\n\n通过连续地修改这三个属性来实现动画效果，将物体变换（transform）至屏幕空间使得物体渲染到屏幕上。在游戏中三维物体几乎都是以三角形组成，三角形的三个顶点（vertex）用点（point）来表示。\n\n### 坐标系\n常用的坐标系有三种：\n1. 笛卡尔坐标：由x、y、z三根轴组成\n2. 圆柱坐标：垂直高度h，从垂直轴发射的辐射轴r，偏航角（yaw）角度θ组成\n3. 球坐标：俯视角（pitch）、偏航角（yaw）、半径长度组成\n\n<img src=\"/img/202012/001.png\" width=\"50%\" height=\"50%\">\n\n> 坐标系选用的例子：\n> Q:若要让物体向漩涡一样绕着主角旋转时用什么坐标系？\n> A:圆柱坐标。要绘制漩涡动画，只需要简单地在θ上加上恒定角速率，在辐射轴r上加入少许向内的恒定线性速率，在h上加上向上的恒定线性速率，物体就会慢慢地旋转向上到角色中\n\n### 矢量\n矢量由三个标量（x、y、z）组成，即若要在三维空间中表示一个点，至少需要3个参数。\n> 矢量**v** = [x, y, z]\n\n#### 笛卡尔基矢量\n笛卡尔积单位矢量：在笛卡尔坐标系中，沿着三根轴并且模为1的矢量成为笛卡尔积单位矢量，分别表现为:\n1. **i** = [1, 0, 0]\n2. **j** = [0, 1, 0]\n3. **k** = [0, 0, 1]\n\n在笛卡尔坐标系中的任意点或矢量都能用3个标量与三个基矢量乘积之和表示标量。例如[5 3 -2] = 5**i** + 3**j** - 2**k**。\n\n#### 矢量运算\n\n##### 矢量与标量乘法 - 矢量的缩放\n矢量**a**与标量s相乘，等于**a**中的每个分量和s相乘:\n<img src=\"/img/202012/002.png\" width=\"50%\" height=\"50%\">\n矢量与标量相乘表示矢量方向不变，缩放矢量的模，乘以-1表示将矢量方向反转（头尾互换）。其中，s被称为**缩放因子**(scale factor)，其中，矢量的每个轴的缩放因子也会不同，因此将矢量缩放时每个轴的缩放因子是否相同，将其分为：统一缩放和非统一缩放。非统一缩放可以表示为矢量与缩放矢量的分量积,设矢量**a**、缩放矢量**s**,则a的分量积公式如下：\n<img src=\"/img/202012/003.png\" width=\"50%\" height=\"50%\">\n\n* 统一缩放：表现为标量s乘以矢量**a**。\n* 非统一缩放：表现为两个矢量的分量积，分量积不等于两个矢量相乘。这种运算方式又被称为阿达马积。\n\n##### 矢量的加法与减法\n矢量的相加等于将两个矢量首尾相连后剩下首尾相连所形成的新的矢量。\n<img src=\"/img/202012/004.png\" width=\"50%\" height=\"50%\">\n\n矢量的相减等同于矢量加上另一个矢量的反方向矢量。\n<img src=\"/img/202012/005.png\" width=\"50%\" height=\"50%\">\n\n二者表现如下：\n<img src=\"/img/202012/006.png\" width=\"50%\" height=\"50%\">\n\n矢量与点的加减运算如下：\n* 方向 + 方向 = 方向\n* 方向 - 方向 = 方向\n* 点 + 方向 = 点\n* 点 - 点 = 方向\n* 点 + 点 = **无意义**\n\n##### 模\n矢量的模等于矢量的各个标量的平方和开根号。\n<img src=\"/img/202012/007.png\" width=\"50%\" height=\"50%\">\n类似于勾股定理，在二维空间中，z轴为0，则可以更加直观的看到矢量模的计算：\n<img src=\"/img/202012/008.png\" width=\"50%\" height=\"50%\">\n\n> 我们在进行模的比较的时候通常可以用模的平方来比较，这样可以减少开销\n\n##### 矢量运算的实际应用\n设某人工智能的角色所在位置为P1，其速度为**v**，则可以找到他的下一帧位置P2，方法是把**v**以△t缩放，再加上P1。等式为P2 = P1 + **v**△t。这称为**显式欧拉法**。其中速度**v**恒定才有效。\n<img src=\"/img/202012/009.png\" width=\"50%\" height=\"50%\">\n\n##### 归一化与单位矢量\n单位矢量：模为1的矢量\n\n给定一个矢量**v**，其模为v=|**v**|,将其转化为方向相同的单位矢量**u**的过程如下：\n<img src=\"/img/202012/010.png\" width=\"50%\" height=\"50%\">\n这个过程称之为**归一化**\n\n##### 法矢量\n在三维世界中，一个平面可以由平面上的一点P以及一个垂直于该平面的矢量组成。这个矢量又被称为**法矢量**。\n> 法矢量不等于单位矢量，他的模不一定为1\n\n##### 点积和投影\n矢量之间可以相乘，这种相乘跟上面所描述的*分量积*完全不同。通常最常用的乘法有两种：\n* 点积：又被称为标量积或内积\n* 叉积：又被称为矢量积或外积\n\n两个矢量的点积的结果是一个标量，这个标量等于两个矢量的各个标量相乘的和：\n<img src=\"/img/202012/011.png\" width=\"50%\" height=\"50%\">\n\n点积也可以写成两个矢量的模相乘后再乘以两个矢量之间夹角的余弦：\n<img src=\"/img/202012/012.png\" width=\"50%\" height=\"50%\">\n\n点积符合交换律，且在加法上符合分配律。\n\n若**u**是单位矢量，则矢量**a**与矢量**u**的点积等于在**u**所在的直线上矢量**a**的投影。\n<img src=\"/img/202012/013.png\" width=\"50%\" height=\"50%\">\n\n若矢量与自身相乘，由于矢量的夹角θ为0°，所谓cosθ=1，得矢量与自身的点积为矢量模的平方\n<img src=\"/img/202012/014.png\" width=\"50%\" height=\"50%\">\n\n点积通常用于判断两个矢量是否共线或垂直，也可用来判断两个矢量是否大致在相同或相反方向：\n* 共线：**a**·**b**=|**a**||**b**|=ab。夹角θ为0，cosθ=1\n* 共线但是方向相反：**a**·**b**=-ab。夹角θ为180，cosθ=-1\n* 垂直：**a**·**b**=0，cosθ=0\n* 相同方向：**a**·**b** > 0(即cosθ > 0,夹角小于90°)\n* 相反方向：**a**·**b** < 0(即cosθ < 0,夹角大于90°)\n\n<img src=\"/img/202012/015.png\" width=\"50%\" height=\"50%\">\n\n##### 叉积\n两个矢量的叉积会产生一个新的矢量，新的矢量垂直于相乘的两个矢量所组成的平面，因此，叉积只存在与三维空间。\n<img src=\"/img/202012/016.png\" width=\"50%\" height=\"50%\">\n\n叉积的模等于两个相乘矢量的模乘以两个矢量夹角的正弦。所以当两个矢量共线时，他们的叉积为0。\n<img src=\"/img/202012/017.png\" width=\"50%\" height=\"50%\">\n\n若两条矢量分别是平行四边形的两条边，则两个矢量的叉积等于这个平行四边形的面积。\n<img src=\"/img/202012/018.png\" width=\"50%\" height=\"50%\">\n\n叉积的方向根据所选择的坐标系法则有关，左右法则和右手法则会得到不同方向的叉积。\n\n叉积不符合交换律，叉积的先后顺序影响最终结果。但是符合反交换律。\n<img src=\"/img/202012/019.png\" width=\"50%\" height=\"50%\">\n\n在加法上符合分配律\n<img src=\"/img/202012/020.png\" width=\"50%\" height=\"50%\">\n\n根据叉积的性质，可以得到笛卡尔积之间互相转换的公式：\n<img src=\"/img/202012/021.png\" width=\"50%\" height=\"50%\">\n\n#### 点和矢量的线性插值\n在游戏中，为了保证两个点之间移动时的顺滑，需要得到点到点之间的的中间点。为了得到这个中间点的计算称为**线性插值**，通常简写成LERP。其定义如下，设从点**A**到点**B**，其中的中间点**L**与**A**的距离是**A**到**B**的距离的β（0 ≤ β ≤ 1）。则：\n<img src=\"/img/202012/022.png\" width=\"50%\" height=\"50%\">\n从几何上看效果如下：\n<img src=\"/img/202012/023.png\" width=\"50%\" height=\"50%\">\n\n## 矩阵\n矩阵（matrix）由mxn个标量组成的长方形数组，在游戏世界中方便用于表示旋转（transformaction）、平移（translation）、缩放（scale）。\n\n3x3的矩阵表示纯旋转；4x4的矩阵表示旋转、平移、缩放。\n\n### 矩阵乘法\n当两个矩阵的内维相等的时候才能相乘，设**A**为m × p的矩阵，**B**为p × n的矩阵，那么称m × n的矩阵**C**为矩阵**A**与**B**的乘积，记作**C**=**A**·**B**。\n新的矩阵的各个标量计算如下：\n<img src=\"/img/202012/024.png\" width=\"50%\" height=\"50%\">\n\n矩阵的乘法先后顺序影响最终结果，不符合乘法交换律。矩阵的乘法有时又被称为**串接**。因为矩阵表示一次变换，矩阵的相乘表示将多个变换串接起来。\n\n### 矩阵表示点和矢量\n点和矢量都可以表示为行矩阵（1 × n）或列矩阵（n × 1）。其中n表示使用中的空间纬度，通常是2或3.例如矢量**v**=(3 4 -1)可以写成\n<img src=\"/img/202012/025.png\" width=\"50%\" height=\"50%\">\n或\n<img src=\"/img/202012/026.png\" width=\"50%\" height=\"50%\">\n\n两种矩阵方式的选择会影响矩阵相乘的次序。因为矩阵相乘的时候，两个矩阵的内维需要相等。所以：\n* 要把1 × n行矢量乘以n × n矩阵，矢量必须置于矩阵的左方<img src=\"/img/202012/027.png\" width=\"50%\" height=\"50%\">\n* 要把n × n矩阵乘以n × 1列矩阵，矢量必须位于矩阵的右方<img src=\"/img/202012/028.png\" width=\"50%\" height=\"50%\">\n\n### 单位矩阵\n单位矩阵与其他任何矩阵**M**相乘都等于**M**本身。其表现为对角线元素都是1，其他元素都是0。通常写作**I**。\n<img src=\"/img/202012/029.png\" width=\"50%\" height=\"50%\">\n\n### 逆矩阵\n设一个矩阵为**A**，则它的逆矩阵表示为**A⁻¹**,逆矩阵能还原矩阵的变换。所以矩阵与其逆矩阵相乘结果是单位矩阵。通常用高斯消去法或LU分解求得。矩阵串街后求逆等于反向串接各个矩阵的逆矩阵。\n<img src=\"/img/202012/030.png\" width=\"50%\" height=\"50%\">\n\n### 转置矩阵\n矩阵**M**的转置写作**Mᵀ**,转置矩阵就是以原来矩阵的对称轴做反射。和逆矩阵一样，矩阵串接后的转置矩阵等于反向串街各个矩阵的转置矩阵。\n<img src=\"/img/202012/031.png\" width=\"50%\" height=\"50%\">\n\n转置矩阵有一个重要的特点：\n> 纯旋转的矩阵他的逆矩阵与转置矩阵是相同的\n\n因此基于此，通常用转置矩阵代替逆矩阵，因为求转置矩阵的速度比求逆矩阵快。\n\n### 齐次坐标\n当点或矢量从三维(3 × 3)延伸至四维(4 × 4)的过程称为**齐次坐标**。因为4 × 4矩阵能够同时表示旋转、平移、缩放，因此在游戏中最常用的就是4 × 4矩阵。\n\n三维空间中，若一个矢量**r**绕z轴旋转ø°，则可以表示为：\n<img src=\"/img/202012/032.png\" width=\"50%\" height=\"50%\">\n但是3 × 3矩阵无法表示平移与缩放，而4 × 4可以\n<img src=\"/img/202012/033.png\" width=\"50%\" height=\"50%\">\n\n### 变换方向矢量\n矩阵同时携带旋转、平移、缩放这三个变换信息，当矩阵作用在点上时，三者都可以产生作用。但是当用矩阵变换一个方向矢量时要忽略**平移**效果。因为方向矢量并不会产生平移，而且一旦平移就会改变他的模，这不是我们想要看到的。\n\n在方向矢量与矩阵相乘是，把矢量的齐次坐标中的ω设置成0即可。\n<img src=\"/img/202012/034.png\" width=\"50%\" height=\"50%\">\n\n在将四维的齐次坐标转化为三维的非齐次坐标的时候通常是将x、y、z分别除以ω，因此当ω为0的时候会产生无穷大，因为在三维空间中的纯方向矢量在四维空间中表示一个无限远的点。\n<img src=\"/img/202012/035.png\" width=\"50%\" height=\"50%\">\n\n### 基础变换矩阵\n从上可以知道4 × 4矩阵可以表示旋转、平移与缩放。他们的分布如下：\n<img src=\"/img/202012/036.png\" width=\"50%\" height=\"50%\">\n\n* 左上角的3 × 3矩阵**U**代表旋转或缩放\n* 1 × 3平移矢量**t**\n* 3 × 1矢量**O** = [0 0 0]ᵀ\n* 右下角标量1\n\n从中可以看到，最右边一列的标量都是常量，并不会随着矩阵的功能而改变，因此为了节省空间，在计算机中可以用4 × 3矩阵代替。\n\n### 坐标空间\n确定物体的具体坐标之前需要确定参照物用于构建坐标系，在三维游戏世界，根据参照物的不同将其分为三个坐标空间：世界空间、模型空间、观察空间。三者的区别如下：\n* 世界空间：以游戏世界中的某点为原点，游戏世界中所有物体都可以通过世界空间表示，这个坐标将所有物体联系起来组成虚拟世界。是一个固定坐标。\n* 模型空间：基于游戏中对象的坐标空间，由游戏中对象的某点为原点与其自身质点所构成的坐标系，通常是笛卡尔坐标系。\n* 观察空间：又称摄像机空间，是固定在摄像机的坐标系，他的原点置于摄像机的焦点。\n\n### 基的变更\n上面描述的三者空间可以互相转化，即模型空间与观察空间可以转化为世界空间。这样当玩家角色与物体接触的时候通过基于同一个坐标系的数据进行判断是否产生碰撞。三个坐标构成层次关系，其中世界坐标是最底层的父坐标。\n\n把点或者矢量从坐标系**C**转移到坐标系**P**写作**M**ᴄ→ᴘ,设点在坐标系C中的点坐标为**P**ᴄ,在坐标系**P**中的坐标是**P**ᴘ。则他们的转化公式如下：\n<img src=\"/img/202012/037.png\" width=\"50%\" height=\"50%\">\n其中：\n* **i**ᴄ为子空间x轴的单位基矢量，这个矢量用父空间坐标表示\n* **j**ᴄ为子空间y轴的单位基矢量，这个矢量用父空间坐标表示\n* **k**ᴄ为子空间z轴的单位基矢量，这个矢量用父空间坐标表示\n* **t**ᴄ为子坐标系相对于父坐标系的平移\n\n例子如下：\n假设子空间绕z轴旋转角度ᵞ，没有平移，得到公式如下：\n<img src=\"/img/202012/038.png\" width=\"50%\" height=\"50%\">\n旋转示例如下：\n<img src=\"/img/202012/039.png\" width=\"50%\" height=\"50%\">\n\n在坐标空间的变换过程中有两种选择：变换坐标系或者变换矢量：\n<img src=\"/img/202012/040.png\" width=\"50%\" height=\"50%\">\n具体选择哪一种要看情况而定。\n\n## 四元数\n3 × 3矩阵并不是最理想的旋转表达形式，原因如下：\n1. 3 × 3矩阵有9个浮点数来旋转矢量，但实际上旋转只有三个自由度（偏航角、俯仰角、滚动角）\n2. 用矢量矩阵乘法来旋转矢量需要3个点积运算。\n3. 在计算机图形学中，表示两个矢量位置的变换通常需要顺滑的过度，因此需要计算两个位置中间的值，这用矩阵计算很麻烦。即不能完全实现游戏对象的变换。\n\n因此采用四元数来表示旋转，定义如下：\n> 单位长度的四元数能代表三维旋转\n\n即四元数中四个标量的平方和等于1的情况下都能表示三维旋转。\n\n### 单位四元数视为三维旋转\n四元数中有四个标量。\n<img src=\"/img/202012/042.png\" width=\"50%\" height=\"50%\">\n前三个标量组成的矢量**v**是旋转的单位轴乘以旋转半角的正弦。第四个标量qs是旋转半角的余弦。可以写成:\n<img src=\"/img/202012/041.png\" width=\"50%\" height=\"50%\">\n其中**a**为旋转轴方向的单位矢量，θ是旋转角度。旋转的方向遵守右手守则。从上面可以得到以下等式：\n<img src=\"/img/202012/043.png\" width=\"50%\" height=\"50%\">\n\n### 四元数运算\n\n#### 乘法\n乘法是四元数最常用的计算方法之一，给定两个四元数q和p，他们分别代表旋转**Q**和**P**，则qp代表两个旋转的合成旋转，即先旋转**Q**再旋转**P**。这种跟旋转相关的四元数乘法又叫做格拉斯曼积，定义的pq乘积如下：\n<img src=\"/img/202012/044.png\" width=\"50%\" height=\"50%\">\n结果分成矢量部分和标量部分，其中矢量部分的结果为四元数的x、y、z，标量部分是四元数的w。\n\n#### 共轭以及逆四元数\n四元素q的逆四元数写作q⁻¹,逆四元数和原四元数的乘积等于1.即qq⁻¹=（0**i** + 0**j** + 0**k** + 1）= 1.所以四元数[0 0 0 1]代表零旋转。\n\n四元数q的共轭写作qº（这里的º应该是*，我打不出来。）定义如下：\n<img src=\"/img/202012/045.png\" width=\"50%\" height=\"50%\">\n从公式中可以知道共轭是四元数矢量部分求反，但保持标量部分不变，有了这个定义，逆四元数又可以写成：\n<img src=\"/img/202012/046.png\" width=\"50%\" height=\"50%\">\n由于我们用于旋转的四元数都是单位长度，因此分母为1，化简如下：\n<img src=\"/img/202012/047.png\" width=\"50%\" height=\"50%\">\n所以四元数的逆四元数等于四元数的共轭，但是求共轭的速度快于求逆。因此通常可以用共轭代替求逆的步骤。同时这也比计算3 × 3的逆矩阵快，所以这一步可以用作性能的优化。\n\n多个四元数之积的逆四元数等于相反的逆四元数相乘，共轭同理。\n<img src=\"/img/202012/048.png\" width=\"50%\" height=\"50%\">\n\n### 用四元数旋转矢量\n用四元数旋转矢量，首先要把矢量转化为四元数形式。由于四元数比矢量多了一个标量，只要将第四个标量设置为0即可。给定矢量**v**,他的四元数形式表示为**v**ᶥ=[**v** 0]=[x y z 0]。\n\n要用四元数q旋转矢量**v**，需要先用q乘以**v**然后再乘以q的逆四元数。\n<img src=\"/img/202012/049.png\" width=\"50%\" height=\"50%\">\n因为旋转的四元数都是单位四元数，所以可以用四元数的共轭代替逆四元数：\n<img src=\"/img/202012/050.png\" width=\"50%\" height=\"50%\">\n\n> 在游戏中，各个模型空间相对于世界空间就可以用四元数表示，当要将模型空间内的坐标转化为世界空间的坐标时，只需要将其乘以模型空间定量的四元数即可\n\n#### 四元数的串接\n从上面的四元数用于矢量可以知道，四元数的串接类似于一层一层包裹矢量：\n<img src=\"/img/202012/051.png\" width=\"50%\" height=\"50%\">\n\n### 等价的四元数和矩阵\n任何三维旋转都可以从3 × 3矩阵表达方式和四元数表达方式之间自由变换。设q=[q**v** qs] = [x y z w]，则转化为矩阵表达**R**如下：\n<img src=\"/img/202012/052.png\" width=\"50%\" height=\"50%\">\n类似的，**R**也可以通过特定的计算得到四元数。下面是一段C/C++代码用于将矩阵转化为四元数:\n\n```C++\n#include \"math.h\"\n#include \"iostream\"\n\nvoid rotMatrixToQuternion(const float R[3][3], float q[4])\n{\n    float trace = R[0][0] + R[1][1] + R[2][2];\n    \n    //检测主轴\n    if (trace > 0.0f) {\n        float s = sqrt(trace + 1.0f);\n        q[3] = s * 0.5f;\n\n        float t = 0.5f / s;\n        q[0] = (R[2][1] - R[1][2]) * t;\n        q[1] = (R[0][2] - R[2][0]) * t;\n        q[2] = (R[1][0] - R[0][1]) * t;\n    } else {\n        //主轴为负\n        int i = 0;\n        if (R[1][1] > R[0][0]) i = 1;\n        if (R[2][2] > R[i][i]) i = 2;\n\n        static const int next[3] = {1, 2, 0};\n        int j = next[i];\n        int k = next[j];\n\n        float s = sqrt((R[i][i] - (R[j][j] + R[k][k])) + 1.0f);\n        q[i] = s * 0.5f;\n\n        float t;\n        if (s != 0.0f)  t = 0.5f / s;\n        else            t = s;\n\n        q[3] = (R[k][j] - R[j][k]) * t;\n        q[j] = (R[j][i] + R[i][j]) * t;\n        q[k] = (R[k][i] + R[i][k]) * t;\n    }\n}\n```\n\n### 旋转性的线性插值\n在两个旋转中间，为了保证物体变换表现的顺滑，需要获取到物体在变换时处于两个旋转中间的位置，这个点就叫做插值。\n\n虽然两个旋转矩阵之间也可以做插值，但是他们的计算开销远远大于四元数。\n\n最简单快捷的插值方法就是套用四维矢量的线性插值（LERP）至四元数。给定两个代表旋转A、B的四元数qᴀ、qʙ，可以找出旋转A到旋转B之间β百分点的中间旋转qʟᴇʀᴘ:\n<img src=\"/img/202012/053.png\" width=\"50%\" height=\"50%\">\n> 插值后的四元数需要再次归一，因为插值计算无法保证矢量长度。\n\n在图像中体现如下：\n<img src=\"/img/202012/054.png\" width=\"50%\" height=\"50%\">\n\n#### 球体中的插值\nLERP计算的缺点在于没有考虑四元数其实是四维**超球**上的点。LERP其实是沿着球体的弦进行插值，这样会导致：当β以恒定速率改变时，旋转动画并不是以恒定速率变换的，旋转会在两端慢，在中间骤然加快。为了解决这个问题产生了一个新的插值：**球面线性插值**（SLERP）。SLERP使用正弦和余弦在四维超球面的大圆上进行插值，而不是沿着弦上插值，这样，当β以恒定速率改变时，插值结果会以常速角速度变化。两者的区别如下：\n<img src=\"/img/202012/055.png\" width=\"50%\" height=\"50%\">\n\nSLERP公式如下：\n<img src=\"/img/202012/056.png\" width=\"50%\" height=\"50%\">\n其中wᴘ和wǫ取代（1-β）和β，这两个参数使用到了两个四元数之间的正弦与余弦夹角。\n<img src=\"/img/202012/057.png\" width=\"50%\" height=\"50%\">\n\n虽然SLERP比LERP更加完善，但同时计算插值的开销也更大，具体使用那种插值计算需要根据实际情况来看，在游戏中，优化也占很重要的一部分。\n\n## 各种旋转\n### 欧拉角\n欧拉角能表示旋转，由三个标量组成（偏航角、俯视角、滚动角）。\n\n优点：\n* 小巧，只由3个浮点数组成\n* 直观，容易把三个数值视觉化\n* 对围绕单轴的旋转容易插值\n\n缺点：\n* 对任意方向的旋转不方便插值\n* 会遭遇万向节死锁\n* 旋转次序影响最终结果\n* 依赖数据多，需要有x/y/z轴和前/左右/上方向上的映射\n\n### 3 x 3矩阵\n优点：\n* 不受万向节锁的影响，独一无二地表达任意旋转\n* 旋转可以通过矩阵乘法直接施加在矢量或点上\n* 市面上大多数CPU或GPU都针对矩阵乘法做了内建支持\n* 纯旋转的逆矩阵为转置矩阵，求解方便\n\n缺点：\n* 不直观，看见一个大数字表无法直观的将其想象在三维空间的变换\n* 不容易插值，计算繁琐\n* 占用空间大，需要占用9个存储空间\n\n### 轴角\n通过一个单位矢量定义旋转轴，一个标量定义旋转角表示旋转。类似于四元数表示法[旋转轴 旋转角]，写成[**a** θ]的形式。其中**a**是旋转轴、θ是旋转角。\n\n优点：\n* 表现直观，直接存在旋转轴与旋转角\n* 紧凑，只占用4个存储空间\n\n缺点：\n* 无法进行简单的插值\n* 轴角形式无法直接施加于点或矢量，需要将其先转化为四元数\n\n### 四元数\n与轴角的区别：旋转轴矢量长度为旋转半角的正弦，第四个分量是旋转半角的余弦。\n\n优点：\n* 四元数乘法能串接旋转，并把旋转直接施加于矢量和点\n* 可以轻易地用LERP和SLERP进行旋转插值\n* 存储空间小，只需要4个浮点数的存储空间\n\n### SQT变换\n4 x 4表示任意变换（旋转、平移、缩放），但是占用空间大，因此，将四元数加上平移矢量与缩放因子来实现任意仿射变换。他的体积比4 x 4矩阵小。统一缩放只需要8个存储空间，非统一缩放需要10个存储空间。\n> 因为统一缩放的时候缩放因子是一个标量，而非统一缩放的时候缩放因子是由三个标量组成的矢量\n\n优点：\n* 可以表示任意仿射变换\n* 空间比4 x 4矩阵小\n* 容易插值，各个部分可以采用不同的插值算法，平移矢量可以用LERP、四元数则可以矢量LERP或SLERP。\n\n### 其他\n其他还有**对偶四元数**、**旋转和自由度**等方式。\n\n","tags":["数学"]},{"title":"【剑指 offer】15.二进制中1的个数","url":"/2020/11/30/【剑指-offer】15-二进制中1的个数/","content":"来源：力扣（LeetCode）\n\n链接：https://leetcode-cn.com/problems/er-jin-zhi-zhong-1de-ge-shu-lcof\n\n### 题目描述\n请实现一个函数，输入一个整数（以二进制串形式），输出该数二进制表示中 1 的个数。例如，把 9 表示成二进制是 1001，有 2 位是 1。因此，如果输入 9，则该函数输出 2。\n\n示例 1：\n```bash\n输入：00000000000000000000000000001011\n输出：3\n解释：输入的二进制串 00000000000000000000000000001011 中，共有三位为 '1'。\n```\n\n示例 2：\n```bash\n输入：00000000000000000000000010000000\n输出：1\n解释：输入的二进制串 00000000000000000000000010000000 中，共有一位为 '1'。\n```\n\n示例 3：\n```bash\n输入：11111111111111111111111111111101\n输出：31\n解释：输入的二进制串 11111111111111111111111111111101 中，共有 31 位为 '1'。\n```\n\n提示：\n* 输入必须是长度为 32 的 二进制串 。\n<!-- more -->\n### 我的解题\n#### 思路\n通过逐位比较来确定给定数字中出现的1的个数，这里我一开始使用了取模，实际上还有更快的位运算，不过这里有个小坑，即题目中给定的int需要看成无符号位，即当int为负数的时候最高位是1。因此需要在Java中通过>>>来达成数字的无符号移动。\n\n#### 结果\n```java\npublic class Solution {\n    // you need to treat n as an unsigned value\n    public int hammingWeight(int n) {\n        int size = 0;\n        while (n != 0) {\n            size += (n & 1);\n            n >>>= 1;\n        }\n        return size;\n    }\n}\n```\n### 最优解\n#### 思路\n有两种方法，一种就是上面的通过每一位的比较来得到数字中1的个数，但是还有一个更快的方法，即通过n&(n-1)这样的位运算。n-1在二进制中表示将n的最低位的1修改成0。与上面方法的区别就是，逐位计算循环次数固定是数字的位数，这种方法的循环次数则是数字中1的个数。设上一种方法的时间复杂度为O(N)、这种时间复杂度为O(M)，则N >= M.\n<img src=\"https://pic.leetcode-cn.com/9bc8ab7ba242888d5291770d35ef749ae76ee2f1a51d31d729324755fc4b1b1c-Picture10.png\" width=\"50%\" height=\"50%\">\n\n#### 解法\n```java\npublic class Solution {\n    // you need to treat n as an unsigned value\n    public int hammingWeight(int n) {\n        int size = 0;\n        while (n != 0) {\n            size += 1;\n            n = n & (n - 1);\n        }\n        return size;\n    }\n}\n```","tags":["LeetCode算法题"]},{"title":"【剑指 offer】14- II. 剪绳子 II","url":"/2020/11/18/【剑指-offer】14-II-剪绳子-II/","content":"来源：力扣（LeetCode）\n\n链接：https://leetcode-cn.com/problems/jian-sheng-zi-ii-lcof\n### 题目描述\n给你一根长度为 n 的绳子，请把绳子剪成整数长度的 m 段（m、n都是整数，n>1并且m>1），每段绳子的长度记为 k[0],k[1]...k[m - 1] 。请问 k[0]*k[1]*...*k[m - 1] 可能的最大乘积是多少？例如，当绳子的长度是8时，我们把它剪成长度分别为2、3、3的三段，此时得到的最大乘积是18。\n\n答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1。\n\n示例 1：\n```bash\n输入: 2\n输出: 1\n解释: 2 = 1 + 1, 1 × 1 = 1\n```\n示例 2:\n```bash\n输入: 10\n输出: 36\n解释: 10 = 3 + 3 + 4, 3 × 3 × 4 = 36\n```\n\n提示：**2 <= n <= 1000**\n<!-- more -->\n### 我的解题\n#### 思路\n这题根昨天的题目几乎一样，不过要注意的一点就是由于多了一个取模的条件，所以直接通过dp的max比较已经不行了。但是思路还是一样的。根据数学公式得出，数字n按照拆分的优先级分别是3、2、1，因此我们可以通过循环一个一个减去对应的数字然后保留计算结果，要注意的是，当结果已经小于等于4的时候需要特殊对待。\n* n = 4：直接结果与4相乘并退出循环呢\n* n < 3：结果与n相乘并退出循环\n\n#### 结果\n```java\nclass Solution {\n    public int cuttingRope(int n) {\n        if (n <= 3) {\n            return n - 1;\n        } else if (n == 4) {\n            return n;\n        }\n\n        int p = 1000000007;\n        long ans = 1L;\n        int a = 3;\n        while (n > 0) {\n            ans *= a;\n            ans %= p;\n            n -= 3;\n            if (n == 4 || n < 3) {\n                a = n;\n            }\n        }\n        return (int) ans;\n    }\n}\n```\n\n### 最优解\n#### 思路\n数学的推导过程与上一题一样，但是在最后需要考虑**大数求余**的问题。针对这个问题主要有两种解决思路：\n1. 循环求余\n2. 快速冥求余\n\n两种方法都是根据以下求余循环法则推导而成：\n> (xy)%p = (x%p)(y%p)%p\n\n##### 1.循环求余\n根据上述公式推导得：<br>\n<img src=\"/img/202011/009.png\" width=\"50%\" height=\"50%\"><br>\n我们可以通过寻汗操作n1、n2...na对p的余数，保证每轮的中间值都在int32的范围内。\n\n```python\n# 求 (x^a) % p —— 循环求余法\ndef remainder(x, a, p):\n    rem = 1\n    for _ in range(a):\n        rem = (rem * x) % p\n    return rem\n```\n##### 2.快速冥求余\n根据求余公式推导出：<br>\n<img src=\"/img/202011/010.png\" width=\"50%\" height=\"50%\"><br>\n当a为奇数时，a/2不是整数，因此将其分为两种情况（“//”表示向下取整的除法）\n<img src=\"/img/202011/011.png\" width=\"50%\" height=\"50%\"><br>\n利用以上公式，可通过循环操作每次把指数 aa 问题降低至指数 a//2 问题，只需循环 log_2(N) 次，因此可将复杂度降低至对数级别。封装方法代码如下所示。\n\n```python\n# 求 (x^a) % p —— 快速幂求余\ndef remainder(x, a, p):\n    rem = 1\n    while a > 0:\n        if a % 2: rem = (rem * x) % p\n        x = x ** 2 % p\n        a //= 2\n    return rem\n```\n\n#### 结果\n```java\nclass Solution {\n    public int cuttingRope(int n) {\n        if(n <= 3) return n - 1;\n        int b = n % 3, p = 1000000007;\n        long rem = 1, x = 3;\n        for(int a = n / 3 - 1; a > 0; a /= 2) {\n            if(a % 2 == 1) rem = (rem * x) % p;\n            x = (x * x) % p;\n        }\n        if(b == 0) return (int)(rem * 3 % p);\n        if(b == 1) return (int)(rem * 4 % p);\n        return (int)(rem * 6 % p);\n    }\n}\n```","tags":["LeetCode算法题"]},{"title":"【剑指 offer】14- I. 剪绳子","url":"/2020/11/17/【剑指-offer】14-I-剪绳子/","content":"来源：力扣（LeetCode）<br>\n链接：https://leetcode-cn.com/problems/jian-sheng-zi-lcof\n### 题目描述\n给你一根长度为 n 的绳子，请把绳子剪成整数长度的 m 段（m、n都是整数，n>1并且m>1），每段绳子的长度记为 k[0],k[1]...k[m-1] 。请问 k[0]\\*k[1]\\*...\\*k[m-1] 可能的最大乘积是多少？例如，当绳子的长度是8时，我们把它剪成长度分别为2、3、3的三段，此时得到的最大乘积是18。\n\n示例 1：\n\n```bash\n输入: 2\n输出: 1\n解释: 2 = 1 + 1, 1 × 1 = 1\n```\n\n示例 2:\n```bash\n输入: 10\n输出: 36\n解释: 10 = 3 + 3 + 4, 3 × 3 × 4 = 36\n```\n提示：\n> 2 <= n <= 58\n<!-- more -->\n### 我的解题\n#### 思路\n第一眼就认为用动态规划求解：设将n长的绳子分成m份后每份的乘积为f(n, m)，且 1 < m <= n,但这样就很难得到状态转移方程，因此思路不通。\n\n后来用f(n)表示将n份绳子分成至少2份后各个结果最大的积。则f(0) = 0, f(1) = 1;后面我自己想状态转移方程就想不到了。所以还是直接看的答案。\n\n### 最优解\n#### 思路\n\n##### 动态规划\n对于的正整数 n，当 n≥2 时，可以拆分成至少两个正整数的和。令 k 是拆分出的第一个正整数，则剩下的部分是 n−k，n−k 可以不继续拆分，或者继续拆分成至少两个正整数的和。由于每个正整数对应的最大乘积取决于比它小的正整数对应的最大乘积，因此可以使用动态规划求解。\n\n* **dp数组的含义**： dp[i] 表示将正整数 i 拆分成至少两个正整数的和之后，这些正整数的最大乘积。\n* **边界条件**： 0 不是正整数，1 是最小的正整数，0 和 1 都不能拆分，因此 dp[0]=dp[1]=0。\n* **状态转移方程**：当 i≥2 时，假设对正整数 i 拆分出的第一个正整数是 j（1≤j<i），则有以下两种方案：\n    * 将 i 拆分成 j 和 i−j 的和，且 i−j 不再拆分成多个正整数，此时的乘积是 j×(i−j)；\n    * 将 i 拆分成 j 和 i−j 的和，且 i−j 继续拆分成多个正整数，此时的乘积是 j×dp[i−j]。\n\n因此，当 j 固定时，有 dp[i]=max(j×(i−j),j×dp[i−j])。由于 j 的取值范围是 1 到 i−1，需要遍历所有的 j 得到 dp[i] 的最大值，因此可以得到状态转移方程如下：<br>\n<img src=\"/img/202011/002.png\" width=\"50%\" height=\"50%\">\n\n##### 数学思想\n设将长度为 n 的绳子切为 a 段：<br>\n<img src=\"/img/202011/003.png\" width=\"50%\" height=\"50%\">\n\n本题等价于求解：<br>\n<img src=\"/img/202011/004.png\" width=\"50%\" height=\"50%\">\n\n以下公式为“算术几何均值不等式” ，等号当且仅当 n1 = n2 = ... = na 时候成立：<br>\n<img src=\"/img/202011/005.png\" width=\"50%\" height=\"50%\">\n\n> 推论一： 将绳子 以相等的长度等分为多段 ，得到的乘积最大。\n\n<img src=\"/img/202011/006.png\" width=\"70%\" height=\"70%\">\n<img src=\"/img/202011/007.png\" width=\"70%\" height=\"70%\">\n<img src=\"/img/202011/008.png\" width=\"50%\" height=\"50%\">\n\n#### 结果\n\n##### 动态规划\n```java\nclass Solution {\n    public int cuttingRope(int n) {\n        int[] dp = new int[n + 1];\n        dp[0] = 0;\n        dp[1] = 1;\n        for (int i = 2; i <= n; i++) {\n            for (int j = 1; j < i; j++) {\n                dp[i] = Math.max(dp[i], Math.max(j * (i - j), j * dp[i - j]));\n            }\n        }\n        return dp[n];\n    }\n}\n```\n\n##### 数学思想\n```java\nclass Solution {\n    public int cuttingRope(int n) {\n        if (n <= 3) {\n            return n - 1;\n        }\n        int a = n / 3;\n        int b = n % 3;\n        if (b == 0) {\n            return (int) Math.pow(3, a);\n        }\n        if (b == 1) {\n            return (int) Math.pow(3, a - 1) * 4;\n        }\n        return (int) Math.pow(3, a) * 2;\n    }\n}\n```","tags":["LeetCode算法题"]},{"title":"【剑指 offer】13. 机器人的运动范围","url":"/2020/11/16/【剑指-offer】13-机器人的运动范围/","content":"来源：力扣（LeetCode）\n链接：https://leetcode-cn.com/problems/ji-qi-ren-de-yun-dong-fan-wei-lcof\n### 题目描述\n地上有一个m行n列的方格，从坐标 [0,0] 到坐标 [m-1,n-1] 。一个机器人从坐标 [0, 0] 的格子开始移动，它每次可以向左、右、上、下移动一格（不能移动到方格外），也不能进入行坐标和列坐标的数位之和大于k的格子。例如，当k为18时，机器人能够进入方格 [35, 37] ，因为3+5+3+7=18。但它不能进入方格 [35, 38]，因为3+5+3+8=19。请问该机器人能够到达多少个格子？\n\n\n示例 1：\n\n```bash\n输入：m = 2, n = 3, k = 1\n输出：3\n```\n\n示例 2：\n```bash\n输入：m = 3, n = 1, k = 0\n输出：1\n```\n\n提示：\n\n> 1 <= n,m <= 100\n> 0 <= k <= 20\n<!-- more -->\n### 我的解题\n#### 思路\n题目中给了一个限定条件*不能进入行坐标和列坐标的数位之和大于k的格子*，即从点（0，0）出发的机器人在这个条件内能都达到的所有方块数量，同时要注意，所有能够到达的方块都是和点（0，0）连续的，例如当k=2时候，点（100，100），光看k的值是满足的但是机器人无法到达这个点。\n\n这种涉及到连续路径查询，且只需要查询个数不需要查询具体某条路线的问题可以用广度优先搜索来完成。\n\n#### 结果\n```java\nclass Solution {\n    public int movingCount(int m, int n, int k) {\n        //广度优先搜索\n        Queue<int[]> queue = new LinkedList<>();\n        Set<String> used = new HashSet<>();\n        used.add(\"0,0\");\n        queue.add(new int[] {0, 0});\n        int[][] foots = {\n            {1, 0},\n            {-1, 0},\n            {0, 1},\n            {0, -1}\n        };\n        int count = 0;\n\n        while (!queue.isEmpty()) {\n            int[] num = queue.poll();\n            int mK = getK(num[0], num[1]);\n            if (mK > k) {\n                continue;\n            }\n\n            for (int[] foot : foots) {\n                int x = foot[0] + num[0];\n                int y = foot[1] + num[1];\n                if (x < 0 || y < 0 || x >= m || y >= n) {\n                    continue;\n                }\n                String key = x + \",\" + y;\n                if (used.contains(key)) {\n                    continue;\n                }\n                used.add(key);\n\n                if (getK(x, y) > k) {\n                    continue;\n                }\n                queue.add(new int[] {x, y});\n            }\n\n            count++;\n        }\n        return count;\n    }\n\n    private int getK(int i, int j) {\n        int sum = 0;\n        while (i > 0) {\n            int n = i % 10;\n            sum += n;\n            i /= 10;\n        }\n        while (j > 0) {\n            int n = j % 10;\n            sum += n;\n            j /= 10;\n        }\n        return sum;\n    }\n\n}\n```\n### 最优解\n#### 思路\n\n##### 思路一：广度优先搜索\n不过在\n```java\nint[][] foots = {\n    {1, 0},\n    {-1, 0},\n    {0, 1},\n    {0, -1}\n};\n```\n这一步可以用\n```java\nint[][] foots = {\n    {1, 0},\n    {0, 1}\n};\n```\n来替代，用于减少循环次数，因为机器人只会往右或下行走。\n\n##### 思路二：递推\n考虑到方法一提到搜索的方向只需要朝下或朝右，我们可以得出一种递推的求解方法。\n定义 vis[i][j] 为 (i, j) 坐标是否可达，如果可达返回 1，否则返回 0。\n首先 (i, j) 本身需要可以进入，因此需要先判断 i 和 j 的数位之和是否大于 k ，如果大于的话直接设置 vis[i][j] 为不可达即可。\n否则，前面提到搜索方向只需朝下或朝右，因此 (i, j) 的格子只会从 (i - 1, j) 或者 (i, j - 1) 两个格子走过来（不考虑边界条件），那么 vis[i][j] 是否可达的状态则可由如下公式计算得到：<br>\n> **vis[i][j]=vis[i−1][j] or vis[i][j−1]**\n\n即只要有一个格子可达，那么 (i, j) 这个格子就是可达的，因此我们只要遍历所有格子，递推计算出它们是否可达然后用变量 ans 记录可达的格子数量即可。\n初始条件 vis[i][j] = 1 ，递推计算的过程中注意边界的处理。\n\n#### 结果\n```java\nclass Solution {\n    public int movingCount(int m, int n, int k) {\n        if (k == 0) {\n            return 1;\n        }\n        boolean[][] vis = new boolean[m][n];\n        int ans = 1;\n        vis[0][0] = true;\n        for (int i = 0; i < m; ++i) {\n            for (int j = 0; j < n; ++j) {\n                if ((i == 0 && j == 0) || get(i) + get(j) > k) {\n                    continue;\n                }\n                // 边界判断\n                if (i - 1 >= 0) {\n                    vis[i][j] |= vis[i - 1][j];\n                }\n                if (j - 1 >= 0) {\n                    vis[i][j] |= vis[i][j - 1];\n                }\n                ans += vis[i][j] ? 1 : 0;\n            }\n        }\n        return ans;\n    }\n\n    private int get(int x) {\n        int res = 0;\n        while (x != 0) {\n            res += x % 10;\n            x /= 10;\n        }\n        return res;\n    }\n}\n```","tags":["LeetCode算法题"]},{"title":"【剑指 offer】12. 矩阵中的路径","url":"/2020/11/15/【剑指-offer】12-矩阵中的路径/","content":"链接：https://leetcode-cn.com/problems/ju-zhen-zhong-de-lu-jing-lcof\n著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。\n\n### 题目描述\n请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一格开始，每一步可以在矩阵中向左、右、上、下移动一格。如果一条路径经过了矩阵的某一格，那么该路径不能再次进入该格子。例如，在下面的3×4的矩阵中包含一条字符串“bfce”的路径（路径中的字母用加粗标出）。\n\n```bash\n[[\"a\",\"b\",\"c\",\"e\"],\n[\"s\",\"f\",\"c\",\"s\"],\n[\"a\",\"d\",\"e\",\"e\"]]\n```\n\n但矩阵中不包含字符串“abfb”的路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入这个格子。\n\n \n\n示例 1：\n\n```bash\n输入：board = [[\"A\",\"B\",\"C\",\"E\"],[\"S\",\"F\",\"C\",\"S\"],[\"A\",\"D\",\"E\",\"E\"]], word = \"ABCCED\"\n输出：true\n```\n\n\n示例 2：\n```bash\n输入：board = [[\"a\",\"b\"],[\"c\",\"d\"]], word = \"abcd\"\n输出：false\n```\n\n提示：\n```bash\n1 <= board.length <= 200\n1 <= board[i].length <= 200\n```\n<!-- more -->\n### 我的解题\n#### 思路\n首先想到的就是深度优先搜索，即遍历board，判断下标元素是否和word的首字符一致，如果一致则判断周围四个格子是否有字符与word的下一个字符一致。为了避免循环判断，采用一个数据结构记录已经走过的路程。\n\n#### 结果\n```java\n\nclass Solution {\n    public boolean exist(char[][] board, String word) {\n        char[] words = word.toCharArray();\n        int[][] used = new int[board.length][board[0].length];\n        for (int i = 0; i < board.length; i++) {\n            for (int j = 0; j < board[0].length; j++) {\n                if (board[i][j] != words[0]) {\n                    continue;\n                }\n                if (check(board, i, j, words, 0, used)) {\n                    return true;\n                }\n            }\n        }\n        return false;\n    }\n\n    private boolean check(char[][] board, int i, int j, char[] words, int wordIndex, int[][] used) {\n        if (i < 0 || i >= board.length || j < 0 || j >= board[0].length) {\n            if (wordIndex >= words.length) {\n                return true;\n            } else {\n                return false;\n            }\n        }\n        if (used[i][j] == 1) {\n            return false;\n        }\n        if (wordIndex >= words.length) {\n            return true;\n        }\n        if (board[i][j] != words[wordIndex]) {\n            return false;\n        }\n        used[i][j] = 1;\n        int[][] foots = {\n            {1, 0},\n            {-1, 0},\n            {0, 1},\n            {0, -1}\n        };\n        for (int[] foot : foots) {\n            int x = i + foot[0];\n            int y = j + foot[1];\n            if (check(board, x, y, words, wordIndex + 1, used)) {\n                return true;\n            }\n        }\n        used[i][j] = 0;\n        return false;\n    }\n}\n```\n### 最优解\n#### 思路\n本问题是典型的矩阵搜索问题，可使用 深度优先搜索（DFS）+ 剪枝 解决。\n\n* **深度优先搜索**： 可以理解为暴力法遍历矩阵中所有字符串可能性。DFS 通过递归，先朝一个方向搜到底，再回溯至上个节点，沿另一个方向搜索，以此类推。\n* **剪枝**： 在搜索中，遇到 这条路不可能和目标字符串匹配成功 的情况（例如：此矩阵元素和目标字符不同、此元素已被访问），则应立即返回，称之为 可行性剪枝 。\n<!-- ![]() -->\n<img src=\"/img/202011/001.png\" width=\"50%\" height=\"50%\">\n\n##### DFS 解析：\n\n* **递归参数**： 当前元素在矩阵 board 中的行列索引 i 和 j ，当前目标字符在 word 中的索引 k 。\n* **终止条件**：\n    1. 返回 falsefalse ： (1) 行或列索引越界 或 (2) 当前矩阵元素与目标字符不同 或 (3) 当前矩阵元素已访问过 （ (3) 可合并至 (2) ） 。\n    2. 返回 truetrue ： k = len(word) - 1 ，即字符串 word 已全部匹配。\n* **递推工作**：\n    1. 标记当前矩阵元素： 将 board[i][j] 修改为 空字符 '' ，代表此元素已访问过，防止之后搜索时重复访问。\n    2. 搜索下一单元格： 朝当前元素的 上、下、左、右 四个方向开启下层递归，使用 或 连接 （代表只需找到一条可行路径就直接返回，不再做后续 DFS ），并记录结果至 res 。\n    3. 还原当前矩阵元素： 将 board[i][j] 元素还原至初始值，即 word[k] 。\n* **返回值**： 返回布尔量 res ，代表是否搜索到目标字符串。\n\n> 使用空字符（Python: '' , Java/C++: '\\0' ）做标记是为了防止标记字符与矩阵原有字符重复。当存在重复时，此算法会将矩阵原有字符认作标记字符，从而出现错误。\n\n##### 复杂度分析：\n> M,N 分别为矩阵行列大小， K 为字符串 word 长度。\n\n* 时间复杂度 O(3^K * MN) : 最差情况下，需要遍历矩阵中长度为 KK 字符串的所有方案，时间复杂度为O(3^K).矩阵中共有 MN 个起点，时间复杂度为 O(MN) 。\n    * 方案数计算设字符串长度为 KK ，搜索中每个字符有上、下、左、右四个方向可以选择，舍弃回头（上个字符）的方向，剩下 3 种选择，因此方案数的复杂度为O(3^K) 。\n* 空间复杂度 O(K) ： 搜索过程中的递归深度不超过 K ，因此系统因函数调用累计使用的栈空间占用 O(K) （因为函数返回后，系统调用的栈空间会释放）。最坏情况下 K = K=MN ，递归深度为 MN ，此时系统栈使用 O(MN) 的额外空间。\n\n#### 结果\n```java\nclass Solution {\n    public boolean exist(char[][] board, String word) {\n        char[] words = word.toCharArray();\n        for(int i = 0; i < board.length; i++) {\n            for(int j = 0; j < board[0].length; j++) {\n                if(dfs(board, words, i, j, 0)) return true;\n            }\n        }\n        return false;\n    }\n    boolean dfs(char[][] board, char[] word, int i, int j, int k) {\n        if(i >= board.length || i < 0 || j >= board[0].length || j < 0 || board[i][j] != word[k]) return false;\n        if(k == word.length - 1) return true;\n        board[i][j] = '\\0';\n        boolean res = dfs(board, word, i + 1, j, k + 1) || dfs(board, word, i - 1, j, k + 1) || \n                      dfs(board, word, i, j + 1, k + 1) || dfs(board, word, i , j - 1, k + 1);\n        board[i][j] = word[k];\n        return res;\n    }\n}\n```","tags":["LeetCode算法题"]},{"title":"【剑指 offer】07. 重建二叉树","url":"/2020/11/11/【剑指-offer】07-重建二叉树/","content":"来源：力扣（LeetCode）\n链接：https://leetcode-cn.com/problems/zhong-jian-er-cha-shu-lcof\n### 题目描述\n输入某二叉树的前序遍历和中序遍历的结果，请重建该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。\n\n例如，给出\n\n> 前序遍历 preorder = [3,9,20,15,7]\n> 中序遍历 inorder = [9,3,15,20,7]\n返回如下的二叉树：\n\n```bash\n   3\n  / \\ \n 9  20 \n   /  \\ \n  15   7 \n```\n\n限制：\n\n> 0 <= 节点个数 <= 5000\n<!-- more -->\n### 我的解题\n#### 思路\n首先确定两种遍历方式的区别：\n* 前序遍历：根节点->左节点->右节点\n* 中序遍历：左节点->根节点->右节点\n\n因此，在中序遍历中，在根节点之前被访问的节点都位于左子树，在根节点之后被访问的节点都位于右子树。同时前序遍历的数据顺序对于跟节点来说，等于如下结构【根节点】【所有左节点】【所有右节点】，因此，在通过跟节点从中序遍历中得到跟节点的左右字节点队列与个数后，能根据左右字节点的数量从前序遍历中分解出左节点队列和右节点队列，到了这步通过迭代即可遍历树。\n\n> 其实我只到两种遍历的区别，没有第一时间意识到通过宏观的角度将数组分成左右两个节点的队列，太久没有接触树的概念生疏了。\n\n#### 结果\n```java\n/**\n * Definition for a binary tree node.\n * public class TreeNode {\n *     int val;\n *     TreeNode left;\n *     TreeNode right;\n *     TreeNode(int x) { val = x; }\n * }\n */\nclass Solution {\n    public TreeNode buildTree(int[] preorder, int[] inorder) {\n        if (preorder == null || preorder.length == 0) {\n            return null;\n        }\n        Map<Integer, Integer> indexMap = new HashMap<Integer, Integer>();\n        int length = preorder.length;\n        for (int i = 0; i < length; i++) {\n            indexMap.put(inorder[i], i);\n        }\n        TreeNode root = buildTree(preorder, 0, length - 1, inorder, 0, length - 1, indexMap);\n        return root;\n    }\n\n    public TreeNode buildTree(int[] preorder, int preorderStart, int preorderEnd, int[] inorder, int inorderStart, int inorderEnd, Map<Integer, Integer> indexMap) {\n        if (preorderStart > preorderEnd) {\n            return null;\n        }\n        int rootVal = preorder[preorderStart];\n        TreeNode root = new TreeNode(rootVal);\n        if (preorderStart == preorderEnd) {\n            return root;\n        } else {\n            int rootIndex = indexMap.get(rootVal);\n            int leftNodes = rootIndex - inorderStart, rightNodes = inorderEnd - rootIndex;\n            TreeNode leftSubtree = buildTree(preorder, preorderStart + 1, preorderStart + leftNodes, inorder, inorderStart, rootIndex - 1, indexMap);\n            TreeNode rightSubtree = buildTree(preorder, preorderEnd - rightNodes + 1, preorderEnd, inorder, rootIndex + 1, inorderEnd, indexMap);\n            root.left = leftSubtree;\n            root.right = rightSubtree;\n            return root;\n        }\n    }\n}\n```\n ### 最优解\n #### 思路\n 第一个通过迭代的思路同上。因此这里介绍另一种方法：*迭代*。\n 例如要重建的是如下二叉树。\n\n```bash\n        3\n       / \\\n      9  20\n     /  /  \\\n    8  15   7\n   / \\\n  5  10\n /\n4\n```\n\n其前序遍历和中序遍历如下。\n\n>preorder = [3,9,8,5,4,10,20,15,7]\n>inorder = [4,5,8,10,9,3,15,20,7]\n\n前序遍历的第一个元素 3 是根节点，第二个元素 9 可能位于左子树或者右子树，需要通过中序遍历判断。\n\n中序遍历的第一个元素是 4 ，不是根节点 3，说明 9 位于左子树，因为根节点不是中序遍历中的第一个节点。同理，前序遍历的后几个元素 8、5、4 也都位于左子树，且每个节点都是其上一个节点的左子节点。\n\n前序遍历到元素 4，和中序遍历的第一个元素相等，说明前序遍历的下一个元素 10 位于右子树。那么 10 位于哪个元素的右子树？从前序遍历看，10 可能位于 4、5、8、9、3 这些元素中任何一个元素的右子树。从中序遍历看，10 在 8 的后面，因此 10 位于 8 的右子树。把前序遍历的顺序反转，则在 10 之前的元素是 4、5、8、9、3，其中 8 是最后一次相等的节点，因此前序遍历的下一个元素位于中序遍历中最后一次相等的节点的右子树。\n\n根据上述例子和分析，可以使用栈保存遍历过的节点。初始时令中序遍历的指针指向第一个元素，遍历前序遍历的数组，如果前序遍历的元素不等于中序遍历的指针指向的元素，则前序遍历的元素为上一个节点的左子节点。如果前序遍历的元素等于中序遍历的指针指向的元素，则正向遍历中序遍历的元素同时反向遍历前序遍历的元素，找到最后一次相等的元素，将前序遍历的下一个节点作为最后一次相等的元素的右子节点。其中，反向遍历前序遍历的元素可通过栈的弹出元素实现。\n\n* 使用前序遍历的第一个元素创建根节点。\n* 创建一个栈，将根节点压入栈内。\n* 初始化中序遍历下标为 0。\n* 遍历前序遍历的每个元素，判断其上一个元素（即栈顶元素）是否等于中序遍历下标指向的元素。\n    * 若上一个元素不等于中序遍历下标指向的元素，则将当前元素作为其上一个元素的左子节点，并将当前元素压入栈内。\n    * 若上一个元素等于中序遍历下标指向的元素，则从栈内弹出一个元素，同时令中序遍历下标指向下一个元素，之后继续判断栈顶元素是否等于中序遍历下标指向的元素，若相等则重复该操作，直至栈为空或者元素不相等。然后令当前元素为最后一个想等元素的右节点。\n* 遍历结束，返回根节点。\n\n#### 结果\n```java\n/**\n * Definition for a binary tree node.\n * public class TreeNode {\n *     int val;\n *     TreeNode left;\n *     TreeNode right;\n *     TreeNode(int x) { val = x; }\n * }\n */\nclass Solution {\n    public TreeNode buildTree(int[] preorder, int[] inorder) {\n        if (preorder == null || preorder.length == 0) {\n            return null;\n        }\n        TreeNode root = new TreeNode(preorder[0]);\n        int length = preorder.length;\n        Stack<TreeNode> stack = new Stack<TreeNode>();\n        stack.push(root);\n        int inorderIndex = 0;\n        for (int i = 1; i < length; i++) {\n            int preorderVal = preorder[i];\n            TreeNode node = stack.peek();\n            if (node.val != inorder[inorderIndex]) {\n                node.left = new TreeNode(preorderVal);\n                stack.push(node.left);\n            } else {\n                while (!stack.isEmpty() && stack.peek().val == inorder[inorderIndex]) {\n                    node = stack.pop();\n                    inorderIndex++;\n                }\n                node.right = new TreeNode(preorderVal);\n                stack.push(node.right);\n            }\n        }\n        return root;\n    }\n}\n```","tags":["LeetCode算法题"]},{"title":"【剑指-offer】05.替换空格","url":"/2020/11/09/【剑指-offer】05-替换空格/","content":"来源：力扣（LeetCode）\n链接：https://leetcode-cn.com/problems/ti-huan-kong-ge-lcof\n### 题目描述\n请实现一个函数，把字符串 s 中的每个空格替换成\"%20\"。\n\n示例 1：\n\n> 输入：s = \"We are happy.\"\n> 输出：\"We%20are%20happy.\"\n\n限制：\n> 0 <= s 的长度 <= 10000\n<!-- more -->\n### 我的解题\n#### 思路\n有两种方法：\n1. 调用replace库函数直接替换\n2. 遍历字符串，用if判断字符是否是空格，如果是则替换\n\n#### 结果\n我这里使用方法2\n```java\nclass Solution {\n    public String replaceSpace(String s) {\n        StringBuilder sb = new StringBuilder();\n        for (char c : s.toCharArray()) {\n            if (c == ' ') {\n                sb.append(\"%20\");\n            } else {\n                sb.append(c);\n            }\n        }\n        return sb.toString();\n    }\n}\n```\n\n ### 最优解\n #### 思路\n 如果想把这道题目做到极致，就不要只用额外的辅助空间了！\n当然这只是针对C++的情况下，因为在C++中，字符数组是可以在原有的基础上扩充的。\n\n首先扩充数组到每个空格替换成\"%20\"之后的大小。然后从后向前替换空格，也就是双指针法。\n\n为什么不从前往后扩充呢？如果从前往后扩充的话时间复杂度就是O(n^2)了。因为每次添加元素都要将添加元素之后的所有元素向后移动。\n\n其实很多数组填充类的问题，都可以先预先给数组扩容带填充后的大小，然后在从后向前进行操作。这么做有两个好处：\n1. 不用申请新数组。\n2. 从后向前填充元素，避免了从前先后填充元素要来的 每次添加元素都要将添加元素之后的所有元素向后移动。\n\n#### 结果\n```C++\npublic:\n    string replaceSpace(string s) {\n        int count = 0; // 统计空格的个数\n        int sOldSize = s.size();\n        for (int i = 0; i < s.size(); i++) {\n            if (s[i] == ' ') {\n                count++;\n            }\n        }\n        // 扩充字符串s的大小，也就是每个空格替换成\"%20\"之后的大小\n        s.resize(s.size() + count * 2);\n        int sNewSize = s.size();\n        // 从后先前将空格替换为\"%20\"\n        for (int i = sNewSize - 1, j = sOldSize - 1; j < i; i--, j--) {\n            if (s[j] != ' ') {\n                s[i] = s[j];\n            } else {\n                s[i] = '0';\n                s[i - 1] = '2';\n                s[i - 2] = '%';\n                i -= 2;\n            }\n        }\n        return s;\n    }\n};\n```","tags":["LeetCode算法题"]},{"title":"【剑指 offer】03.数组中重复的数字","url":"/2020/11/08/【剑指-offer】03-数组中重复的数字/","content":"来源：力扣（LeetCode）\n链接：https://leetcode-cn.com/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof\n### 题目描述\n找出数组中重复的数字。\n\n\n在一个长度为 n 的数组 nums 里的所有数字都在 0～n-1 的范围内。数组中某些数字是重复的，但不知道有几个数字重复了，也不知道每个数字重复了几次。请找出数组中任意一个重复的数字。\n\n示例 1：\n> 输入：\n> [2, 3, 1, 0, 2, 5, 3]\n> 输出：2 或 3 \n> 输入：\n> [2, 3, 1, 0, 2, 5, 3]\n> 输出：2 或 3 \n \n\n限制：\n\n> 2 <= n <= 100000\n<!-- more -->\n### 我的解题\n#### 思路\n首先，其中的数字大小限定在0～n-1的范围内，既然给定了大小，那么直接创建对应长度的数组，即*int[] list = new int[nums.length]*，然后遍历数组nums，取出结果n,并将n作为下标在list数组中+1，如果结果大于1则表示当前n重复出现。\n\n这里其实就应用了空间换时间的策略，通过创建对应length的数组来加快取值过程，当然也可以使用哈希，那么我们就需要创建Set，为了加快速度，可以在创建对象的时候传入初始容量length。\n\n#### 结果\n```java\nclass Solution {\n    public int findRepeatNumber(int[] nums) {\n        int[] list = new int[nums.length];\n        for (int i = 0; i < nums.length; i++) {\n            int n = nums[i];\n            list[n]++;\n            if (list[n] > 1) {\n                return n;\n            }\n        }\n        return -1;\n    }\n}\n```\n ### 最优解\n #### 思路\n 由于数字大小的范围是0～n-1，刚好数数组的下标对应，我们可以通过某些操作将值与下标对应，一旦某个索引的值不是一个，则找到了重复的数组，即发生了哈希碰撞。\n #### 结果\n```java\nclass Solution {\n    public int findRepeatNumber(int[] nums) {\n        //设索引初始值为 i = 0\n        int i = 0;\n        //遍历整个数组 nums \n        while(i < nums.length) {\n            //索引 i 的值为 i,无需执行交换操作，查看下一位\n            if(nums[i] == i) {\n                i++;\n                continue;\n            }\n            //索引 nums[i] 处的值也为 nums[i]，即找到一组相同值，返回 nums[i] 即可\n            if(nums[nums[i]] == nums[i]) return nums[i];\n            //执行交换操作，目的是为了使索引与值一一对应，即索引 0 的值为 0，索引 1 的值为 1\n            int tmp = nums[i];\n            nums[i] = nums[tmp];\n            nums[tmp] = tmp;\n        }\n        //如果遍历整个数组都没有找到相同的值，返回 -1\n        return -1;\n    }\n}\n```\n### 复杂度分析\n遍历数组需要O(n)的时间。\n\n在代码中的*continue*，这表示在while的一次循环中里只有这次循环将 索引(i) 与 索引值(num[i]) 匹配到了，才会执行下一次循环。\n\n在每一次的循环过程中，索引(i) 与 索引值(num[i]) 匹配到后，在后续的循环过程中不会操作它们，所以虽然一开始的循环过程中，执行的交换操作较多，但在后续的循环过程中根本不需要再执行操作了。\n\n根据均摊复杂度分析 ，总的时间复杂度为 O(N) ，N 为数组的长度。","tags":["LeetCode算法题"]}]